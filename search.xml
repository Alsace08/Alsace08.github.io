<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>一维随机游走的常返性证明</title>
      <link href="/2022/08/11/random-walk/"/>
      <url>/2022/08/11/random-walk/</url>
      
        <content type="html"><![CDATA[<blockquote><p>最近对随机游走的相关问题比较感兴趣。随机游走的常返性质是一个较为经典的问题，众所周知，维度小于等于 2 时常返，大于 2 时非常返。对于一维情形，严格来说，只有当左右移动概率对称时，常返性才得以满足。本文对一维随机游走的常返性进行证明，作为一个没有系统学习过随机过程相关课程的非数学科班出身的学生，我用组合数学的相关思路给出证明过程。</p></blockquote><p>先给出问题的形式化定义：</p><p>考虑在一维空间 $\mathbb{Z}$ 上的随机游走 $X = {X_n}_{n \geq 0}$。一个粒子在 $0$ 时刻从状态 $0$ 出发，即 $X_0 = 0$。每次移动时，以 $p$ 的概率向右移动一个单位，以 $q = 1 - p$ 的概率向左移动一个单位。请证明状态 $0$ 是常返的充要条件是 $p = q = 1/2$，即</p><p>$$<br>\sum_{n=1}^{\infty} \mathbb{P}(X_n = 0, X_k \neq 0, k = 1, …, n - 1 | X_0 = 0) = 1 \Leftrightarrow p = q = 1/2<br>$$</p><p>我们将向右移动记为 $+1$，向左移动记为 $-1$。前 $2n$ 步中，我们根据每一时刻的随机移动方向，得到一个动作序列。该动作序列由 $+1$ 和 $-1$ 组成，记为 ${a_{2n} | a_k = \pm 1, k = 1, …, 2n}$。例如，若前 $4$ 步的随机移动方向分别为 “右左左右”，则对应的动作序列为 ${+1, -1, -1, +1}$。</p><p>定义出了动作序列，我们需要挖掘一下动作序列中存在的限制条件。我们首先考虑只在正半轴游走的情形，负半轴的情形与正半轴完全对称：</p><p>（1）第 $2n$ 步时需要回到原点，这意味着，前 $2n$ 步中向左和向右移动的次数相等，均为 $n$ 次。换作动作序列的形式化表述，即 $a_1 + a_2 + \dotsb + a_{2n} = 0$；</p><p>（2）第 $2n$ 步时，是第一次回到原点，这意味着在第 $2n$ 步之前，向右移动的次数总是多于向左移动的次数（这个结论可以模拟出栈入栈的思路）。因此，形式化表述为，对于任意正整数 $k &lt; 2n$，均有 $a_1 + a_2 + \dots + a_k &gt; 0$ 恒成立。</p><p>这个约束条件的形式十分类似于著名的 Catalan 数。Catalan 数的一个重要的形式化定义如下[1]：</p><blockquote><p>考虑由 $n$ 个 $+1$ 和 $n$ 个 $-1$ 构成的 $2n$ 序列 ${a_1, a_2, \dots, a_{2n}}$，其部分和总满足 $a_1 + a_2 + … + a_k \geq 0$，则序列的个数等于第 $n$ 个 Catalan 数 $C_n = \frac{1}{n+1} C_{2n}^n (n \geq 0)$。</p></blockquote><p>与 Catalan 数的性质唯一不同的是，我们给出的动作序列要求部分和严格大于 0，而 Catalan 数给出的序列允许部分和等于 0。事实上，根据条件我们不难得出，$a_1 = +1$，$a_{2n} = -1$。这时，限制条件（2）可以转化为</p><p>$$<br>a_2 + a_3 + \dots + a_{k} \geq 0 \quad k = 2, 3, …, 2n-1<br>$$</p><p>成功地将问题化归为 Catalan 数的形式。满足该条件的序列个数即为第 $n - 1$ 个 Catalan 数 $\frac{1}{n} C_{2(n-1)}^{n-1} = \frac{1}{2} \cdot \frac{1}{2n-1} C_{2n}^n$。考虑正负半轴的对称性，满足<strong>在第 $2n$ 步时首次回到原点</strong>的游走序列数为 $\frac{1}{2n-1} C_{2n}^n$。再乘以游走的概率 $p^n q^n$，最终的概率计算为</p><p>$$<br>\mathbb{P}(X_{2n} = 0, X_k \neq 0, k = 1, …, 2n - 1 | X_0 = 0) = \frac{1}{2n-1} C_{2n}^n (pq)^n<br>$$</p><p>接下来只需要证明</p><p>$$<br>\sum_{n=1}^{\infty} \frac{1}{2n-1} C_{2n}^n (pq)^n = 1 \Leftrightarrow p = q = 1/2<br>$$</p><!-- 与 Catalan 数的性质唯一不同的是，我们给出的动作序列要求部分和严格大于 0，而 Catalan 数给出的序列允许部分和等于 0。因此，需要重新考虑一个更加特殊的 Catalan 数的形式。我们来考虑一个更一般性的问题，叫做**伯特兰投票问题**。伯特兰投票问题是指，在一场选举中，候选人 $A$ 得到了 $p$ 张选票，而候选人得到了 $q$ 张选票 $(p > q)$，那么在整个点票过程中，$A$ 的票数都严格大于 $B$ 的概率是多少。这个问题的答案是$$\frac{p-q}{p+q}$$投票问题中，给出了**部分和严格大于0**的相关表述。我们看到，动作序列限制条件的第（2）条中所说，当步数小于 $2n$ 的时候，部分和严格大于 0。这是否意味着，我们可以将前 $2n-1$ 步的随机游走，转化为一个 $2n-1$ 票数的伯特兰投票问题？很显然，序列中的 $a_{2n} = -1$，因此前 $2n-1$ 个数中，有 $n$ 个 $+1$ 和 $n-1$ 个 $-1$。**问题转化为：候选人 $A$ 得到了 $n$ 张选票，而候选人得到了 $n-1$ 张选票。那么在整个点票过程中，$A$ 的票数都严格大于 $B$ 的可能情况有多少种？**首先，$A$ 的票数都严格大于 $B$ 的概率是 $\frac{n - (n-1)}{n + (n-1)} = \frac{1}{2n-1}$。而对于一个有 $n$ 个 $+1$ 和 $n-1$ 个 $-1$ 的序列，共有 $C_{2n-1}^n$ 中排列方式。因此可能的情况数为$$\frac{1}{2n-1}C_{2n-1}^n = \frac{1}{2} \cdot \frac{1}{2n-1} C_{2n}^n$$这也就是满足限制条件的所有动作序列的情况数。之后，再乘上前 $2n-1$ 步中，每一步游走对应的概率，即 $p^n q^{n-1}$，则最终计算得出的第 $2n$ 步时第一次回到原点的概率为$$\frac{1}{2} \cdot \frac{1}{2n-1} C_{2n}^n p^n q^{n-1} = \frac{1}{2q} \cdot \frac{1}{2n-1} C_{2n}^n (pq)^n$$接下来只需要证明$$\sum_{n=1}^{\infty} \frac{1}{2q} \cdot \frac{1}{2n-1} C_{2n}^n (pq)^n = 1 \Leftrightarrow p = q = 1/2$$ --><p>考虑<strong>和函数</strong></p><p>$$<br>\begin{aligned}<br>    S(x) &amp;= \sum_{n=1}^{\infty} \frac{1}{2n-1} C_{2n}^n x^n \\<br>    &amp;= \sum_{n=1}^{\infty} \frac{1}{2n-1} \cdot \frac{1}{n!} \cdot \frac{(2n)!}{n!} x^n \\<br>    &amp;= \sum_{n=1}^{\infty} \frac{1}{n!} \cdot \frac{1}{2n-1} \cdot \frac{2n \cdot (2n-1) \cdot (2n-2)!}{n \cdot (n-1)!}\\<br>    &amp;= \sum_{n=1}^{\infty} \frac{2}{n!} \cdot \frac{(2n-2)!}{(n-1)!} x^n \\<br>\end{aligned}<br>$$</p><p>求导得</p><p>$$<br>\begin{aligned}<br>    S’(x) &amp;= 2 \cdot \sum_{n=1}^{\infty} \frac{1}{(n-1)!} \cdot \frac{[2(n-1)]!}{(n-1)!} x^{n-1} \\<br>    &amp;= 2 \cdot \sum_{n=0}^{\infty} \frac{1}{n!} \cdot \frac{(2n)!}{n!} x^{n} \\<br>    &amp;= 2 \cdot \sum_{n=0}^{\infty} \frac{1}{n!} \cdot \frac{(2n)!}{n!} \cdot \frac{1}{4^n} (4x)^{n} \\<br>    &amp;= 2 \cdot \sum_{n=0}^{\infty} \frac{1}{n!} \cdot (\frac{1}{2}) (\frac{3}{2}) \dots (\frac{2n-1}{2}) \cdot (4x)^{n} \\<br>    &amp;= \frac{2}{\sqrt{1-4x}}</p><p>\end{aligned}<br>$$</p><p>积分后得</p><p>$$<br>S(x) = 1 - \sqrt{1-4x}<br>$$</p><p>因此，一维随机游走的常返概率最终计算为</p><p>$$<br>\sum_{n=1}^{\infty} \mathbb{P}(X_n = 0, X_k \neq 0, k = 1, …, n - 1 | X_0 = 0) = 1 - \sqrt{1-4pq}<br>$$</p><p>令 $1 - \sqrt{1-4pq} = 1$，则 $pq = \frac{1}{4}$，$p = q = \frac{1}{2}$，充要性证毕。</p><p><strong>参考书目</strong></p><p>[1] （美）布鲁迪（Brualdi, R.A.）著；冯速等译. 组合数学（原书第5版）[M]. 北京：机械工业出版社，2012.4：164-165.</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率论 </tag>
            
            <tag> 随机游走 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitHub + Hexo 个人博客搭建（一）：博客框架搭建</title>
      <link href="/2022/08/10/hexo%E6%90%AD%E5%BB%BA/"/>
      <url>/2022/08/10/hexo%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文将从零开始构建一个 hexo 个人博客的基本框架。本教程面向 window 用户。</p></blockquote><h2 id="1-安装准备"><a href="#1-安装准备" class="headerlink" title="1 安装准备"></a>1 安装准备</h2><h2 id="2-github-创建仓库"><a href="#2-github-创建仓库" class="headerlink" title="2 github 创建仓库"></a>2 github 创建仓库</h2><p>打开 github 的 repositories 界面，点击右上角的 <code>new</code> 按钮。创建一个新的项目仓库。</p><!-- ![创建仓库](/pic/hexo/仓库.png) --><img src="/pic/hexo/仓库.png" width="50%" height="50%"><p>进入创建仓库界面之后，需要填写 <code>repository name</code>。注意，<font color=Red>此处的地址后缀必须是 github.io</font></p><!-- ![新的项目仓库](/pic/hexo/创建仓库.png) --><img src="/pic/hexo/创建仓库.png" width="50%" height="50%"><p>其余部分可选填，之后点击 <code>create repository</code>，完成仓库的创建。</p><h2 id="3-hexo-环境部署"><a href="#3-hexo-环境部署" class="headerlink" title="3 hexo 环境部署"></a>3 hexo 环境部署</h2><p>hexo 是我们的个人博客网站的框架。我们需要在电脑里的任意位置创建一个文件夹，命名任意（我命名为 Homepage，为便于叙述，之后统称为[BlogRoot]，即文件夹的根目录）。hexo 框架以及以后你自己发布的网页都在这个文件夹中。创建好后，<font color=Red>进入文件夹的根目录[BlogRoot]中</font>，鼠标右键打开 <code>git bash</code>，运行命令安装 hexo</p><pre class="line-numbers language-npm" data-language="npm"><code class="language-npm">npm install -g hexo-cli <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装完成后，初始化博客</p><pre class="line-numbers language-hexo" data-language="hexo"><code class="language-hexo">hexo init blog<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>初始化完成后，我们可以在本地测试一下博客的效果。同样地，在根目录下打开 <code>git bash</code>，依次执行命令</p><pre class="line-numbers language-hexo" data-language="hexo"><code class="language-hexo">hexo g # 生成网页hexo s # 本地预览<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>如图所示</p><!-- ![hexo初始化](/pic/hexo/hexo初始化.png) --><img src="/pic/hexo/hexo初始化.png" width="50%" height="50%"><p>我们可以打开 <code>http://localhost:4000/</code> 页面，查看博客的初始化效果，如图所示<br><img src="/pic/hexo/hexo%E9%A2%84%E8%A7%88.png" alt="博客初始化预览效果"></p><p>界面中展示了没有发表文章时的默认界面，使用了名为 next 的默认主题（后续可对主题进行魔改）。至此，我们完成了在本地对 hexo 博客的环境进行部署的操作。</p><h2 id="4-hexo-网站推送"><a href="#4-hexo-网站推送" class="headerlink" title="4 hexo 网站推送"></a>4 hexo 网站推送</h2><p>上一部分的操作，仅仅是在本地对 hexo 博客进行了预览。我们需要将其发布到网上，才能使网站被互联网上的其他访客所访问。这里需要用到<font color=Red>根目录下</font>的一个至关重要的文件 <code>_config.yml</code>，这是我们后续对网站进行部署的<font color=Red>站点配置文件</font>。</p><!-- ![站点配置文件](/pic/hexo/站点配置.png) --><img src="/pic/hexo/站点配置.png" width="50%" height="50%"><p>推送网站的关键，便是将本地的 hexo 与刚刚创建的 github 仓库关联起来。打开 <code>_config.yml</code> 文件后，来到文件的末尾部分，修改如下内容</p><pre class="line-numbers language-yml" data-language="yml"><code class="language-yml">deploy:  type: git  repository: https:&#x2F;&#x2F;github.com&#x2F;Alsace08&#x2F;alsaceym.github.io # 此处修改为你的 github 仓库的地址  branch: main<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>注意，若想要成功部署到网页上，你的<font color=Red>网站地址必须和 github 用户名完全一致！</font> 否则会出现 404 报错，部署失败。</p><p>保存好配置文件后，安装部署插件</p><pre class="line-numbers language-npm" data-language="npm"><code class="language-npm">npm install hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>接下来，运行如下三条命令，完成博客的上线部署</p><pre class="line-numbers language-hexo" data-language="hexo"><code class="language-hexo">hexo clean # 清除缓存hexo g # 生成网页hexo d # 启动在线部署<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>此时，打开我们在 github 仓库中设置的网站地址，即可看到与之前在本地预览中一致的效果。</p>]]></content>
      
      
      <categories>
          
          <category> hexo 个人博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo 个人博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bert进阶模型</title>
      <link href="/2022/08/08/bert%E8%BF%9B%E9%98%B6%E6%A8%A1%E5%9E%8B/"/>
      <url>/2022/08/08/bert%E8%BF%9B%E9%98%B6%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>随着以 GPT、BERT 为代表的预训练语言模型的提出，很多的工作集中在进一步优化预训练语言模型，使之在各类自然语言处理任务上获得更好的效果。</p></blockquote><h2 id="2-Roberta"><a href="#2-Roberta" class="headerlink" title="2 Roberta"></a>2 Roberta</h2><ul><li>预训练阶段：改进了 MLM 任务，舍弃了 NSP 任务</li><li>微调阶段：扩大训练数据，优化模型参数</li></ul><h3 id="2-1-MLM-任务-——-动态掩码机制"><a href="#2-1-MLM-任务-——-动态掩码机制" class="headerlink" title="2.1 MLM 任务 —— 动态掩码机制"></a>2.1 MLM 任务 —— 动态掩码机制</h3><p>在 BERT 的预训练任务中，MLM 任务的输入语句掩码是在预训练阶段完成的，这导致了生成的掩码是静态的。同一个文本只有一种掩码形式，数据的复用效率较低。</p><ul><li>改进：动态掩码技术，掩码的位置和方向在模型的训练阶段实时计算，因此对于同一个文本来说，每一轮都会有不同的掩码方式，当训练轮数较大时，数据复用率大大增加。</li></ul><h3 id="2-2-NSP-任务-——-舍弃与任务拓展"><a href="#2-2-NSP-任务-——-舍弃与任务拓展" class="headerlink" title="2.2 NSP 任务 —— 舍弃与任务拓展"></a>2.2 NSP 任务 —— 舍弃与任务拓展</h3><p>在 BERT 预训练阶段，NSP 任务（预测两段文本是否构成前后关系）被用来提升模型在句子粒度的上下文提取能力。为了解 NSP 任务的有效性，RoBERTa 进行了再探索。</p><ul><li>改进：对比了 4 种实验设置<ul><li><p>文本对输入 + NSP：由一对文本构成输入（每个文本由多句组成），整体长度小于 512。<code>（原始 BERT 的输入形式）</code></p></li><li><p>句子对输入 + NSP：由一对句子构成输入，大多数情况下整体长度会大于 512。因此可以增大数据吞吐量。</p></li><li><p>跨文档整句输入：由一对文本构成输入，到达文档末端时继续从下一个文档抽取句子，添加分隔符表示文档边界。<strong>此处不使用 NSP 损失。</strong></p></li><li><p>文档内整句输入：由一对文本构成输入，到达文档末端时不允许继续从下一个文档抽取句子。因此可以增大数据吞吐量。<strong>此处不使用 NSP 损失。</strong></p></li></ul></li></ul><p>当使用 NSP loss 时（前两个任务），输入文本对的效果更好，因为句子对长度较短，长距离依赖较难捕获；当不使用 NSP loss 时（后两个任务），“文档内整句输入”好于“跨文档整句输入”，且两者均由于使用 NSP loss 的任务效果。</p><p>然而，“文档内整句输入”导致批次大小是一个可变量（输入的 token 数小于 512），因此采用了“跨文档整句输入”，舍弃了原始的 NSP 任务。</p><h3 id="2-3-数据与参数优化"><a href="#2-3-数据与参数优化" class="headerlink" title="2.3 数据与参数优化"></a>2.3 数据与参数优化</h3><ul><li><p>预训练数据：从 16GB 扩充到 160GB，包含 5 个数据来源。</p></li><li><p>批次大小与预训练步数</p><ul><li>原始 BERT：batch = 256，step = 1M</li><li>RoBERTa：batch = 8192，step = 500K</li></ul></li><li><p>词表</p><p>  * </p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 预训练语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp 中的神经网络基础</title>
      <link href="/2022/08/04/nlp-model/"/>
      <url>/2022/08/04/nlp-model/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN 实现文本情感分类任务</title>
      <link href="/2022/07/31/textcnn/"/>
      <url>/2022/07/31/textcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 中常见的 tensor 操作</title>
      <link href="/2022/07/31/tensor-operation/"/>
      <url>/2022/07/31/tensor-operation/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/tensor.png" alt="scalar - vector - tensor"></p><blockquote><p>我们在训练深度网络时，不可避免地会涉及到对张量（tensor）的操作，例如维度变换、数据填充等。每种操作都在 torch 库中有对应的函数，然而，由于操作种类繁多，我们很难记住所有命令并将其区分开来，且容易造成混淆。因此，本文罗列了若干常用的张量操作命令及对应的参数设置，方便以后在进行深度模型的部署时进行查询调用。</p></blockquote><h2 id="1-单个张量的维度操作"><a href="#1-单个张量的维度操作" class="headerlink" title="1 单个张量的维度操作"></a>1 单个张量的维度操作</h2><p>对于单个张量的操作，常见的有维度的变形、扩张、压缩，以及在指定维度下的填充等操作。</p><h3 id="1-1-维度变形"><a href="#1-1-维度变形" class="headerlink" title="1.1 维度变形"></a>1.1 维度变形</h3><blockquote><p>torch.view(shape)：新旧张量数据元素相同，但是尺寸不同</p></blockquote><ul><li>shape - 变形后的尺寸</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 2)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; a.view(2, 16)print(a.size())     # torch.Size([2, 16])a &#x3D; a.view(8, -1)print(a.size())     # torch.Size([8, 4])a &#x3D; a.view(8, 3)print(a.size())     # RuntimeError: shape &#39;[8, 3]&#39; is invalid for input of size 32<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>注：若某一维度的 shape 为 -1 ，则自动计算维度后填充，如例 3</li></ul><h3 id="1-2-维度压缩"><a href="#1-2-维度压缩" class="headerlink" title="1.2 维度压缩"></a>1.2 维度压缩</h3><blockquote><p>torch.squeeze(input, dim=None, out=None)：除去输入张量 input 中维数为 1 的维度。例如，输入张量维度为 (a * 1 * b * c * 1)：（1）若不指定维度 dim 的具体数值，则返回张量的维度为 (a * b * c)；（2）若指定维度，当对应维度的维数为 1，则在对应维度上压缩，例如 dim = 1，当对应维度的维数不为 1，则不进行压缩操作，例如 dim = 0。</p></blockquote><ul><li>input (Tensor) – 输入张量</li><li>dim (int, optional) – 如果给定，则只会在给定维度压缩</li><li>out (Tensor, optional) – 输出张量</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a, dim&#x3D;2)print(a.size())     # torch.Size([4, 4, 2, 1])a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a, dim&#x3D;1)print(a.size())     # torch.Size([4, 4, 1, 2, 1])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-3-维度扩展"><a href="#1-3-维度扩展" class="headerlink" title="1.3 维度扩展"></a>1.3 维度扩展</h3><blockquote><p>torch.unsqueeze(input, dim=None, out=None)：有维度压缩，就有维度扩展，即对输入张量 input 的指定维度插入维数 1。</p></blockquote><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 插入维度的索引</li><li>out (Tensor, optional) – 输出张量</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 2)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; torch.unsqueeze(a, dim&#x3D;1)print(a.size())     # torch.Size([4, 1, 4, 2])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-4-维度填充"><a href="#1-4-维度填充" class="headerlink" title="1.4 维度填充"></a>1.4 维度填充</h3><blockquote><p>torch.nn.functional.pad(input, pad, mode=’constant’, value=0)：不改变维度，仅改变维度数值，在某个维度上进行参数的扩充</p></blockquote><ul><li>pad - 扩充维度，预先定义出某维度上的扩充参数（具体见示例）</li><li>mode - 扩充方法：’constant’, ‘reflect’ 和 ‘replicate’ 三种模式，分别表示常量，反射，复制</li><li>value - 扩充时指定补充值，仅在 mode = ‘constant’ 时有效</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">import torch.nn.functional as Fa &#x3D; torch.zeros(2, 2, 1)print(a)# tensor([[[0.],#          [0.]],#         [[0.],#          [0.]]])print(a.size())# torch.Size([2, 2, 1])a &#x3D; torch.zeros(2, 2, 1)a &#x3D; F.pad(a, pad&#x3D;(1, 2), mode&#x3D;&#39;constant&#39;, value&#x3D;1)  # 在倒数第一个维度上，左边填充 1 个维数，右边填充 2 个维数print(a)# tensor([[[1., 0., 1., 1.],#          [1., 0., 1., 1.]],#         [[1., 0., 1., 1.],#          [1., 0., 1., 1.]]])print(a.size())# torch.Size([2, 2, 4])a &#x3D; torch.zeros(2, 2, 1)a &#x3D; F.pad(a, pad&#x3D;(1, 2, 2, 1), mode&#x3D;&#39;constant&#39;, value&#x3D;1)    # 在倒数第一个维度上，左边填充 1 个维数，右边填充 2 个维数；在倒数第二个维度上，左边填充 2 个维数，右边填充 1 个维数print(a)# tensor([[[1., 1., 1., 1.],#          [1., 1., 1., 1.],#          [1., 0., 1., 1.],#          [1., 0., 1., 1.],#          [1., 1., 1., 1.]],#         [[1., 1., 1., 1.],#          [1., 1., 1., 1.],#          [1., 0., 1., 1.],#          [1., 0., 1., 1.],#          [1., 1., 1., 1.]]])print(a.size())# torch.Size([2, 5, 4])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/tensor/Fpad.png" alt="torch.nn.functional 中的 padding 操作示意图"></p><h3 id="1-5-维度置换"><a href="#1-5-维度置换" class="headerlink" title="1.5 维度置换"></a>1.5 维度置换</h3><blockquote><p>torch.permute(dims)：对张量进行对应维度上的置换，维数值不变。</p></blockquote><ul><li>dims：指定换位顺序，例如 dims=(1, 0, 2)，则维度 0 和维度 1 置换顺序。</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(6, 3, 7)print(a.size())     # torch.Size([6, 3, 7])a &#x3D; x.permute(2, 0, 1)print(a.size())     # torch.Size([7, 6, 3])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-多个张量的维度操作"><a href="#2-多个张量的维度操作" class="headerlink" title="2 多个张量的维度操作"></a>2 多个张量的维度操作</h2><p>对于多个张量的操作，常见的有张量之间的拼接与拆分。</p><h3 id="2-1-维度合并"><a href="#2-1-维度合并" class="headerlink" title="2.1 维度合并"></a>2.1 维度合并</h3><h4 id="2-1-1-不产生新维度"><a href="#2-1-1-不产生新维度" class="headerlink" title="2.1.1 不产生新维度"></a>2.1.1 不产生新维度</h4><blockquote><p>torch.cat(input, dim)：将两个相同维度的张量合并成一个新的张量。想要拼接的维度上的数值可以不同，但其余维度上的数值应完全一致</p></blockquote><ul><li>input：输入张量</li><li>dim：按照维度 dim 进行合并</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.rand(4, 32, 8)b &#x3D; torch.rand(5, 32, 8)c &#x3D; torch.cat([a, b], dim&#x3D;0)print(c.size())     # torch.Size([9, 32, 8])a &#x3D; torch.rand(4, 32, 8)b &#x3D; torch.rand(5, 32, 8)c &#x3D; torch.cat([a, b], dim&#x3D;1)print(c.size())     # RuntimeError: Sizes of tensors must match except in dimension 1. Got 4 and 5 in dimension 0 (The offending index is 1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-2-产生新维度"><a href="#2-1-2-产生新维度" class="headerlink" title="2.1.2 产生新维度"></a>2.1.2 产生新维度</h4><blockquote><p>torch.stack(input, dim)：将若干维度相同，每个维度的数值也相同的张量合并成一个新的张量，并在最外层扩张一个新的维度，该维度的维数即为合并的张量的数目</p></blockquote><ul><li>input：输入张量</li><li>dim：按照维度 dim 进行合并</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.rand(32, 8)b &#x3D; torch.rand(32, 8)c &#x3D; torch.rand(32, 8)d &#x3D; torch.stack([a, b, c], dim&#x3D;0)print(d.size())     # torch.Size([3, 32, 8])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-维度拆分"><a href="#2-2-维度拆分" class="headerlink" title="2.2 维度拆分"></a>2.2 维度拆分</h3><blockquote><p>torch.split(split_size, dim=0)：将一个张量在某个维度上进行拆分，拆分后该维度上的数值发生改变，其余维度的数值不变</p></blockquote><ul><li><p>split_size</p><ul><li>如果是一个数字 num，表示将维度为 dim 中的值按照 num 平均拆分成多个 tensor；</li><li>如果是一个列表 [num1, num2, num3, …]，表示将维度为 dim 中的值按照该列表进行分配，生成指定个数的 tensor</li></ul></li><li><p>dim：按照维度 dim 进行拆分</p></li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">d &#x3D; torch.rand(6, 32, 8)a, b, c &#x3D; d.split([3, 2, 1], dim&#x3D;0)print(a.size())     # torch.Size([3, 32, 8])print(b.size())     # torch.Size([2, 32, 8])print(c.size())     # torch.Size([1, 32, 8])# 能够整除d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split(3, dim&#x3D;0)print(a.size())     # torch.Size([3, 32, 8])print(b.size())     # torch.Size([3, 32, 8])# 无法整除，则取余d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split(4, dim&#x3D;0)print(a.size())     # torch.Size([4, 32, 8])print(b.size())     # torch.Size([2, 32, 8])# 报错示例 1 d &#x3D; torch.rand(6, 32, 8)a, b, c &#x3D; d.split(3, dim&#x3D;0)print(a.size())print(b.size())print(c.size())# ValueError: not enough values to unpack (expected 3, got 2)# 报错示例 2d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split([3, 2, 1], dim&#x3D;0)print(a.size())print(b.size())# ValueError: too many values to unpack (expected 2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>在报错示例 1 中我们发现，该拆分操作旨在将 dim=0 上的 6 降为 3，因此只能拆分出 6/3 = 2 个张量。而在赋值语句中，我们设置了 a, b, c 三个张量，由于无法拆分出这么多张量，故返回报错结果。</li><li>在报错示例 2 中我们发现，该拆分操作返回三个张量结果，而我们设置的函数接收值仅有两个，故返回报错结果。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM+CRF 实现命名实体识别任务</title>
      <link href="/2022/07/28/LSTM-CRF/"/>
      <url>/2022/07/28/LSTM-CRF/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/LSTM+CRF.jpg" alt="LSTM+CRF 模型架构图"></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 命名实体识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表——经典题型总结</title>
      <link href="/2022/07/17/%E9%93%BE%E8%A1%A8%E5%88%B7%E9%A2%98/"/>
      <url>/2022/07/17/%E9%93%BE%E8%A1%A8%E5%88%B7%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h2><p>维护三个结点，分别是当前结点 now，下一个结点 nxt，上一个结点 last，每次更新 now-&gt;next = last，之后三个结点依次右移，直至结尾。</p><h2 id="检测链表中的环，并判断入口位置"><a href="#检测链表中的环，并判断入口位置" class="headerlink" title="检测链表中的环，并判断入口位置"></a>检测链表中的环，并判断入口位置</h2><p><font color=Red>快慢指针</font></p><p>使用两个指针 fast 和 slow，slow 每次向后移动一个，fast 每次移动两个。slow 指针进入环后，在遍历完整个环之前，一定会和 fast 指针相遇。</p><p>相遇后，新增一个 ptr 指针，和 slow 指针同时移动，相遇点即为环入口。</p><p><img src="/pic/%E9%93%BE%E8%A1%A8/%E9%93%BE%E8%A1%A8%E7%8E%AF%E6%A3%80%E6%B5%8B.png" alt="链表环检测"></p><h2 id="检测两链表中的重合节点"><a href="#检测两链表中的重合节点" class="headerlink" title="检测两链表中的重合节点"></a>检测两链表中的重合节点</h2><p><font color=Red>方法一：双指针求解链表长度</font></p><p>首先求解链表 A 和 B 的长度 $L_A$ 和 $L_B$，之后让长的那个链表先走 $|L_B - L_A|$ 的距离，之后同时出发，直到相遇。</p><p><font color=Red>方法二：双指针无重新遍历</font></p><p>设 A 链表的不重合部分长度为 a，B 链表的不重合部分长度为 b，两链表重合部分长度为 c。</p><p>headA 和 headB 分别指向 A 和 B 的头结点，同时开始向右移动，到达尾结点时，转到另一个链表的头部继续移动，直到相遇。两个指针相遇时，均正好走过 a + b + c 的距离。</p><h2 id="返回链表倒数第N个节点"><a href="#返回链表倒数第N个节点" class="headerlink" title="返回链表倒数第N个节点"></a>返回链表倒数第N个节点</h2><p><font color=Red>方法一：简单遍历</font></p><p>先遍历求出链表长度 $L$，再遍历找出第 $L-N+1$ 个元素。</p><p><font color=Red>方法二：栈</font></p><p>首次遍历时将元素入栈，之后从栈顶弹出第 $N$ 个元素。（考虑栈的特性，当需要从链表的尾部往前遍历时，都可以使用栈来操作）</p><p><font color=Red>方法三：双指针（常用思路）</font></p><p>使用两个指针 first 和 second，其中 first 比 second 超前 $n$ 位，当 first 到达末尾（判定 first-&gt;next == NULL即可）。second 刚好到达倒数第 $n$ 位。</p><h2 id="链表相加"><a href="#链表相加" class="headerlink" title="链表相加"></a>链表相加</h2><p><img src="/pic/%E9%93%BE%E8%A1%A8/%E9%93%BE%E8%A1%A8%E7%9B%B8%E5%8A%A0.png" alt="链表相加"></p><p>链表压栈，后弹栈求和；或先反转链表，之后顺序求和。</p><h2 id="链表重排"><a href="#链表重排" class="headerlink" title="链表重排"></a>链表重排</h2><p><img src="/pic/%E9%93%BE%E8%A1%A8/%E9%93%BE%E8%A1%A8%E9%87%8D%E6%8E%92.png" alt="链表重排"></p><p><font color=Red>方法一：线性表</font></p><p>将链表转化为数组，之后就可以随机访问了。</p><p><font color=Red>方法二：链表中点+反转链表+合并</font></p><h2 id="回文链表"><a href="#回文链表" class="headerlink" title="回文链表"></a>回文链表</h2><p>利用快慢双指针找到中间数，之后反转前半部分链表，最后从中间数开始向两边遍历，进行判断。</p><h2 id="链表排序"><a href="#链表排序" class="headerlink" title="链表排序"></a>链表排序</h2><p><img src="/pic/%E9%93%BE%E8%A1%A8/%E9%93%BE%E8%A1%A8%E6%8E%92%E5%BA%8F.png" alt="链表排序"></p>]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题库 </tag>
            
            <tag> 链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>形态学：语言的词汇</title>
      <link href="/2022/07/17/%E5%BD%A2%E6%80%81%E5%AD%A6/"/>
      <url>/2022/07/17/%E5%BD%A2%E6%80%81%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文所述内容参考《语言引论》一书的第三章。</p></blockquote><p>词汇是语言知识的重要一环，并构成我们心理语法的一个组成部分。懂得一个词，就意味着知道某个特定的语音序列与特定的语义相关联。每个词都是一个音义结合的单位，因此我们的心理词库所储存的每个词必须列出其独特的语音表征，决定词的发音，并附上语义。</p><p>音义关系具有任意性。有些词发音相同，但意义不同（例如 bear 和 bare）；另一些词意义相同，但发音不同（例如 sofa 和 couch）。</p><h2 id="1-词典"><a href="#1-词典" class="headerlink" title="1 词典"></a>1 词典</h2><p>绝大多数词典，均以“规范”而非“描写”语词为己任。</p><p>对于词典中的每个词，都需要提供下列信息：（1）拼写；（2）标准发音；（3）一个或多个语义定义；（4）词性。</p><h2 id="2-实义词和功能词"><a href="#2-实义词和功能词" class="headerlink" title="2 实义词和功能词"></a>2 实义词和功能词</h2><ul><li><p>实义词：名词、动词、形容词、副词等我们可以加以考虑的事物、行为、属性、观念等概念，又称开放类词。</p></li><li><p>功能词：连词、介词、冠词、代词等用来界定语法关系，几乎没有语义内容的词汇，又称封闭类词。</p></li></ul><p>人脑处理功能词和实义词的方式不同，脑损害患者和其他有特定语言障碍的病人对于理解功能词，比理解实义词困难得多。</p><p>实义词用来表达语义，功能词则将实义词与更大的语法语境连成一体。两者在语言中各司其职。</p><h2 id="3-语素"><a href="#3-语素" class="headerlink" title="3 语素"></a>3 语素</h2><p>语素是<font color=Red>语法形式的最基本单位</font>，是语音和语义的任意结合体，是一切语言中最小的符号。一个语素可以是单个语音或多个音节，例如 a（单个语音）、child（单个音节）、water（两个音节）等。</p><p>一个词由一个或一个以上的语素构成，例如 im-possible，由两个语素构成。</p><p>用形态学来阐释语言创造性：我们既可以把一个词分解为其组成成分，对整词的词义进行理解或猜测，还可以将语素结合起来创造新词。</p><h3 id="3-1-黏着语素和自由语素"><a href="#3-1-黏着语素和自由语素" class="headerlink" title="3.1 黏着语素和自由语素"></a>3.1 黏着语素和自由语素</h3><p>形态学知识包含两个部分：（1）关于单个语素的知识；（2）关于语素结合规则的知识。</p><ul><li><p>自由语素：本身就构成词的语素，例如 boy、man。</p></li><li><p>黏着语素：永远不能自己构成词，但总是词的组成部分，是词缀。例如 -er、-ist。</p></li></ul><h4 id="3-1-1-前缀和后缀"><a href="#3-1-1-前缀和后缀" class="headerlink" title="3.1.1 前缀和后缀"></a>3.1.1 前缀和后缀</h4><p>词缀根据出现在其他语素的前面还是后面，分为前缀和后缀。</p><ul><li>同一种含义的语素，在不同语言下的规则方式不同。</li></ul><blockquote><p>在英语中，复数语素 -s 是后缀，但在墨西哥的伊斯姆斯-萨波特克语中，复数语素 -ka 是前缀。</p></blockquote><ul><li>同一个语素，在不同语言下含义不同。</li></ul><blockquote><p>语素 -ak 在土耳其语和卡罗克语（太平洋西北部岛屿上的一种美洲土著语）中的意义不同。在土耳其语中，表示将一个动词派生为名词，而在卡罗克语中，则表示将名词派生为副词，表示“在…里面”。</p></blockquote><p>同时进一步说明：<font color=Red>音义关系具有任意性</font>。</p><h4 id="3-1-2-中缀"><a href="#3-1-2-中缀" class="headerlink" title="3.1.2 中缀"></a>3.1.2 中缀</h4><p>一些语言中还存在中缀，即插入其他语素中间的语素。</p><blockquote><p>菲律宾的邦托克语中有这样一种中缀，-um- 插入名词或形容词的第一个辅音之后，用以将名词/形容词转化为动词，例如：fikas（强壮） —— fumikas（是强壮的）。</p></blockquote><p>英语的中缀通常只能将表达猥亵义的整个词插入另一个词中，最常见的 fuckin，插入后例如 un-fuckin-believable。</p><h4 id="3-1-3-外接缀"><a href="#3-1-3-外接缀" class="headerlink" title="3.1.3 外接缀"></a>3.1.3 外接缀</h4><p>一些语言中还存在外接缀，即在同一个词基语素的开头和末尾附加上的语素。</p><blockquote><p>德语中，规则动词的过去分词，通过在动词词根加上前缀 ge- 和后缀 -t 构成。例如：lieb（爱） —— geliebt（爱的过去分词）。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 形态学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022计算机保研记录 | 夏令营经历分享</title>
      <link href="/2022/07/16/%E4%BF%9D%E7%A0%941/"/>
      <url>/2022/07/16/%E4%BF%9D%E7%A0%941/</url>
      
        <content type="html"><![CDATA[<h2 id="0-写在前面的话"><a href="#0-写在前面的话" class="headerlink" title="0 写在前面的话"></a>0 写在前面的话</h2><h2 id="1-个人背景"><a href="#1-个人背景" class="headerlink" title="1 个人背景"></a>1 个人背景</h2><ul><li><p>本科院校：北京航空航天大学</p></li><li><p>专业：人工智能 </p></li><li><p>排名（前五学期）：3/31（10%）</p></li><li><p>英语：CET-4 581，CET-6 549</p></li><li><p>竞赛&amp;荣誉：数竞、蓝桥杯省一，美赛H，若干校级科创竞赛奖项，以及校级和企业奖学金</p></li><li><p>科研&amp;项目：两段科研经历，均为自然语言处理方向。一篇 ICDM（DM, CCF-B） 二作，一篇 COLING（NLP，CCF-B） 一作，夏令营时均为在投状态；无横向项目经历</p></li></ul><p>根据我参加夏令营的经验，个人认为在<font color=Red>初审阶段</font>，各项指标的重要程度大致排序为：院校 $\approx$ 排名 &gt; 科研 &gt; 竞赛 &gt; 英语，这里的竞赛指的是数学、数模，包括蓝桥杯等普通编程类竞赛，但是如果是 ACM 竞赛，其含金量应该等同于一项优质的科研经历。</p><p>那么就我个人的背景而言，首次，最占优的是本科院校和科研经历。本科就读于中上游 985，科研经历较为丰富，且不是类似于大创、互联网+这种横向课题经历，而是真正的研究经历，且方向较为纯粹，始终聚焦于自然语言处理（这也是我研究生阶段坚定选择的研究方向）。其次，竞赛和英语平平无奇，四六级不高不低，竞赛也都是省级奖项，没有什么有含金量的国家级奖项，整体不加分也不减分。最后是排名，这是我整个背景的一大劣势，且这一劣势在初审阶段十分致命。由于疫情原因，大多数夏令营采取了线上的形式，因此海投现象非常严重。于是，不少院校会采用直接筛选排名的方式来初步过滤简历，这对于 10% 的排名来说，简直是有苦说不出。这项劣势在初审阶段也确实得到了验证，例如复旦和人大这两所强com学校，都是大概率因为排名的原因未通过初审。</p><h2 id="2-夏令营报名情况"><a href="#2-夏令营报名情况" class="headerlink" title="2 夏令营报名情况"></a>2 夏令营报名情况</h2><p>我在夏令营阶段共计投递 18 个院系，入营 9 个，参营 6 个（包含 1 个不发放 offer 的清华计算机系），获得 offer 4 个。具体情况罗列如下。</p><table>    <tr>        <th>院校/研究所</th><th>院系/专业</th><th>报考层次</th><th>入营</th><th>offer</th><th>备注</th>    </tr>    <tr>        <td rowspan="2">清华大学</td><td>计算机系</td><td>不区分硕博</td><td>√</td><td></td><td>不发放offer，机试前50%直通预推免</td>    </tr>    <tr>        <td>深圳研究生院-信息科学与技术学部（AI项目）</td><td>专硕</td><td>√</td><td>√</td><td></td>    </tr>    <tr>        <td rowspan="3">北京大学</td><td>计算机学院</td><td>硕士</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>智能学院</td><td>硕士</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>深圳研究生院</td><td>硕士</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>复旦大学</td><td>计算机科学与技术学院</td><td>学硕</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>上海交通大学</td><td>电子信息与电气工程学院</td><td>直博</td><td>√</td><td>√</td><td>已提前联系好导师</td>    </tr>    <tr>        <td>中国人民大学</td><td>高瓴人工智能学院</td><td>学硕</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td rowspan="2">中国科学技术大学</td><td>计算机科学与技术学院</td><td>不区分硕博</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>大数据学院</td><td>不区分硕博</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td rowspan="2">南京大学</td><td>人工智能学院</td><td>硕士</td><td>√</td><td>×</td><td>笔试未通过</td>    </tr>    <tr>        <td>计算机科学与技术系</td><td>硕士</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td rowspan="4">中科院</td><td>自动化研究所</td><td>不区分硕博</td><td>√</td><td>√</td><td>获得专硕和直博选择权</td>    </tr>    <tr>        <td>计算技术研究所</td><td>不区分硕博</td><td>√</td><td></td><td>vipl实验室（nlp方向），通过一轮考核，后放弃</td>    </tr>    <tr>        <td>软件研究所</td><td>不区分硕博</td><td>√</td><td>√</td><td>中文信息处理实验室，提前面试获得offer</td>    </tr>    <tr>        <td>深圳先进研究院</td><td>不区分硕博</td><td>×</td><td></td><td></td>    </tr>    <tr>        <td>北京师范大学</td><td>人工智能学院</td><td>不区分硕博</td><td>√</td><td></td><td>放弃参营</td>    </tr>        <tr>        <td>西湖大学</td><td>工学院</td><td>直博</td><td>√</td><td></td><td>放弃参营</td>    </tr>    </table><p>总体来说，我最担心的初审环节，似乎并没有预期那么糟糕（因为排名 10% 太拉垮），入营率 50%。最意外的是清华和自所，在报名阶段最不抱希望的两个营，结果全部通过初审（最后都获得 offer），尤其是清华，居然贵系和清深双入，可能对于清华来说，反而不是唯 GPA 论的初筛方式。最难过的是人大高瓴，本来以为稳入，结果被拒。最遗憾的是复旦，我的夏令营第一目标就是复旦的 nlp 组，结果复旦是我第一个被拒的营，直接让我的希望破灭（因为 nlp 组的名额在夏令营必定被抢光，预推免不会留坑）。</p><p>整体过程十分感慨，不到最后一刻，永远不知道会有什么惊喜。<font color=Red>那些你觉得毫无希望的营，可能没有你想象的那么难，那些你觉得稳进的营，也可能有更厉害的人把你卷下去。</font></p><h2 id="3-参营记录"><a href="#3-参营记录" class="headerlink" title="3 参营记录"></a>3 参营记录</h2><h4 id="7-2-7-3-清华大学-计算机系"><a href="#7-2-7-3-清华大学-计算机系" class="headerlink" title="7.2-7.3 清华大学 计算机系"></a>7.2-7.3 清华大学 计算机系</h4><p>清华贵系的夏令营不发 offer，只有机试环节，机试排名靠前可以获得直通预推免面试的资格。因此，我并没有把重心放在贵系上，只是抱着体验的态度去参加。贵系的机试极其硬核，算是全国高校 cs 夏令营里面独一档的难度，可以说就是为 ACMer 准备的。况且进入贵系夏令营的有很多 ACM 选手，因此我这种没有任何算法竞赛基础的，只能是当一当分母。</p><p>贵系机试共 3 道题，总时长 3 小时，满分 300。第一题比较送分，关于图论和拓扑排序，基本上 10 min AC。后两题开始坐牢，都是和树有关的题目，最后一题可能是一道树形 DP + 各种优化，我想了半天也没有想出正解，最后暴力搜索骗了一些分。最后以 130 分收场。当然，不出所料没有拿到直通预推免资格。据某绿群群友所说，大概今年需要达到 170 分左右，才可能通过机试。</p><p>结果：未获得直通预推免资格</p><h4 id="7-2-7-5-清华大学-深圳国际研究生院（信息科学与技术学部AI项目）"><a href="#7-2-7-5-清华大学-深圳国际研究生院（信息科学与技术学部AI项目）" class="headerlink" title="7.2-7.5 清华大学 深圳国际研究生院（信息科学与技术学部AI项目）"></a>7.2-7.5 清华大学 深圳国际研究生院（信息科学与技术学部AI项目）</h4><p>清深是我收到的第一个获得入营资格的夏令营，也是参加的第一个真正意义上的夏令营。在此特地“表扬”一下清深招办，在半夜三点二十发来了入营通知。果然清华这种高校半夜都是不睡觉工作的吗（？）</p><p>首先介绍一下清深，全称为深圳国际研究生院，是清华大学下属的一个院系，而不是分校区，因此你的学籍是留在清华本部的，说到底就是清华大学的学生（虽然在民间可能认可度存在差异）。清深的培养以就业为导向，地处深圳这块 IT 风水宝地，学生在研究生阶段基本以实习为主，因此就业前景十分广阔，但是科研氛围相对不够浓厚。清深信息学部的学位均为专硕（0854电子信息），无学硕，因此在报考时，需要慎重考虑是否能够接受。</p><p>清深的夏令营考核一向以硬核著称，往年据说都是组队做项目，答辩通过之后才能进入面试。初试的考核以实验室为单位，一共分为 5 个实验室，可以自由选择 1-2 个实验室参加对应的初试考核，之后组内竞争面试资格。AI 学部的实验室方向很杂，网络、机器人、知识图谱、CV等等应有尽有，因此入营的学生也很杂，来自五花八门的专业。我选择了唯一一个跟我研究兴趣匹配的实验室 —— 知识工程研究中心。该实验室也是唯一一个不用组队完成项目的，考核方式为笔试 + 机试。</p><p>7.3 全天实验室考核。笔试内容包含数学、机器学习、深度学习、AI 前沿进展等相关知识，共 16 道大题，包含简答和计算。其中，前八题为数学题，基本为概率论计算题，考察较细，难度不低。后八题是 AI 相关，知识点涉及贝叶斯分类、K-means、SVM、反向传播等，以及一些前沿知识了解，整体考察较为全面。机器学习、深度学习相关的考题对我而言基本没有什么难度，概率论相关计算题因为很久很久没有碰过了，做起来比较吃力，空了不少。最后估摸着能考到 75-80/100 的样子。机试内容三道编程题，两小时，分别为线段树、动态规划和搜索+优化（maybe），需要有一定的算法基础。机试实时公布得分排名，我最终获得了 250/300 分，排名第一。机试完成后，我基本能确定自己可以通过初试了。</p><p>7.4 晚公布了入围面试的名单，总共 90 人，入围了 54 人，通过率刚好 60%。这里忍不住批评一下清深的办事效率，大半夜十二点半发来面试通知的邮件，结果我被安排在了早八面试。好不容易熬到了通知准备睡觉，结果一点多钟接到清深的电话，提醒我明早面试……真是又好气又好笑，虽然确实很负责，但是这办事效率和办公时间也太阴间了吧……夏令营体验直接大幅度下降。</p><p>7.5 全天面试，我被分到了第一个。面试共两部分，第一部分英文考核（5min），随机抽一段英文文章，限时朗读并翻译。第二部分是导师问答，首先 5-7 min 自我介绍，之后英文+中文提问。清深的提问基本根据你的简历和初审提交的材料来进行，关于英文提问，我被问到了个人的研究兴趣以及我的论文中的一个名词解释，中文提问相对没有很硬核，会偏向于聊天的形式。几个老师围在一张桌子前，轮流发问，提问内容包括但不限于一些科研细节、研究计划、有没有报其他地方的夏令营等，以及一个老师看我研究 nlp，问了我一个 BERT 相关的问题，我基本上都能对答如流。但是面试过程中有一段让我印象深刻，一个老教师对着我的成绩单，问我军事理论这门课为什么考这么低（问完之后其他老师都笑了）。当时愣了一下，没想到会被问到这个奇怪的问题，很快意识到可能被压力面了，于是整理了一下思路从容交待。之后他又看到我的成绩单上有三门离散数学课，然后就问了我一个群论的问题，我如实回答说没学过，并解释了一下这三门离散分别学了什么。老师很讶异，又问了我一个图论问题，让我解释一下“欧拉树”。结果这个概率我又刚好没学过…于是只能很尴尬地承认也没学过。之后老师就没有再追问下去。</p><p>虽然有这么一个尴尬的小插曲，但是整体面试很顺利，我跟老师之间的交流比较顺畅，没有任何磕绊的地方。面试完当天下午，意外的收到了知识工程研究中心发来的消息，向我抛出了橄榄枝，询问我去清深的意愿，并且愿意给我本部 tj 老师的招生名额。过了大概两周，tj 老师组那边亲自联系了我，说我在夏令营中的表现特别好，让我进组一段时间，看看双方是否合适。然而，…</p><p>总体来说，清深 AI 的优营率不算高，估计在 20% - 30% 的样子。不得不说，清深的报名网站是我见过的所有报名系统中 UI 设计最美观的（不愧是清华大学），最后还有一个很像 928 系统确认的这么一个确认环节，感觉很有仪式感hhh。</p><p><img src="/pic/%E4%BF%9D%E7%A0%94/%E6%B8%85%E6%B7%B1%E9%A2%84%E5%BD%95%E5%8F%96.jpg" alt="清深AI预录取资格"></p><p>结果：获得预录取offer</p><h4 id="7-7-南京大学-人工智能学院"><a href="#7-7-南京大学-人工智能学院" class="headerlink" title="7.7 南京大学 人工智能学院"></a>7.7 南京大学 人工智能学院</h4><p>南大 AI 是我唯一一个参营但未通过考核的院系。南大 AI、CS 和 SE 都是首先需要进行笔试，通过笔试初筛才能够进入面试环节。南大是出了名的海王营，笔试的作用其实就是帮助老师进行一个初筛的过程。南大在夏令营初审阶段会放大量的学生入营（就这样我还没入 CS 营），之后统一笔试。</p><p>笔试由大量的选择题和若干填空简答构成，总时长 90min。涉及知识点包括线代、概率论、数理统计、数据结构、机器学习、深度学习等若干科目，其中不乏概念理解题和计算题，要求知识点掌握必须全面。整体来说难度不小，也基本无从准备。</p><p>两三天后出结果，发现没收到笔试通过的邮件，G 了，属实意料之外。不过本身并没有太想去南大 AI，也就作罢。</p><p><strong>结果：笔试一轮游，未进入面试阶段</strong></p><h4 id="7-8-7-10-上海交通大学-电子信息与电气工程学院"><a href="#7-8-7-10-上海交通大学-电子信息与电气工程学院" class="headerlink" title="7.8-7.10 上海交通大学 电子信息与电气工程学院"></a>7.8-7.10 上海交通大学 电子信息与电气工程学院</h4><p>上交的夏令营算是华五里面最难入营的一个，据说每年直硕只招 985 rank 1-2。因此，有自知之明的我直接放弃了直硕的想法（因为排名只有 10%），而是提前联系了一位 nlp 方向的青椒，并点明了跟随他读博的意愿。在六月份的时候，我和老师联系过 3-4 次，在一些学术问题上有过一些简单的 proposal 交流（因为是直博，老师希望双方提前对彼此都有更深的了解）。交流几次之后，老师对我的个人情况以及能力都比较满意，也承诺说愿意在夏令营阶段把我捞上岸，希望双方能够成功匹配上。</p><p>上交的直博面试相对偏重为一个形式，因为需要与导师达成双选后，学院才会颁发优营，而面试只需要 60 分及格，就可以进入师生双选阶段。因此，上交直博的关键在于联系导师，导师直接决定你是否获得 offer。面试主要针对英语和科研经历提问，一个面试组中，对你的研究领域不熟悉的老师可能会偏重于问专业课，对你的领域熟悉的老师会深究科研细节。面试一共 20min，首先是 5min 带 PPT 的自我介绍，介绍结束后是英文问答，问到了我三个关于论文的问题，分别是论文的 reaseach direction、novelty 和 difficulty。</p><p><strong>结果：进入师生双选环节</strong></p><h4 id="7-11-7-15-中科院自动化研究所"><a href="#7-11-7-15-中科院自动化研究所" class="headerlink" title="7.11-7.15 中科院自动化研究所"></a>7.11-7.15 中科院自动化研究所</h4><p>自所是我认为办的最好的一个夏令营，它真的是按照“夏令营”的模式来办的。前三天是各个实验室导师代表的讲座，介绍各实验室的情况以及研究进展。同时，还安排了每个实验室单独的师生交流会，以及自所学生的经验分享会。总之是干货满满。</p><p>自所是典型的强 com 院校，入营全凭自身简历。今年据说一共报名了 2600 多人，入营约 300 个，形势基本持平往年。但是自所的优营率很高，据说前 40 % 学硕、直博和专硕任选，40% - 60% 只能专硕，60% - 80% 候补，最后 20 % 淘汰。再加上入营之后会有一些人直接放弃考核，因此只要不是过于拉垮，基本都能拿到 offer。自所的考核形式也是比较特殊。首先将所有学院分为若干组，然后组内竞争名额，每个组内学生的院校背景比较平均。我们组一共 27 人，其中 24 个 985，剩余 3 个 211，且院校基本无重复。</p><p>最后两天是面试时间，每个人 12min，包括 1min 自我介绍，3min 数学英语测试，8min 专业问答。之前一直听说自所很喜欢问数学，而且问的比较细，但实际面试时发现并没有想象中那么可怕。我抽到的数学问题是列举一些离散型概率分布，以及举一个伯努利分布的例子，还算是比较平稳。专业问答环节基本围绕简历提问，问到一些科研细节，以及在科研方面的开放型问题，一些见解和看法。同时，也会就你的竞赛经历进行一些提问，我的简历上因为写到了蓝桥杯，所以问了我一个问题“什么是动态规划”。除此之外，没有问到任何专业知识的内容，剩下就是简单的唠家常环节。</p><p>大约五天之后出了结果，拿到了直博和专硕的任选权。其实对于中科院来说，个人认为直博比硕士更香。因此，我并不遗憾没有拿到学硕资格。</p><p><strong>结果：获得直博/专硕 offer</strong></p><h4 id="7-18-7-22-中科院软件研究所"><a href="#7-18-7-22-中科院软件研究所" class="headerlink" title="7.18-7.22 中科院软件研究所"></a>7.18-7.22 中科院软件研究所</h4><p>虽然名字叫软件所，但是其中不乏一些研究软件之外的实验室，例如比较出名的中文信息处理实验室（以下简称“中文实验室”），每年都是软件所最火的实验室之一。这也是我此行唯一的目的。早在五月初，中文实验室便开放了报名通知，我也是早早地投出了简历。之后过了两个月，直到七月初，才收到实验室的考核通知，首先第一轮是机试环节。</p><p>7.4 参加了第一轮机试，在牛客平台上进行。机试一共 8 道题，总时长 90 min，分值为两题 10 分，三题 30 分，三题 40 分，计分规则为选取最高得分的三题计入总分。机试难度适中，涉及动态规划、二叉树、单调栈、大模拟等若干知识点，整体思维量较小，对算法要求不算很高。我在前 70 min 拿下了 110 分，就直接退出考试了。当然，很顺利地通过了机试。</p><p>大约有 12 人通过了一轮机试，进入到第二轮的面试环节。7.11 参加了第二轮面试，面试分为两个阶段，第一阶段是导师问答，首先用 PPT 进行一个五分钟的自我展示，之后导师提问，一般是对着简历上的内容问一些细节，之后也会唠一些家常，询问一些读研读博的意愿之类的，就我而言没有问到任何专业课知识。第二阶段是手撕代码，考核老师给一道题目，要求先叙述思路，若思路正确，则共享电脑屏幕，用任意语言在任意 IDE 上实现具体代码。问到我的是一个数组划分问题，将一个数组分成两个子数组，要求两个子数组各自的和之差最小。简单思索之后，我想出了一个类似 0-1 背包的解法，整体还算比较顺利。</p><p>结果在第二天下午收到了实验室学长的微信消息，说是实验室的 s 老师想找我面谈（一开始以为我在北京，想约我线下）。当时就感觉基本稳了，果不其然，s 老师先是给我介绍了一些实验室的基本情况，之后让我随便提问，以增进了解为目的，并且明确表示了给我发放 offer 的意愿。在聊天过程中我也了解到，第一轮面试中的 12 个人里面，实验室只选出了 2 个人通过了考核，这也让我十分惊喜，感觉自己的能力得到了老师们的一致认可。</p><p>后续 7.18 开营，我就直接免去了机试和面试的环节。当然，根据实验室要求，在 7.20 的时候补测了一场笔试，其实是英语考核，包含阅读、写作等内容。总之，我的软件所之旅在开营之前，就已经完美收官了。</p><p><strong>结果：提前获得实验室 offer，夏令营免试</strong></p><h4 id="7-19-7-22-中科院计算技术研究所"><a href="#7-19-7-22-中科院计算技术研究所" class="headerlink" title="7.19-7.22 中科院计算技术研究所"></a>7.19-7.22 中科院计算技术研究所</h4><p>计算所的考核方式很特别，采用实验室考核制。因此，这是一个典型的弱 com 院校，只要获得了实验室导师的考核邀请，甚至可以不用入营，也能够参与考核。当然，这就要求提前联系好导师，因为导师话语权巨大。</p><p>在填报夏令营时，除了填报系统之外，还会让你填报两个意愿导师，并且榜单实时更新公示。因为我是 nlp 方向的，因此选择了计算所唯一一个做 nlp 的 fy 老师。事实证明，物以稀为贵，再加上 vipl 实验室自身的加成，fy 老师成为今年最火爆的导师，填报人数超过 70 人（可能更多）。</p><p>不过很荣幸收到了 vipl 实验室的考核通知，最后参与考核的人数为 35 人。事实证明，强组不仅考核难度大，考核过程还十分折腾。下面展示一下具体考核流程。</p><p><img src="/pic/%E4%BF%9D%E7%A0%94/%E8%80%83%E6%A0%B8%E5%AE%89%E6%8E%92.png" alt="vipl-nlp方向 考核安排"></p><p><img src="/pic/%E4%BF%9D%E7%A0%94/%E8%80%83%E6%A0%B8%E5%86%85%E5%AE%B9.png" alt="vipl-nlp方向 考核内容"></p><p>7.19 先参加了第一轮笔试，考核内容为数学90min + 算法60min + 英语60min。数学考试共六个大题，前两题比较送分，一个特征值计算，一个概率计算。后面四道题开始上难度，据我个人回忆分别是严格最值点证明、随机游走、梯度恒等式证明、矩阵范数最值等相关问题，对数学水平要求很高，绝不仅仅是做一些作业题或者考研题就能掌握的，需要较高的数学能力来支撑。总之我在考场上做懵了，后四题完整地写了两题（也不知道正确与否），还有两题基本没动。</p><p>算法考核给了五个题目，要求分别写出算法思路，以及相应的时间复杂度。感觉难度分化比较大，涉及贪心、动态规划、图论、二分等若干知识点。前两题也是基本送分，后三题需要较强的算法基础和思维能力。我在考场上一共做出了前三题，后两题纯暴力写法，时间复杂度直接上天。</p><p>英语考核共三部分，第一部分翻译，一个中译英，一个英译中。第二部分摘要，给出了一段论文段落，要求写一个 200 词的英文摘要，和 400 字的中文摘要。给的那篇文章是生物信息学相关的，因为没有相关知识基础，只能逐词逐句硬翻，这也是我耗时最多的一个题目。最后一部分是用 150 词的英文介绍自己上过的一门课。因为平时经常读英文文献的缘故，这样的开放式写作对我来说基本上问题不大，10 min 就完成了写作。</p><p>总体而言，数学和算法难度大，英语题量大，题型新，没有一门是顺利完成的。第一轮笔试应该是刷掉一半的人进入下一轮，当天晚上接到通知，成功通过了第一轮考核。然而不幸的是，由于第二天上午软件所笔试的冲突，我不得不在两者之间进行选择。因为此时我已经拿到了中文信息实验室的offer，如果这时候鸽掉，未免过于遗憾。于是我做了一个决定，放弃了后续 vipl 的考核，中途弃赛退出。</p><p>至此，我的夏令营之旅全部结束。</p><p><strong>结果：通过第一轮笔试，后放弃考核</strong></p>]]></content>
      
      
      <categories>
          
          <category> 保研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研夏令营 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督学习</title>
      <link href="/2022/07/01/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2022/07/01/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅谈自动摘要生成任务</title>
      <link href="/2022/07/01/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/"/>
      <url>/2022/07/01/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/Automatic-Summarization.png" alt="自动文本摘要生成"></p><h2 id="1-抽取式摘要"><a href="#1-抽取式摘要" class="headerlink" title="1 抽取式摘要"></a>1 抽取式摘要</h2><blockquote><p>抽取式摘要任务本质上已经变成了一个序列标注任务，即对每个句子打标签，判定这个句子到底要不要被放在摘要里面。当然，这里的标注不一定是标注整个句子，也可以是一些更细粒度的特征，后面会介绍几个相应的算法。对于通用模型架构而言，首先是 encoder，经过句子级别的 encoder 和文档级别的 encoder，获得原文句子和文章级别的 embedding 表征。之后是 decoder，利用输出的摘要语句和原文的语义编码，来映射到对应的序列标注，获取最终抽取的结果。</p></blockquote><p><img src="/pic/summarization/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.png" alt="序列标注任务：encoder-decoder 架构"></p><h3 id="1-1-基于句子的抽取"><a href="#1-1-基于句子的抽取" class="headerlink" title="1.1 基于句子的抽取"></a>1.1 基于句子的抽取</h3><p>比较常用的是 RNN 和 Transformer 的架构。RNN 的这篇文章在构建文档级别的语义表征的时候，不仅利用了句子级别的含义表征，还掺杂了一些其他的元素，比如句子的显著性、新颖性、绝对和相对位置等因素，通过词嵌入的形式加入在最终的表征里面。之后 Transformer 出现，Transformer 就取代了 RNN 成为了新的主流架构。Bertsum 这篇文章，是利用 Bert 模型在摘要任务上面做的微调。它是将每个句子开头的 CLS 字符的词嵌入作为整个句子的表征，经过 BERT 输出之后，新建了一个摘要层，用来微调抽取式摘要任务，它里面给了 MLP、RNN、Transformer 三种类型的架构。这也是Transformer 架构用在抽取式摘要上的一个主流方法，直到现在很多做抽取式摘要的还把它作为 baseline 来对比实验。</p><p><img src="/pic/summarization/RNN+transformer.png" alt="（左）SummaRuNNer，即 RNN 架构；（右）BertSum，即 Transformer 架构"></p><p>这是两个主流架构，并没有引入过多的摘要任务的归纳偏置，之后的很多研究也继续在抽取句子这个级别上做了很多延展性的工作，主要还是基于<font color=Red>如何表征句子的语义</font>，让句子表征蕴含更多有价值的信息。我们可以看下这篇文章，这是 20 年 ACL 上的一篇抽取式摘要的文章，它仍然是对原文中句子层面的内容进行的抽取，但是在进行句子表征时，加入了一个叫做关键词的内容。因为一个句子中肯定存在很多不重要的信息，关键的东西就那么几个，比方说人物、地点之类的，那它的思路就是要把每个句子中的关键词信息充分地融在句子表征当中，首先构建了一个叫做 nerual topic 的 model 来提取句子中的关键词，之后对关键词和句子构建图模型，最后利用图神经网络对图中的句子结点进行分类，获得句子的序列标注。近两年来这种图模型的方法不断地涌现出来，包括复旦大学之前做过几篇也是用图来构建摘要文本的，这里不赘述。图模型对于文本及其之间的关联，能够非常好的表现出来。</p><p><img src="/pic/summarization/graphsum.png" alt="GraphSum 模型架构"></p><h3 id="1-2-基于子句的抽取"><a href="#1-2-基于子句的抽取" class="headerlink" title="1.2 基于子句的抽取"></a>1.2 基于子句的抽取</h3><p>句子层面的抽取做到这个地步很难继续深入了，所以相关研究人员开始不满足于句子的抽取，把思路转向了更加细粒度的方法，比如说将一个句子拆分若干子句，然后标注子句是否被提取。这里介绍两个模型，一个是 DiscoBert，这个模型的思想跟上面 GraphSum 比较像，也是构图，只不过那个是构建的句子中的关键词和句子之间的图，并且只对句子结点进行标注，但这个模型是把句子拆成子句，然后构建子句之间的图模型，并且对子句进行了序列标注，在粒度上更细了一步。</p><p>它的子句采用了 RST 树来进行提取，是一种句法分析树，利用句法分析来获取子句之间的关系。之后还是先用 Transformer 对每个子句进行编码，然后把图模型放进图神经网络里面进行结果预测，得到最终每个子句的标注。</p><p><img src="/pic/summarization/DiscoBert.png" alt="DiscoBert 模型架构"></p><p>同样的，还有一个 SSE 模型，也是做了子句粒度上的抽取。这个似乎更加简单，它甚至没有构图，纯粹是用句法树抽了一下子句（基于 Penn Treebank），然后放进 Transformer 里面做了一个二分类任务。</p><p><img src="/pic/summarization/SSE.png" alt="SSE 模型架构：（左）子句抽取；（右）二分类编码器"></p><p>所以我们发现，如果这个提取的粒度是介于词语和整句之间的，大多数现有的工作都是通过一些基础性的句法分析模型来获取句子结构，之后构建子句之间的联系来回归到原有的 RNN 或者 Transfomer 框架之中。</p><h3 id="1-3-混合粒度抽取"><a href="#1-3-混合粒度抽取" class="headerlink" title="1.3 混合粒度抽取"></a>1.3 混合粒度抽取</h3><p>当然，如果粒度更细一点的话，可能就不需要做这些预处理的工作（句法分析等），比如说这一篇名叫 swap-net 的工作，它是考虑了句子粒度和词语粒度这两个混合粒度。也就是说，在抽取的摘要中，可能会同时包含原文中的整句和词语，因为原文中可能会有一些很重要的关键词，如果放在整句当中，它的重要性可能不会那么显著，甚至会被忽视，为了避免这种现象的出现，它在 Decoder 解码时设计了一个交换机制。每一步的解码进行一个判定，判定抽取原文中的句子还是词语。如果抽句子，给原文中的所有句子输出一个概率分布，然后选取概率最高的句子，如果抽词语，同理给词语输出概率分布。这样一个交换机制，能够使得抽取的摘要在保证语法和逻辑完整的同时，嵌入了更多重要的关键词。</p><p><img src="/pic/summarization/swapnet.png" alt="Swap-net 模型架构"></p><h3 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h3><p>总结了这些关于抽取式摘要的方法，不管是抽整句也好，还是抽子句、抽词语，总之现在的方法正在不断地往更细粒度的方向去发展，更加注重模型对于句间逻辑关系，表述灵活性等等方面的表征。但是我在调研的时候并没有发现纯抽词语的方法，就算是抽词语，像上面提到的swap-net，也是边抽词语边抽句子的。</p><p>究其原因，个人认为，首先抽取式摘要相比较生成式摘要的一个最大的优势，便是我们从原文中抽取出的句子，至少是合乎语法并且逻辑通顺的，唯一需要处理的是句子和句子之间的关联是否足够强，以及信息冗余的问题，这是生成式摘要所具有挑战性的任务。生成式摘要由于是从词表里面抽词语，所以拼成的句子合不合语法与逻辑还要另说。那对于抽取式摘要来说，如果单抽词语，那它的最大优势，也就是合乎语法以及句内逻辑通顺，就利用不上了。另一个是，如果变成了单抽词语，抽取式摘要这个问题就退化成了生成式摘要问题，甚至应该叫进化，但是进化的不完全。因为生成式摘要是从一个很庞大的外部词表里面去抽词语，但是抽取式摘要是从原文包含的词语构成的词表里面去抽，从这点上来看，抽取式摘要的灵活性就被大大局限住了。虽然可能对原文中的关键词抽取的会更加准确，但是从整个摘要级别来看，可能整个语句的组织和含义会非常受限，语法也不能够得到保证。所以单做词语级别的抽取式摘要，不如直接做生成式摘要。</p><p>因此，它为生成式摘要方法也算是提供了一些启发，因为原文中的词语构成的词表，对于摘要来说一定是一个核心词表，单从外部词表来抽词语的话，可能会提取不清原文中的关键词。所以就出现了生成式摘要中的一个非常经典的架构，叫做指针生成网络。这个网络它不仅抽取外部词表中的词语，还有一定的概率回过头抽取原文中的关键词，可以说是利用到了抽取式摘要的一些特性，最大限度地保留住了原文中的一些琐碎的信息。</p><h2 id="2-生成式摘要"><a href="#2-生成式摘要" class="headerlink" title="2 生成式摘要"></a>2 生成式摘要</h2><h3 id="2-1-seq2seq-方法-——-初代指针网络"><a href="#2-1-seq2seq-方法-——-初代指针网络" class="headerlink" title="2.1 seq2seq 方法 —— 初代指针网络"></a>2.1 seq2seq 方法 —— 初代指针网络</h3><p>最早的指针-生成网络是在 16 年被提出的，当时的生成式模型主要是基于 RNN 模型。纯 RNN 模型的思路就是，每次解码的时候，都从外部词表中选取最大概率的那一个，作为当前步生成的词语。但是这就带来一个问题，原文中的关键词可能会被忽视掉，一些细节很难被保留。于是出现了指针生成网络。<font color=Red>指针指的是指向原文中的关键词，生成就是从外部词表中选取词语。</font>这篇文章里面除了指针网络还给了这样几个技巧。一个是缩减了词表，只保留了一些高频词和原文中的词语，还有是在词向量中嵌入了一些语言学特征。这些方法对于摘要任务的速度和精度上都有所提升。因为我们知道，相比于抽取式摘要，生成式摘要最大的缺点，一个是训练速度慢，一个是可能语法逻辑不通顺。</p><blockquote><p>Trick 1：LVT 方法 —— 考虑到摘要的多数词来自原文，采用 LVT 方法，用于缩减 decoder 词汇表，只保留一定数量的高频词和原文所包含的词。这样做大大降低了decoder的soft-max计算耗时，并且加速模型收敛(模型只需关注核心词)。<br>Trick 2：词向量融合语言特征 —— 词嵌入中融入了一些语言学特征，包括NER，TF，IDF，以及词性POS。转为离散值，用one-hot向量表示，与词向量一起拼接为一个较长的向量。<br>Trick 3：指针-生成转换器 —— Decoder中，G表示generator(基于Seq2seq生成一个词)，P表示pointer(直接copy原文中的一个词)。当switch开关为1时，采用generator；当switch开关为0时，采用pointer。pointer计算Attention分布，基于Attention分布生成一个pointer位置指针，直接copy原文中与位置指针对应的词即可。</p></blockquote><p><img src="/pic/summarization/%E5%A2%9E%E5%BC%BA%E7%89%88RNN.png" alt="基于 RNN 的 seq2seq 模型"></p><p>指针生成网络的核心思想在于，既能够抽取外部词表中的词语，又能够抽取原文中的词语，这样能够最大限度地去锁住原文中的关键信息。最早的指针生成网络机制是，在每一步解码的时候，有一个 switch机制，先去判定是生成原文中的词语，还是从词表里面选择词语，这就有点像刚刚提到的 swap-net 模型，因为它也是解码的时候用 switch 机制来判定输出整句还是输出词语。之后利用attention 权值来选择最大概率的词语。</p><p><img src="/pic/summarization/%E5%A2%9E%E5%BC%BA%E7%89%88RNN2.png" alt="初代指针网络模型架构"></p><h3 id="2-2-增强版指针-生成网络"><a href="#2-2-增强版指针-生成网络" class="headerlink" title="2.2 增强版指针-生成网络"></a>2.2 增强版指针-生成网络</h3><p>但是初代的指针网络，在选择指针和生成器的时候是分离开来的。也就是说，模型会先判断用外部词表还是原文词表，之后就只盯着某一个具体的词表去抽。在这个基础上，改良版本的指针网络被提出，这个方法的改进之处在于，模型同时去考虑外部的词表和原文的词表，把这两个词表生成词语的概率做一个加权的叠加，来选择最终的生成词语。</p><p>改进后的指针-生成网络，可以看到，每一步解码的时候，原文中的每个词语都有一个 attention 权重，外部词表也会有一个生成的概率分布。之后，通过设置一个概率 p，来衡量到底是多考虑一些原文中的词语，还是外部的词语，将两个概率分布相加之后，得到最终的概率分布。如果是未登录词，概率就设置为 0。相比较最初的指针生成网络而言，这样的生成方式可能会加入一些综合考量的因素在里面，能够在保留原文关键信息的同时，让整个生成的语句更加的连贯，这比单独考虑某一个词表会更好一些。</p><p><img src="/pic/summarization/pointernet.png" alt="改良版指针-生成网络架构"></p><p>之后还提到了一个叫汇聚机制（coverage）的小 trick，它的目的主要是消除一些生成词语的重复现象，比如说再前面某个关键词被提取出来了，尽管很重要，但是他不能被一直重复提取，一是冗余，二是可能会使得其他一样也比较重要的信息被忽略了，目光只盯着这一个关键点了。所以说这个机制就是累加了之前所有的 attention 的得分，并且设置一个惩罚机制，惩罚你提取重复的单词。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 文本摘要生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 —— BERT 预训练语言模型</title>
      <link href="/2022/06/19/bert/"/>
      <url>/2022/06/19/bert/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/bert.png" alt="Google 公司推出的 Bert 预训练语言模型"></p><blockquote><p>本文所探讨的论文标题为 《BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding》。BERT 可以称作是预训练语言模型的开山之作了，和 OpenAI 的 GPT 模型是同时期的产物，但性能优于 GPT。BERT 模型基于 Transformer 架构实现，是一种全新的双向编码器语言模型。与ELMo、GPT等单向语言模型不同，BERT 旨在构建一个双向的语言模型，来更好地捕获语句间的上下文语义，使其在更多的下游任务上具有更强的泛化能力。因此，预训练完成的 BERT 模型被迁移到下游任务时，只需要在添加一个额外的输出层便可以进行微调，例如问答和语言推理任务，并不需要针对具体的任务进行模型架构的修改。BERT 模型在概念上简单却具有强大的性能，它在11项经典的自然语言处理任务上取得了最优的效果，包括将 GLUE 数据集的得到提升至80.5%（相比于之前的最优模型提升了7.7%），将SQuAD v1.1问答测试数据集的 F1 值至 93.2（提升了1.5个点），以及将SQuAD v2.0 数据集的 F1 值到 83.1（提升了5.1个点）。</p></blockquote><h2 id="1-研究概述"><a href="#1-研究概述" class="headerlink" title="1 研究概述"></a>1 研究概述</h2><h3 id="1-1-研究背景"><a href="#1-1-研究背景" class="headerlink" title="1.1 研究背景"></a>1.1 研究背景</h3><p>大规模标注语料库的匮乏，成为了制约NLP（Natural Language Processing）领域发展的一大重要因素。为了使NLP模型能够充分地利用海量廉价的无标注数据信息，预训练语言模型（Pre-trained Models, PTMs）应运而生。通过模型预训练，我们可以从海量数据集中初步获取潜在的特征规律，再将这些共性特征移植到特定的任务模型中去，将学习到的知识进行迁移。具体来说，我们需要将模型在一个通用任务上进行参数训练，得到一套初始化参数，再将该初始化模型放置到具体任务中，通过进一步的训练来完成更加特殊的任务。预训练模型的推广，使得许多NLP任务的性能获得了显著提升，它为模型提供了更好的初始化参数，大大提高了其泛化能力。至此，NLP领域进入了一个新的研究阶段。</p><h3 id="1-2-问题分析与解决"><a href="#1-2-问题分析与解决" class="headerlink" title="1.2 问题分析与解决"></a>1.2 问题分析与解决</h3><p>当前的预训练模型主要分为基于特征和微调两大类，但它们大都基于单向的语言模型来进行语言学习表征，这使得许多句子级别的下游任务无法达到最优的训练效果。因此，本文提出了名为BERT的双向预训练表征模型，很大程度上缓解了单向模型带来的约束。同时，引入了“完形填空”和“上下句匹配”分别作为单词级别和句子级别的两大通用任务，对BERT模型进行训练。实验表明， BERT模型的应用使得当前的11个NLP任务均取得了SOTA的效果。</p><h3 id="1-3-相关工作"><a href="#1-3-相关工作" class="headerlink" title="1.3 相关工作"></a>1.3 相关工作</h3><h4 id="1-3-1-基于特征的无监督方法"><a href="#1-3-1-基于特征的无监督方法" class="headerlink" title="1.3.1 基于特征的无监督方法"></a>1.3.1 基于特征的无监督方法</h4><p>基于特征的方法主要是指单词嵌入表征学习。首先将文本级别的输入输出为特征向量的形式，再将预训练好的嵌入向量作为下游任务的输入。</p><p>词嵌入向量[1-5]是单词表征学习的最细粒度。通过统计学习或深度学习方法，文本中的单词被映射至向量空间中的密集向量。随着人们对于文本连贯性的关注，句子[6-7]和段落[8]级别的嵌入表征被提出，更多的数据特征被获取，进一步提升了预训练效果。相比于从头开始的词嵌入训练，预训练的引入对于各类任务的性能具有显著的提升效果。 </p><p>上述模型均从单词拼写的层面进行了表征学习，并没有考虑单词在句中的使用形式。Matthew Peters等人在此基础上提出了名为ELMo[9]的语境字词嵌入表征法，该模型会根据句子的上下文，对同一个单词返回特定语境下不同的嵌入表征。在一些NLP基准任务上[10]，例如情感分析[11]、问答系统[12]、命名实体识别[13]，ELMo均取得了最优性能。这也是NLP领域中第一个开始关注上下文的预训练模型，为本文BERT模型的提出奠定了坚实的基础。</p><h4 id="1-3-2-基于微调的无监督方法"><a href="#1-3-2-基于微调的无监督方法" class="headerlink" title="1.3.2 基于微调的无监督方法"></a>1.3.2 基于微调的无监督方法</h4><p>基于微调的方法主要是指，我们在某些通用任务上预训练完成的模型架构，可以被直接复制到下游任务中，下游任务根据自身需求修改目标输出，并利用该模型进行进一步的训练。也就是说，下游任务使用了和预训练相同的模型，但是获得了一个较优的初始化参数，我们需要对这些参数进行微调，从而在特殊任务上获得最优性能。基于该方法，Alec Radford等人提出了OpenAI GPT[14]模型，它在许多句子级别的任务上获得了SOTA效果。</p><h4 id="1-3-3-基于有监督数据的迁移学习"><a href="#1-3-3-基于有监督数据的迁移学习" class="headerlink" title="1.3.3 基于有监督数据的迁移学习"></a>1.3.3 基于有监督数据的迁移学习</h4><p>我们也可以基于存在大量有监督数据集的任务来获取预训练模型，例如自然语言推理和机器翻译。预训练的思想也被广泛应用到CV领域， Jason Yosinski在ImageNet数据集[15]上获取的预训练模型[16]，在许多下游任务中都取得了较优的性能。</p><h2 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2 解决方法"></a>2 解决方法</h2><h3 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h3><p>在BERT出现之前，已有的预训练语言模型大多为单向模型架构。例如OpenAI 推出的GPT模型[14]，便是引入了Transformer Decoder层[2]中的掩码注意力机制，使得模型能够充分学习上下文语义。然而，单向模型架构仍然限制了预训练模型在NLP任务上的泛化能力，诸多NLP任务难以从单向架构中学习到更多有用的特征，例如问答系统[12]。因此，需要继续对当前的预训练架构进行优化，使得其能够适应更多种类的任务，增强其在NLP领域的通用性。</p><h3 id="2-2-创新思想"><a href="#2-2-创新思想" class="headerlink" title="2.2 创新思想"></a>2.2 创新思想</h3><p>BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，它创造性地将Transformer中的Encoder架构引入预训练模型中，成为第一个使用双向表征的预训练语言模型。同时，为了适应该双向架构，BERT引入了两项新的NLP任务——完形填空和上下句匹配，来捕获词语级别和句子级别的表征，并使之具有更强的泛化能力。</p><h3 id="2-3-具体方法"><a href="#2-3-具体方法" class="headerlink" title="2.3 具体方法"></a>2.3 具体方法</h3><p>BERT整体框架包含Pre-training和Fine-tuning两个阶段，如图2.1所示。Pre-training阶段,模型首先在设定的通用任务上，利用无标签数据进行训练。训练好的模型获得了一套初始化参数之后，再到Fine-tuning阶段，模型被迁移到特定任务中，利用有标签数据继续调整参数，直至在特定任务上重新收敛。</p><p><img src="/pic/bert/%E6%9E%B6%E6%9E%84.png" alt="BERT的pre-training和fine-tuning架构"></p><h4 id="2-3-1-模型架构"><a href="#2-3-1-模型架构" class="headerlink" title="2.3.1 模型架构"></a>2.3.1 模型架构</h4><p>BERT模型采用了Transformer中的Encoder架构，通过引入多头注意力机制，将Encoder块进行堆叠，形成最终的BERT架构。为了适应不同规模的任务，BERT将其结构分为了base和large两类。较小规模的base结构含有12个Encoder单元，每个单元中含有12个Attention块，词向量维度为768；较大规模的large结构含有24个Encoder单元，每个单元中含有16个Attention块，词向量维度为1024。通过使用Transformer作为模型的主要框架，BERT能够更彻底地捕获语句中的双向关系，极大地提升了预训练模型在具体任务中的性能。</p><p>BERT 模型的输入由三部分组成。除了传统意义上的 token 词向量外，BERT 还引入了位置词向量和句子词向量。位置词向量的思想与 Transformer 一致，但 BERT 并未使用其计算公式，而是随机初始化后放入模型一同训练；句子词向量实质上是一个0-1表征，目的是区分输入段落中的上下句。这三种不同意义的词向量相加，构成了最终输入模型的词向量。</p><p><img src="/pic/bert/%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5.png" alt="BERT的输入格式"></p><h4 id="2-3-2-预训练"><a href="#2-3-2-预训练" class="headerlink" title="2.3.2 预训练"></a>2.3.2 预训练</h4><p>BERT的预训练（pre-training）部分使用了完形填空和上下句匹配两项无监督任务。“完形填空”代表了词语级别的预训练任务，该任务对输入句子中若干随机位置的字符进行遮盖，并利用上下文语境对遮盖字符进行预测。“上下句匹配”代表了句子级别的预训练任务，该任务给出两个句子，利用句子之间的语义连贯性判定这两个句子是否存在上下句关系。这两项预训练任务对于大量NLP任务的架构具有更好的代表性，同时也更能匹配模型本身的双向架构，对模型的泛化能力有着巨大的提升帮助。</p><h4 id="2-3-3-微调"><a href="#2-3-3-微调" class="headerlink" title="2.3.3 微调"></a>2.3.3 微调</h4><p>训练具体任务时，我们只需将具体任务中的输入输出传入预训练完成的 BERT 模型，继续调整参数直至模型再次收敛。该过程称为微调（fine-tuning）。相比于预训练来说，微调的代价是极小的。在大部分NLP任务中，我们只需要在GPU上对模型进行几个小时的微调，便可使模型在具体任务上收敛，完成训练。</p><h2 id="3-实验分析与结论"><a href="#3-实验分析与结论" class="headerlink" title="3 实验分析与结论"></a>3 实验分析与结论</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>本文将BERT模型迁移至11个NLP基准任务上进行了微调训练，均取得了SOTA的效果。另外，为了探究模型的不同组成部分对整体性能的影响，本文还进行了若干消融实验，对BERT的预训练任务、模型规模等要素进行了实验评估，充分论证了双向模型的重要性。</p><h3 id="3-2-数据集和主实验分析"><a href="#3-2-数据集和主实验分析" class="headerlink" title="3.2 数据集和主实验分析"></a>3.2 数据集和主实验分析</h3><p>BERT共实现了对于11个NLP基准任务的微调训练，共对应4个数据集。本部分将详细描述各个数据集及其对应的基准任务，并介绍每个数据集上的参数设置和实现细节，以及对主实验的结果进行简要分析。</p><h4 id="3-2-1-GLUE"><a href="#3-2-1-GLUE" class="headerlink" title="3.2.1 GLUE"></a>3.2.1 GLUE</h4><p>GLUE[17]数据集共收集了包含自然语言推理、语义相似性判断等任务在内的9项NLP基准任务，并与OpenAI GPT、ELMo等性能较优的基准模型进行了结果对比。实验微调了3个epoch，将batch size设置为32，并利用验证集选择最佳学习率。实验结果如表3.1所示，结果表明，相较于当前性能最优的模型，BERTBASE 和 BERTLARGE 模型在所有任务上的性能表现均获得了较为可观的提升，平均准确度分别超过SOTA结果4.5%和7.0%。同时，BERTLARGE 在所有任务上的性能均超出了BERTBASE ，且在少样本数据集上的性能尤为突出。</p><p><img src="/pic/bert/GLUE%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.1 GLUE数据集（九项自然语言理解或生成任务）的实验结果"></p><h4 id="3-2-2-SQuAD-v1-1"><a href="#3-2-2-SQuAD-v1-1" class="headerlink" title="3.2.2 SQuAD v1.1"></a>3.2.2 SQuAD v1.1</h4><p>SQuAD v1.1[12]是一个问答任务数据集，共收集了100k组问答语句对。给定一个问句和一个包含答案的文段，需要提取出文段中该问句对应正确答案的文本范围。实验微调了3个epoch，将batch size设置为32，并将学习率固定为5e-5。实验结果如表3.2所示，结果表明，对于集成模型和单一模型这两种框架而言，BERT相比于现有的最优模型在F1指标上分别获得了1.5%和1.3%的提升，且BERT单一模型的性能甚至超过了当前最优的集成模型的性能。</p><p><img src="/pic/bert/SQuADv1.1%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.2 SQuAD v1.1数据集（基础版问答任务）的实验结果"></p><h4 id="3-2-3-SQuAD-v2-0"><a href="#3-2-3-SQuAD-v2-0" class="headerlink" title="3.2.3 SQuAD v2.0"></a>3.2.3 SQuAD v2.0</h4><p>SQuAD v2.0是在SQuAD v1.1数据集上的一个拓展，该数据集中所提供的文段中，有一定的可能性不存在对应答案，从而使得问题更切合实际。实验微调了2个epoch，将batch size设置为48，并将学习率固定为5e-5。实验结果如表3.3所示，结果表明，与先前的若干工作[18, 19]相比，BERT相较于现有的最优模型，在F1指标上获得了5.1%的提升。</p><p><img src="/pic/bert/SQuADv2.0%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.3 SQuAD v2.0数据集（拓展版问答任务）的实验结果"></p><h4 id="3-2-4-SWAG"><a href="#3-2-4-SWAG" class="headerlink" title="3.2.4 SWAG"></a>3.2.4 SWAG</h4><p>SWAG[20]是一个具有对抗性生成情形的自然语言推理数据集，它包含了113k组句子对。通过理解给定的句子，我们需要从对应的四个句子中选择最有可能延续在该句子之后的选项。实验构建了四个输入序列，每个序列包含了给定句子和可能的延续句子之间的连接。同时，还引入了一个参数向量，它与每个句子开头的 [CLS] 符号之间的点积表示该选项的最终得分。实验微调了3个epoch，将batch size设置为16，并将学习率固定为2e-5。实验结果如表3.4所示，结果表明，BERTLARGE的性能相较于ESIM+ELMo提升了27.1%，相较于OpenAI GPT提升了8.3%。</p><p><img src="/pic/bert/SWAG%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.4 SWAG数据集（对抗文本生成任务）的实验结果"></p><h3 id="3-3-消融实验及结果分析"><a href="#3-3-消融实验及结果分析" class="headerlink" title="3.3 消融实验及结果分析"></a>3.3 消融实验及结果分析</h3><p>本部分对BERT模型的多个部分进行了消融实验研究，旨在探寻它们对于整体模型的重要程度。</p><h4 id="3-3-1-预训练任务"><a href="#3-3-1-预训练任务" class="headerlink" title="3.3.1 预训练任务"></a>3.3.1 预训练任务</h4><p>本部分通过对 BERT 预训练任务进行消融，旨在论证 BERT 深度双向模型这一创新思想的重要性。实验共设置了两组消融，其中一组使用双向完形填空任务但不使用上下句预测任务，另一组同样不使用上下句预测任务，但实现完形填空任务时采用从左到右的标准模型。文章首先探究了上下句预测任务的取消带来的影响，发现其严重降低了 QNLI，MNLI 和 SQuAD 1.1 这三个任务的性能。其次，通过改变完形填空任务的训练方式，来探究双向训练带来的影响。实验结果如表 3.5 所示，结果表明，在所有任务上，从左到右的单向模型性能都收获了更差的效果，在 MRPC 和 SQuAD 这两个任务上尤为显著。</p><p><img src="/pic/bert/%E5%AF%B9%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%9E%8D%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.png" alt="表3.5 对预训练任务进行消融的实验结果"></p><h4 id="3-3-2-模型规模"><a href="#3-3-2-模型规模" class="headerlink" title="3.3.2 模型规模"></a>3.3.2 模型规模</h4><p>本部分旨在探究模型大小对微调任务准确度的影响。实验设置了若干具有不同层数、隐层维度以及注意力头数目的模型，并在 GLUE 数据集上进行了微调训练。实验结果如表 3.6 所示，结果表明，即使是在有标签数据量较小的数据集上，随着模型规模的提高，任务的准确度都获得了显著的提升。现有的最大规模的 Transformer 模型[21]具有 235M 的参数量，而 BERTLARGE 进一步将参数量增加至 340M，并且使性能获得了更大的提升。此实验进一步论证了，如果模型已经经过了充分的预训练，那么当将模型缩放到一个极限的规模尺寸时，仍然能够在小规模的微调任务上产生较大的改进。</p><p><img src="/pic/bert/%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%9E%8D%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.png" alt="表3.6 对模型规模进行消融的实验结果"></p><h3 id="3-4-实验总结"><a href="#3-4-实验总结" class="headerlink" title="3.4 实验总结"></a>3.4 实验总结</h3><p>实验结果表明，深层的双向语言模型能够极大地改善 NLP 任务的性能。同时，预训练模型的迁移学习，逐渐成为语言理解系统中不可或缺的一部分，它甚至能够使得一些低资源的任务从深度单向架构中受益。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]    Brown P F, Della Pietra V J, Desouza P V, et al. Class-based n-gram models of natural language[J]. Computational linguistics, 1992, 18(4): 467-480.<br>[2]    Ando R K, Zhang T, Bartlett P. A framework for learning predictive structures from multiple tasks and unlabeled data[J]. Journal of Machine Learning Research, 2005, 6(11).<br>[3]    Blitzer J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning[C]//Proceedings of the 2006 conference on empirical methods in natural language processing. 2006: 120-128.<br>[4]    Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.<br>[5]    Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[6]    Kiros R, Zhu Y, Salakhutdinov R R, et al. Skip-thought vectors[J]. Advances in neural information processing systems, 2015, 28.<br>[7]    Logeswaran L, Lee H. An efficient framework for learning sentence representations[J]. arXiv preprint arXiv:1803.02893, 2018.<br>[8]    Le Q, Mikolov T. Distributed representations of sentences and documents[C]//International conference on machine learning. PMLR, 2014: 1188-1196.<br>[9]    Peters M E, Ammar W, Bhagavatula C, et al. Semi-supervised sequence tagging with bidirectional language models[J]. arXiv preprint arXiv:1705.00108, 2017.<br>[10]    Peters M, Neumann M, Iyyer M, et al. Deep contextualized word representations[A]. Conference of the North American Chapter of the Association for Computational Linguistics[C]. New Orleans, Louisiana, Association for Computational Linguistics, 2018a: 2227-2237.<br>[11]    Socher R, Perelygin A, Wu J, et al. Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proceedings of the 2013 conference on empirical methods in natural language processing. 2013: 1631-1642.<br>[12]    Rajpurkar P, Zhang J, Lopyrev K, et al. Squad: 100,000+ questions for machine comprehension of text[J]. arXiv preprint arXiv:1606.05250, 2016.<br>[13]    Sang E F, De Meulder F. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition[J]. arXiv preprint cs/0306050, 2003.<br>[14]    Radford A, Narasimhan K, Salimans T, et al. Improving language understanding with unsupervised learning[J]. 2018.<br>[15]    Deng J, Dong W, Socher R, et al. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009: 248-255.<br>[16]    Yosinski J, Clune J, Bengio Y, et al. How transferable are features in deep neural networks?[J]. Advances in neural information processing systems, 2014, 27.<br>[17]    Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis platform for natural language understanding[J]. arXiv preprint arXiv:1804.07461, 2018.<br>[18]    Sun F, Li L, Qiu X, et al. U-net: Machine reading comprehension with unanswerable questions[J]. arXiv preprint arXiv:1810.06638, 2018.<br>[19]    Wang W, Yan M, Wu C. Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering[J]. arXiv preprint arXiv:1811.11934, 2018.<br>[20]    Zellers R, Bisk Y, Schwartz R, et al. Swag: A large-scale adversarial dataset for grounded commonsense inference[J]. arXiv preprint arXiv:1808.05326, 2018.<br>[21]    Al-Rfou R, Choe D, Constant N, et al. Character-level language modeling with deeper self-attention[C]//Proceedings of the AAAI conference on artificial intelligence. 2019, 33(01): 3159-3166.</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 预训练语言模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>综述：深度学习中的优化方法</title>
      <link href="/2022/06/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/"/>
      <url>/2022/06/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>优化是深度学习中的关键组成部分。在深度模型中，我们通常需要设计一个模型的损失函数来约束训练过程，并朝着最小化代价函数的方向去训练。然而，由于深度网络的复杂性，使得深度学习中的优化问题面临着诸多困难和挑战，至今仍未得到一个完美的解决方案。目前，深度神经网络的参数学习主要是通过梯度下降法来寻找一组可以最小化结构风险的参数。本文从梯度下降法出发，通过探寻深度优化问题中的若干可改进点，对优化算法的变体进行归类探究。除此之外，文章还简要分析了模型的初始化参数和输入值尺度这两个重要方面，辅助优化算法在深度模型上获得更好的效果。</p></blockquote><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>优化算法是深度学习中十分重要的环节。深度学习中的优化，往往是通过给定一个以模型参数为变量的函数，一般称为损失函数或目标函数，通过具体算法找到合适的参数值，使得该函数最小化[17]。从理论上来说，对于一个存在解析形式的函数，一定能够通过最优性条件[1;13]求解出显式最优解。然而，由于方程求解的难度过大，往往使用基于迭代的数值计算方法去优化目标。较为常见的有梯度下降法、牛顿法、共轭梯度法等。</p><p>摒弃了传统机器学习对于人工特征工程的高度依赖，深度学习往往通过构建复杂度高、参数量大的深层次网络模型，去让模型自主学习到繁杂的数据特征。但是，面对如此高复杂度的含参模型，算力的限制对模型的训练速度提出了很高的要求。同时，高维度的损失函数存在着大量不可预知的特殊因素，例如函数中的鞍点、高原、平坦区域等，这些对优化算法的性能也提出了很高的要求。因此，相较于传统的机器学习优化算法[19;2]，必须在它们的基础上做出相应的改进，以解决它们被应用在深度学习中时面临的诸多困境。</p><p>本文从最传统的一阶和二阶优化算法出发，首先在第 2 节介绍了经典的梯度下降法和牛顿法，为后续的变体算法做铺垫。之后三个章节从三个视角切入，对优化算法的改进版本进行了分类和对比。第 3 节从训练速度和精度的角度出发，探究了在训练过程中，每次训练时输入数据的规模大小对优化问题的影响。第 4 节探究了收敛速度对整个训练过程的影响，通过改进学习率设置来让收敛速度自适应整个训练过程，保证收敛的准确性。第 5 节考虑到随机梯度下降中梯度估计的随机性，故对梯度估计方法进行了修改，使梯度更新在训练过程中更加稳定。最后，第 6 节放眼优化算法之外，探究了模型初始化参数和输入值尺度两大重要因素对优化准确度的影响，并给出了已有的一些解决方案。全文的工作流程以及对优化问题的划分如图 1 所示。</p><p><img src="/pic/optimization/overview.png" alt="图 1：本文对于优化问题的归类划分，以及不同算法之间的关联。文章将按照该分类依据进行对应方法的介绍。"></p><h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2 背景"></a>2 背景</h2><h3 id="2-1-形式化描述"><a href="#2-1-形式化描述" class="headerlink" title="2.1 形式化描述"></a>2.1 形式化描述</h3><p>以有监督模型为例，我们首先对优化问题的定义进行简单的描述。假设输入数据$(x_i, y_i) \in \mathbb{R}^{d_x} \times \mathbb{R}^{d_y}$，$i = 1,2,…,n$，其中$n$为输入的样本数量。对于具体任务而言，我们需要学习一个网络映射，将输入值$x_i$映射为一个新的取值$\hat{y}_i$，作为该网络的输出值，即模型的预测值。之后，将预测值$\hat{y}_i$与真实值$y_i$进行对比，该过程一般通过构造损失函数来实现，该函数往往反映了真实值和预测值之间的误差大小。优化算法所需要解决的，便是最小化该损失函数这一重要问题。</p><p>我们首先构造一个网络映射 $f_{\theta} : \mathbb{R}^{d_x} \times \mathbb{R}^{d_y}$，其中 $\theta$ 为需要优化的网络参数集合。同时，定义损失函数为 $\mathcal{L}(\cdot, \cdot)$，则优化问题可被形式化定义为</p><p>$$<br>    \min_{\theta} \frac{1}{n} \sum_{i=1}^n \mathcal{L} (y_i, f_{\theta}(x_i))<br>$$</p><p>损失函数的选取对于优化问题也十分关键，通常来说，我们需要根据具体任务的需求来设置特定于任务的损失函数。例如，对于回归预测问题，我们通常采用均方误差损失函数，即 $\mathcal{L}(y, y’) = ||y - y’||^2$。对于分类问题，我们通常采用交叉熵或KL散度来比较预测分布和真实分布之间的差异。</p><h3 id="2-2-基础算法"><a href="#2-2-基础算法" class="headerlink" title="2.2 基础算法"></a>2.2 基础算法</h3><p>本部分我们将介绍深度学习优化问题中的两大基础算法 —— 梯度下降法和牛顿法，分别对应基本的一阶和二阶算法。后续针对优化算法的改进，通常在这两个基础算法上进行迭代更新。</p><h4 id="2-2-1-梯度下降法"><a href="#2-2-1-梯度下降法" class="headerlink" title="2.2.1 梯度下降法"></a>2.2.1 梯度下降法</h4><p>梯度下降法是一种基于迭代的最小化目标函数的优化算法。对于函数 $f(x)$， 若 $f(x)$ 在 $x$ 附近连续可微，则 $f(x)$ 下降最快的方向为 $f(x)$ 在 $x$ 点处的梯度反方向。基于这一思想，假设目标函数为 $J(\theta)$ ，其中 $\theta$ 为模型参数，则梯度下降法需要利用该函数的反向梯度 $-\nabla_{\theta} J(\theta)$，来对目标参数进行更新。同时，该方法还设置了一个学习率 $\alpha$，来进一步限制每次更新时的步长大小。</p><p>具体的，假设参数 $\theta$ 的初始值为 $\theta_0$，则对于第 $t$ 次参数更新，迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \nabla_{\theta} J(\theta)<br>$$</p><p>通过迭代更新，将会生成参数序列 $\theta_0. \theta_1, \theta_2, …，\theta_t, …$，直至该序列收敛到期望最优解 $\theta^*$，则算法终止。<br>该算法由于利用了目标函数的一阶导数进行迭代更新，故被归为一阶优化算法。</p><h4 id="2-2-2-牛顿法"><a href="#2-2-2-牛顿法" class="headerlink" title="2.2.2 牛顿法"></a>2.2.2 牛顿法</h4><p>牛顿法[3]是一种基于二阶导数的迭代优化算法。对于目标函数 $J(\theta)$，假设 $J(\theta)$ 连续二阶可微，将其在 $\theta_t$ 处进行二阶泰勒展开，并取二阶近似</p><p>$$<br>    J(\theta) \approx J(\theta_t) + \nabla_{\theta} J(\theta_t)^T (\theta - \theta_t) + \frac{1}{2} (\theta - \theta_t)^T \nabla^2_{\theta} J(\theta_t)(\theta - \theta_t)<br>$$</p><p>其中，$\nabla^2_{\theta} J(\theta_t)$ 是 $J(\theta)$ 在 $\theta_t$ 处的 Hesse 矩阵。令 $\nabla_{\theta} J(\theta) = 0$，则</p><p>$$<br>    \nabla_{\theta} J(\theta_t) + \nabla^2_{\theta} J(\theta_t)(\theta - \theta_t) = 0<br>$$</p><p>设 $\nabla^2_{\theta} J(\theta_t)$ 可逆，则可以得到牛顿法的迭代公式</p><p>$$<br>    \theta_{t+1} = \theta_t - \nabla^2_{\theta} J(\theta_t)^{-1} \nabla_{\theta} J(\theta_t)<br>$$</p><p>相比于基于一阶导数的梯度下降法，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑走了一步之后，坡度是否会变得更大。因此，牛顿法比梯度下降法看得更远一点，能更快地走到最底部。而梯度下降法每次只从当前所处位置选一个坡度最大的方向走一步。可视化效果如图 2 所示。</p><p><img src="/pic/optimization/%E4%B8%80%E9%98%B6%E4%BA%8C%E9%98%B6.png" alt="图 2：一阶和二阶算法的迭代下降过程。红线表示牛顿法，更新方向相对精准，但计算时间相对较长；绿线表示梯度下降法，更新方向相对随机，但节约了大量计算时间。"></p><p>尽管牛顿法收敛更快，但是每次迭代的时间比梯度下降法长，因此相对而言并不常用。接下来介绍的所有优化算法，均在一阶梯度下降法的基础上进行改进。</p><h2 id="3-输入数据的规模：精度与速度的权衡"><a href="#3-输入数据的规模：精度与速度的权衡" class="headerlink" title="3 输入数据的规模：精度与速度的权衡"></a>3 输入数据的规模：精度与速度的权衡</h2><p>深度学习中，为了减少单次训练时的随机性影响，避免因为数据噪声产生的不收敛现象，而引入了“批处理”的思想。简而言之，就是在单次训练过程中，向网络中传入多组数据，数据规模称为 batch size，一次完整的训练称为一个 batch。对于一个batch中的每个训练样本，我们都会得到一个单独的损失值，通过对所有样本的损失值取平均，构造最终的损失函数，来减弱单样本的随机性误差。然而，随着batch size的增大，每次训练需要的样本数增多，所耗费的时间代价也会随之增加。因此，需要在精度和速度之间，找寻到一个微妙的平衡。</p><p>本部分将以 batch size 为分类依据，介绍三种典型算法。首先介绍随机梯度下降（SGD）和批量梯度下降（BGD），分别对应速度和精度的两个极端情形。最后在此基础上引入小批量梯度下降（MSGD），通过调整控制 batch size的大小，来平衡精度和速度两项指标之间的关系。</p><h3 id="3-1-全批量梯度下降（BGD）"><a href="#3-1-全批量梯度下降（BGD）" class="headerlink" title="3.1 全批量梯度下降（BGD）"></a>3.1 全批量梯度下降（BGD）</h3><p>对于每一次参数更新，最朴素的思想则是将所有训练样本放入网络，利用完整的数据集进行损失函数的计算。这种思想的主要依据在于，利用全数据集训练时，确定的梯度下降方向能够更好地代表样本总体，从而能够更加准确地朝向极值方向。</p><p>假设数据集规模为 $n$，输入样本为 $(x^{(i)}, y^{(i)})$，$i = 1,2,…,n$，则 $t+1$ 时刻的迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \frac{1}{n} \sum_{i=1}^n \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>大规模样本下，模型收敛的稳定性得到了保证。同时，由于batch size较大，所需要的batch数随之减少，因此总的训练时间会进一步缩短。然而，过大的 batch size ，使得模型的探索性变差，容易在起始点附近很近的地方停止更新。同时，样本中的噪声影响显著削弱，使得函数难以跳出尖锐极小点[9]，从而限制了模型的泛化能力。</p><h3 id="3-2-随机梯度下降（SGD）"><a href="#3-2-随机梯度下降（SGD）" class="headerlink" title="3.2 随机梯度下降（SGD）"></a>3.2 随机梯度下降（SGD）</h3><p>随机梯度下降[16]处理了 batch size 大小为 1 时的情形，即每次进行参数更新时，只使用一个训练样本。假设输入为 $x^{(i)}$，对应标签为 $y^{(i)}$，则迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>相比于全批量梯度下降法，随机梯度下降的一大优势在于，由于每次训练样本的随机性，使得样本中的噪声也能够被充分利用，进而提升了模型的泛化能力。同时，由于每次训练时只需要考虑唯一的样本，因此训练速度也会显著增加，可以被用作在线学习。<br>然而，该方法的缺点也十分明显。由于噪声的增多，尽管模型的泛化能力增强，但也增加了模型的收敛难度。尽管每次迭代整体上会朝着最优方向下降，但每次更新时的方向较为杂乱，且容易陷入局部极小值或鞍点。</p><h3 id="3-3-小批量梯度下降（MSGD）"><a href="#3-3-小批量梯度下降（MSGD）" class="headerlink" title="3.3 小批量梯度下降（MSGD）"></a>3.3 小批量梯度下降（MSGD）</h3><p>在 BGD 和 SGD 中，我们发现，batch size的大小对优化算法的结果，会产生较为显著的影响，且主要集中在“精度”和“速度”这两个重要维度。在 BGD 中，我们使用了全数据集样本进行单次训练。尽管样本数的扩大，会使得模型收敛更加稳定，但单次训练需要遍历完整的数据集，使得训练速度大大降低。在 SGD 中，我们使用了随机的单一样本进行单次训练。由于每次只需计算一个样本，因此单次训练的速度大大提高，但噪声的影响在训练过程中被显著放大，对于模型的收敛准度带来了极大的挑战。</p><p>MSGD 方法的引入，正是为了解决batch size 过大或过小，而导致的精度和速度无法收敛的问题。假设每一次训练中使用 $m$ 个样本，则 $t+1$ 时刻的迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>一般来说，$m$ 的取值在几到几百之间，可以根据具体任务规模而定。batch size的变动可以随之对学习率的大小进行调整。batch size设置较大时，随机梯度方差越小，则训练更稳定，可以设置更大的学习率。batch size 较小时，需要较小的学习率来保证收敛稳定。对于batch size和学习率之间的调整关系，我们可以采用线性缩放原则来进行大致估计，即批量大小增加 $k$ 倍时，学习率也增加 $k$ 倍。</p><p>小批量梯度下降已经称为了梯度下降法中的一个最优变体模型。然而，该算法仍然存在着若干挑战。首先，batch size 作为一个重要的超参数，需要我们根据具体训练效果进行人工调整。其次，学习率设置和梯度估计方法这两部分，在迭代公式中也是不可被忽视的因素。因此，我们需要对这两部分进行进一步的优化，使得 MSGD 方法能够获得更好的性能。</p><h2 id="4-收敛速度控制：学习率调整"><a href="#4-收敛速度控制：学习率调整" class="headerlink" title="4 收敛速度控制：学习率调整"></a>4 收敛速度控制：学习率调整</h2><p>在前一部分中，所有的参数迭代公式均包含一项超参数，即学习率 $\alpha$，用来控制参数在更新迭代时的步幅大小。学习率的选取对于参数的更新至关重要，它决定了网络模型是否能够收敛至全局最小值。若学习率过大，则模型难以收敛，若学习率过小，则模型收敛过慢，如图 3 所示。因此，我们需要在梯度下降法的基础上，对训练过程中的学习率进行优化调整，使得模型的收敛过程更加符合预期效果。</p><p><img src="/pic/optimization/learning_rate.png" alt="图 3：学习率过大或过小时的收敛情况。图（a）表示学习率过大的情况，损失始终处于振荡下降状态，无法准确收敛至最小点；图（b）表示学习率过小的情况，损失下降过于缓慢，导致探索到更优的最小点的概率大大减小。"></p><h3 id="4-1-学习率预热-——-Warmup"><a href="#4-1-学习率预热-——-Warmup" class="headerlink" title="4.1 学习率预热 —— Warmup"></a>4.1 学习率预热 —— Warmup</h3><p>由于刚开始训练时,模型的参数是随机初始化的，此时若选择一个较大的学习率,可能会使得模型不稳定，产生振荡现象。因此，通过预热学习率的方式，可以使得训练初期的学习率较小,在小学习率下，模型可以慢慢趋于稳定。等模型相对稳定后，再去选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p><p>假设预热的迭代总次数为 $T$，初始学习率为 $\alpha_0$，当前迭代次数为 $t$，则最初的 $T$ 次迭代中，学习率动态变化的计算方式为</p><p>$$<br>    \alpha_t = \alpha_0 \cdot \frac{t}{T}<br>$$</p><h3 id="4-2-学习率衰减-——-Decay"><a href="#4-2-学习率衰减-——-Decay" class="headerlink" title="4.2 学习率衰减 —— Decay"></a>4.2 学习率衰减 —— Decay</h3><p>一般来说，在参数更新初期，由于距离最优点较远，因此应该保证较大的学习率，充分探索邻近区域，保证较快的收敛速度。到达后期时，参数逐步逼近最优点，因此需要保证较小的学习率，来防止在最优点附近来回震荡。</p><p>基于这样的经验，我们发现，随着参数更新的不断进行，需要不断降低学习率的数值，使得更新的步幅逐渐变小，保证收敛的稳定性。从这点出发，我们可以通过将学习率常量改变为一个随时间动态可变的数值，来满足这一要求。</p><p>不失一般性，我们将学习率按迭代次数 $t$ 进行衰减。设学习率初始值为 $\alpha_0$，在 $t$ 次迭代时的学习率为 $\alpha_t$，则可给出 $\alpha_t$ 关于迭代次数和学习率初始值的衰减函数表达式 $\alpha_t = f(\alpha_0, t)$。常见的衰减表达式有如下几种：</p><ul><li>阶梯衰减： 该方法每经过 $T_1, T_2, …, T_m$ 次迭代，将学习率衰减为原来的 $\beta_1, \beta_2, …, \beta_m$ 倍，所有参数均为人工设置的超参数。</li><li>指数衰减： 设 $\beta &lt; 1$ 为衰减率，则 $\alpha_t = \alpha_0 \beta^t$，$\beta$ 可取0.95，0.98等。</li><li>自然指数衰减： 设 $\beta$ 为衰减率，则 $\alpha_t = \alpha_0 e^{-\beta \times t}$，$\beta$ 可取0.02，0.05等。</li><li>逆时衰减： 设 $\beta$ 为衰减率，则 $\alpha_t = \alpha_0 \frac{1}{1 + \beta \times t}$。</li><li>余弦衰减： 设 $T$ 为总的迭代次数，则 $\alpha_t = \frac{1}{2} \alpha_0 (1 + \cos (\frac{t \pi}{T}))$。</li></ul><h3 id="4-3-Warmup-和-Decay"><a href="#4-3-Warmup-和-Decay" class="headerlink" title="4.3 Warmup 和 Decay"></a>4.3 Warmup 和 Decay</h3><p>对于一个完整的训练过程，通常结合使用预热和衰减两种学习率调整方式。训练开始时，由于参数初始化的随机性，需要将学习率控制在一个较小的范围，逐渐增大，直至训练稳定。稳定后，模型的分布就较为固定了，若还沿用较大的学习率，就会破坏稳定性，很有可能错过局部最优点。这时应逐步降低学习率，使得模型能够稳定收敛至局部最小值。</p><h3 id="4-4-周期性学习率调整"><a href="#4-4-周期性学习率调整" class="headerlink" title="4.4 周期性学习率调整"></a>4.4 周期性学习率调整</h3><p>通过学习率预热和衰减的方式，收敛稳定性的问题已经可以被初步解决。然而，在参数迭代的过程中，函数鞍点和尖锐最小值的存在，使得模型参数极有可能陷入较差的局部最优解。因此，考虑到这些特殊情况的存在，我们需要适当的对学习率进行增大，使得参数能够有更大概率逃离尖锐或平坦最小值。尽管这样的操作，在短期内可能损害优化过程，但从长远来看，有助于找到更优的局部最小值[7]。</p><p>周期性学习率[12;18]调整方法是在保证模型收敛稳定的基础上，对函数中的特殊点进行了一个更为周到的考虑，使其能够跳出较差的局部最优，向着更优的局部最小去探索。常用的周期学习率调整方法有如下两种：</p><ul><li>循环学习率： 让学习率在某个区间内周期性增大或减小。假设一个循环周期为 $2 \Delta T$，其中前 $\Delta T$ 步线性增大，后 $\Delta T$ 步线性减小。假设循环周期数为 $m$，则第 $t$ 次迭代时，所在的循环周期数 $m$ 为<br>$$<br>  m = [1 + \frac{t}{2\Delta T}]<br>$$</li></ul><p>假设第 $m$ 个周期内，学习率最大和最小值分别为 $\alpha_{\min}^m$ 和 $\alpha_{\min}^m$，则第 $t$ 次迭代的学习率为<br>$$<br>    \alpha_t = \alpha_{\min}^m + (\alpha_{\max}^m - \alpha_{\min}^m)\cdot \max(0, 1 - |\frac{t}{\Delta T} - 2m + 1|)<br>$$</p><ul><li>带热重启的随机梯度下降： 该方法采用热重启方式来替代学习率衰减方法。每间隔一段时间，学习率会重新初始化为某个预设的值，之后继续衰减。</li></ul><p>假设整个迭代过程共重启 $M$ 次，每次的重启周期为 $T_m$，重启后采用余弦衰减方法来降低学习率。则 $t$ 次迭代后的学习率为<br>$$<br>    \alpha_t = \alpha_{\min}^m + \frac{1}{2}(\alpha_{\max}^m - \alpha_{\min}^m)(1 + \cos(\frac{T_{cur}}{T_m} \pi))<br>$$</p><p>其中，$\alpha_{\min}^m$ 和 $\alpha_{\min}^m$ 表示第 $m$ 个周期内，学习率的最大和最小值。$T_m$ 为重启周期，$T_{cur}$ 为上次重启之后的 epoch 数。</p><p>不管是哪种周期调整方式，尽管在某一段时间内，学习率会周期性上升或下降，但从全局来看，学习率整体仍保持着下降的趋势，这也为模型的收敛稳定性提供了保障。</p><h3 id="4-5-自适应学习率"><a href="#4-5-自适应学习率" class="headerlink" title="4.5 自适应学习率"></a>4.5 自适应学习率</h3><p>在梯度下降法中，每次参数迭代的对象都涉及到多个参数。然而，对于每个参数，对应的学习率都完全相同。由于每个参数在对应维度上的收敛速度互不相同，则会导致某些参数对于更新方向不够敏感，从而造成精度损失。早期的一种在训练时适应模型参数各自学习率的启发式方法是，如果损失对于某个给定参数模型的偏导保持相同符号，则学习率增加，反之减小。这也是自适应学习率思想的早期雏形。</p><p>自适应学习率是一种适应于参数的动态学习率设置方法，该方法根据参数收敛的不同情况，分别设置对应的学习率。接下来介绍三种经典的自适应学习率算法 —— AdaGrad、RMSprop、AdaDelta。</p><h4 id="4-5-1-AdaGrad"><a href="#4-5-1-AdaGrad" class="headerlink" title="4.5.1 AdaGrad"></a>4.5.1 AdaGrad</h4><p>AdaGrad 算法[5]采用累计平方梯度来独立适应所有模型参数的学习率，其学习率设置与梯度历史平方值总和呈现反比关系。若该参数的损失偏导较大，则学习率衰减速度较快，若损失偏导较小，则学习率下降较慢。其目的是让参数在更为平缓的倾斜方向上取得更大的进步。</p><p>首先，在第 $t$ 次迭代时，我们需要计算每个参数梯度平方的累计值<br>$$<br>    G_t = \sum_{\tau=1}^{t} g_{\tau} \odot g_{\tau}<br>$$</p><p>其中，$\odot$ 为按元素乘积，$g_{\tau} \in \mathbb{R}^{|\theta|}$ 为第 $\tau$ 次迭代时的梯度。则 AdaGrad 最终的参数更新为</p><p>$$<br>\begin{aligned}<br>\theta_{t+1} &amp;= \theta_t + \Delta \theta \\<br>&amp;= \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\odot g_t<br>\end{aligned}<br>$$</p><p>尽管 AdaGrad 算法考虑到了不同参数的梯度变化情况，但是当经过一定迭代次数之后，若仍然未找到最优点，则由于学习率较小，会很难继续找到最优点。</p><h4 id="4-5-2-RMSprop"><a href="#4-5-2-RMSprop" class="headerlink" title="4.5.2 RMSprop"></a>4.5.2 RMSprop</h4><p>AdaGrad 算法采用了历史累计平方梯度来适应参数学习率，这会使得学习率迅速衰减，因此。RMSprop[20]在此基础上进行了优化，采用指数衰减平均来丢弃遥远的历史。具体来说，它将梯度积累方法改变为指数加权的移动平均方法，使得过于遥远的梯度在调整学习率时的重要性降低。这样的改进，使得学习率并不完全呈现衰减趋势，可变小也可变大，避免了过早衰减的问题。</p><p>方法的整体迭代流程与 AdaGrad 类似，唯一不同的是将累计梯度平方改为了指数衰减移动平均。具体计算方法为</p><p>$$<br>\begin{aligned}<br>G_t &amp;= \beta G_{t-1} + (1 - \beta) g_t \odot g_t \\<br>&amp;= (1 - \beta) \sum_{\tau = 1}^t \beta^{t-\tau} g_{\tau} \odot g_{\tau}<br>\end{aligned}<br>$$</p><p>同样的，RMSprop 最终的参数更新为</p><p>$$<br>\begin{aligned}<br>\theta_{t+1} &amp;= \theta_t + \Delta \theta \\<br>&amp;= \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\odot g_t<br>\end{aligned}<br>$$</p><p>其中，$\beta$ 为衰减率，一般设置为 0.9；$\alpha$ 为初始学习率，一般设置为 0.001。</p><h4 id="4-5-3-AdaDelta"><a href="#4-5-3-AdaDelta" class="headerlink" title="4.5.3 AdaDelta"></a>4.5.3 AdaDelta</h4><p>AdaDelta[22]在 RMSprop 的基础上做了进一步的改进。和 RMSprop 一样，在调整学习率时采用了指数衰减移动平均的方法。唯一不同的时，AdaDelta 在进行参数更新时，其参数更新差值 $\Delta \theta$ 也采用了指数衰减平均的方式进行动态变化。</p><p>具体的，该方法将参数更新表达式中的初始学习率 $\alpha$ 替换为一个随时间变化的参数 $\sqrt{\Delta X^2_{t-1} + \epsilon}$，其中微小量 $\epsilon$ 用来保证数值稳定。则 $\Delta \theta$ 的指数衰减移动平均计算方式为</p><p>$$<br>    \Delta X^2_{t-1} = \beta_1 \Delta X^2_{t-2} + (1 - \beta_1)\Delta \theta_{t-1} \odot \Delta \theta_{t-1}<br>$$</p><p>其中 $\beta_1$ 为衰减率，最终的参数更新方式为<br>$$<br>    \begin{aligned}<br>        \theta_{t+1} &amp;= \theta_t + \Delta \theta \\<br>        &amp;= \theta_t - \frac{\sqrt{\Delta^2_{t-1}} + \epsilon}{\sqrt{G_t + \epsilon}}\odot g_t<br>    \end{aligned}<br>$$</p><h2 id="5-梯度估计修正"><a href="#5-梯度估计修正" class="headerlink" title="5 梯度估计修正"></a>5 梯度估计修正</h2><p>在梯度下降法中，一个重要的前提条件是，函数在某一点的最速下降方向为该点的梯度反方向，因此采用梯度对参数进行迭代更新。然而，在小批量梯度下降中，由于样本选取的随机性，导致其梯度估计也存在一定的随机性，从而导致损失振荡下降，训练不够稳定。因此，需要设计一种新的梯度估计方法，让梯度的更新在训练过程中保持稳定，不受样本随机性的干扰。</p><h3 id="5-1-Momentum-动量法"><a href="#5-1-Momentum-动量法" class="headerlink" title="5.1 Momentum 动量法"></a>5.1 Momentum 动量法</h3><p>若能降低单次训练中随机样本带来的梯度随机性，则能够有效缓解损失振荡的现象。一种较为直接的想法是，采用最近一段时间训练时的平均梯度来代替单次训练的梯度，从而使参数更新的方向更加稳定，提高整体的优化速度。</p><p>动量法[15]则是基于这样的思想，对梯度下降的更新方向进行了稳定，缓解了振荡现象，从而加速了优化速度。具体地，它将参数更新中的负梯度更新方向改为了负梯度的“加权移动平均”方向，使得每个参数的实际更新差值由最近一段时间内梯度的加权平均值决定。在第 $t$ 次迭代中，参数的更新公式为<br>$$<br>    \begin{aligned}<br>    \theta_t &amp;= \theta_{t-1} + \Delta \theta_t \\<br>    &amp;= \theta_{t-1} + \rho \Delta \theta_{t-1} - \alpha g_t \\<br>    &amp;= \theta_{t-1} -  \alpha \sum_{\tau = 1}^t \rho^{t-\tau} g_{\tau}<br>    \end{aligned}<br>$$</p><p>其中，$\rho$ 为动量衰减因子，通常设置为 0.9或近似值；$\alpha$ 为学习率。如图 4 所示，引入了累计加权平均梯度的动量方法后，振荡现象明显减弱，参数能够更快并更准确地向最优方向收敛。</p><p><img src="/pic/optimization/%E5%8A%A8%E9%87%8F.png" alt="图 4：引入动量法前后的梯度下降收敛过程。左侧为引入动量法前的收敛过程，收敛的振荡现象较为严重；右侧为引入动量法后的收敛过程，梯度的更新方向更加准确，减少了收敛的不确定性，也加速了整个收敛过程。"></p><p>动量法实质上来自于一个物理类比。动量是质量和速度的乘积，我们规定单位质量，则动量大小即为速度大小，在迭代公式中即为 $\Delta  \theta$。我们将收敛过程抽象成一个小球从山顶向山谷滚去，在更新公式中，当前时刻的梯度值 $-\alpha g_t$ 即为力的提供者，推动小球沿着最为陡峭的方向向下滚动，从而累计动量，速度不断加快。另外，累计梯度 $\rho \Delta \theta_{t-1}$ 代表了粘滞阻力，当小球遇到上坡或障碍时，该阻力的存在能够及时阻止其偏离方向，从而阻止其发生大幅度振荡，加速其滚动至最低点。</p><h3 id="5-2-Nesterov-加速梯度动量"><a href="#5-2-Nesterov-加速梯度动量" class="headerlink" title="5.2 Nesterov 加速梯度动量"></a>5.2 Nesterov 加速梯度动量</h3><p>动量法中，单一梯度被修正为累计加权平均梯度，有效缓解了振荡现象的发生。然而，这样的方法仍然存在一定的问题。仍以小球滚动为例，动量法中，只有当小球开始产生振荡，偏离最速下降方向之后，它才会意识到错误，开始进行相应的调整。而更科学高效的方法应该是，在小球意识到将要偏离方向之前，就开始有意识地对自身的下降方向进行调整。它应当具备一定的预测判断能力。</p><p>动量法中，我们首先计算了当前时候的负梯度值 $-g_t$，并与上一时刻的梯度更新方向 $\Delta \theta_{t-1}$ 进行加权求和，获取当前时刻的梯度更新结果。而在 Nesterov 加速梯度动量[14]中，梯度计算的执行被安排在施加当前速度之后。具体的计算公式如下</p><p>$$<br>\begin{aligned}<br>    \theta_t &amp;= \theta_{t-1} + \Delta \theta_t \\<br>    &amp;= \theta_{t-1} + \rho \Delta \theta_{t-1} - \alpha \nabla_{\theta}(\theta_{t-1} + \rho \Delta \theta_{t-1})<br>\end{aligned}<br>$$</p><p>Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。在动量法中，梯度更新方式可以被拆分为两步。首先，利用累计梯度校正参数，即 $\hat{\theta}<em>t = \theta</em>{t-1} + \rho \Delta \theta_{t-1}$，之后，再加入当前梯度值，进行梯度下降，即 $\theta_{t} = \hat{\theta}<em>t - \alpha g_t$。然而，我们在首先对参数进行了校正后，仍然对当前时刻的梯度进行了计算，一种更合理的方式应该是，对校正后的参数进行梯度计算，能够进一步缓解当前梯度的随机性带来的误差。因此，将 $\alpha g_t$ 改为 $\alpha \nabla</em>{\theta}(\theta_{t-1} + \rho \Delta \theta_{t-1})$，便得到了 Nesterov 动量的核心思想。该方法与原始动量方法的可视化比较如图 5 所示。</p><p><img src="/pic/optimization/%E5%8A%A8%E9%87%8F%E6%AF%94%E8%BE%83.png" alt="图 5：动量法和Nesterov加速梯度 动量的更新方式比较。左侧为动量法的更新方式，累计动量和当前梯度的计算同时进行；右侧为Nesterov 加速梯度动量的更新方式，首先对当前动量进行更新，之后在更新后的基础上进行梯度计算。"></p><h3 id="5-3-Adam"><a href="#5-3-Adam" class="headerlink" title="5.3 Adam"></a>5.3 Adam</h3><p>Adam 算法[10]是动量法和 RMSprop 的结合，它不但使用动量作为参数更新的方向，同时也可以自适应地调整学习率，帮助其稳定收敛。</p><p>第 $t$ 次迭代时，Adam 算法首先计算了累计梯度平方的加权平均 $G_t$ 和梯度加权平均 $M_t$，计算公式如下<br>$$<br>    \begin{aligned}<br>    &amp;G_t = \beta_1 G_{t-1} + (1 - \beta_1) g_t \odot g_t \\<br>    &amp;M_t = \beta_2 M_{t-1} + (1 - \beta_2) g_t<br>    \end{aligned}<br>$$</p><p>其中，$\beta_1$ 和 $\beta_2$ 分别为两个移动平均的衰减率，取值通常趋近于 1。在迭代初期，$G_t$ 和 $M_t$ 通常会比真实值更小，且偏差较大。因此需要对偏差进行修正<br>$$<br>    \begin{aligned}<br>    &amp;\hat{G}_t = \frac{G_t}{1 - \beta^t_1} \\<br>    &amp;\hat{M}_t = \frac{M_t}{1 - \beta^t_2}<br>    \end{aligned}<br>$$</p><p>修正后，可以得到 Adam 算法的最终更新公式<br>$$<br>    \theta_{t} = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{G}_t} + \epsilon} \hat{M}_t<br>    \label{Adam}<br>$$</p><p>一般来说，$\beta_1$ 取0.999，$\beta_2$ 取0.9，$\epsilon$ 为稳定数值的微小量，一般取 $10^{-8}$。与其他算法相比，Adam 算法在实际应用中表现更为突出。</p><h3 id="5-4-Nadam"><a href="#5-4-Nadam" class="headerlink" title="5.4 Nadam"></a>5.4 Nadam</h3><p>Nadam[4]算法在 Adam 算法的基础上，进一步引入了 Nesterov 加速梯度。在公式<del>\ref{Adam} 中，梯度估计值 $\hat{M}_t$ 是利用动量法进行更新的（参考公式</del>\ref{21} 和~\ref{22}）。Nadam 对梯度估计值进行了调整。修正后的迭代公式为<br>$$<br>    \begin{aligned}<br>        \theta_{t} &amp;= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{G}_t} + \epsilon} \hat{M}<em>t \\<br>        &amp;= \theta</em>{t-1} - \frac{\alpha}{\sqrt{\hat{G}<em>t} + \epsilon} (\frac{\beta_1 M</em>{t-1}}{1 - \beta_1^t} + \frac{(1 - \beta_1)g_t}{1 - \beta_1^t})<br>    \end{aligned}<br>$$</p><h3 id="5-5-梯度截断"><a href="#5-5-梯度截断" class="headerlink" title="5.5 梯度截断"></a>5.5 梯度截断</h3><p>在深层次网络中，由于网络层数过深，在进行梯度的反向传播链式法则计算时，很有可能由于某一层梯度的过小或过大，造成累乘的放大效应，导致梯度消失或梯度爆炸现象的出现。例如，若权重初始化值较小，在各层上相乘得到的数值均在0-1之间，而激活函数梯度也在0-1之间，则连乘后数值会变得非常小，导致梯度消失。权重初始化较大同理。同样的，不合适的激活函数也会导致同样的效果。例如 sigmoid 函数。</p><p>在梯度计算时，最简单的避免梯度消失或梯度爆炸的方式是梯度截断。其思想也很简单，即把梯度限定在一个区间，当梯度的模小于或大于这个区间时就进行截断。一般有按值截断和按模截断两种方式。按值截断即限制了梯度向量中每个元素的取值。若某个参数的梯度小于或大于某个预设的阈值，则将该梯度固定为该阈值。按模截断限制了梯度向量的模长，假设截断阈值为 $\tau$，若 $||g_t|| &lt; \tau$，则保持 $||g_t||$ 不变；若 $||g_t|| &gt; \tau$，则 $g_t = \frac{b}{||g_t||} g_t$。</p><h2 id="6-外部因素制约：参数和输入"><a href="#6-外部因素制约：参数和输入" class="headerlink" title="6 外部因素制约：参数和输入"></a>6 外部因素制约：参数和输入</h2><p>由于深度学习优化问题本身的复杂性，仅仅对优化算法本身进行改进还不足以保证整个问题的完善求解。更多时候，一些外部因素往往会成为优化算法成功执行的制约条件。本部分将从参数初始化和输入值预处理出发，简要介绍如何对优化算法之外的一些关键要素进行优化，从而辅助优化算法在深度学习问题上更好地执行。</p><h3 id="6-1-参数初始化"><a href="#6-1-参数初始化" class="headerlink" title="6.1 参数初始化"></a>6.1 参数初始化</h3><p>深度学习模型的优化算法是迭代算法，因此要求指定一些开始迭代的初始点。训练深度模型是一个困难的问题，大多数优化算法都很大程度地受到初始化选择的影响。初始点能够决定算法是否收敛，有些初始点不稳定，使得该算法会遭遇数值困难。当学习收敛时，初始点可以决定学习收敛得多快，以及是否收敛到一个代价高或低的点。</p><p>现代的参数初始化策略大多为启发式的，该问题事实上仍然没有被很好地解决。比较常用的初始化方法有这样几种：（1）预训练初始化。预训练模型的一大初衷，就是帮助下游任务获得一套更优的初始化参数，加速下游任务的收敛，提高泛化能力。例如经典的BERT[8]、Roberta[11]等模型，至今仍被广泛沿用，尤其是自然语言处理任务。将预训练模型应用在下游任务上的过程被称为微调。（2）随机初始化。随机初始化的一大目的，是为了破坏“对称权重”现象。如果具有相同激<br>活函数的两个隐藏单元连接到相同的输入，且这些单元具有相同的初始参数，那么优化算法将一直以相同的方式更新这两个单元。因此，随机化权重可以确保没有输入模式丢失在前向传播的零空间中，没有梯度模式丢失在反向传播的零空间中，使得不同神经元之间的区分性更好。（3）固定值初始化。对于一些特殊的参数，我们可以根据经验用一个特殊的固定值来进行初始化．比如偏置通常用0 来初始化，但是有时可以设置某些经验值以提高优化效率。</p><p>总体来说，随机初始化通常更能够显著改善优化问题的结果。对于预训练初始化，尽管具有更好的收敛性和泛化性，但是灵活性不够，不能在目标任务上任意地调整网络结构。而固定值初始化过于依赖于经验，很多时候显然不具有说服力。因此，好的初始化方法对于优化问题依然十分重要。</p><p>比较简单的随机初始化方法固定了初始权重的方差，采用经典分布对初始值进行采样。例如高斯分布初始化，参数满足$\theta \sim \mathcal{N}(0, \sigma^2)$。或者采用均匀分布初始化，参数 $\theta$ 在给定区间 $[-r, r]$ 之间选择。若满足方差为 $\sigma^2$，则 $r = \sqrt{3 \sigma^2}$。不管采用何种分布，方差的设置都十分关键。若方差过小，则神经元输出过小，经过多层网络后会逐渐消失；若方差过大，则激活函数容易过饱和，尤其是对于sigmoid类型的函数而言，容易导致梯度消失。</p><p>一种更加有效的随机初始化方式，是根据神经元的连接数量来自适应地调整初始化分布的方差，尽可能保持每个神经元的输入和输出的方差一致。较为典型的是 Glorot and Bengio 提出的标准初始化方法[6]，对于 $m$ 个输入和 $n$ 个输出的全连接层，其初始化参数分布为 </p><p>$$<br>    \theta \sim U(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}})<br>$$</p><p>上面介绍的两种基于方差的初始化方法都是对权重矩阵中的每个参数进行独立采样．由于采样的随机性，采样出来的权重矩阵依然可能存在梯度消失或梯度爆炸问题。为了避免这样的问题出现，我们希望误差项在反向传播中具有范数保持性。具体的，假设一个 $L$ 层的等宽线性网络，前向传播过程为 $y = W^{(L)} W^{(L-1)} \dots W^{(1)} x$，其中 $W^{(l)} \in \mathbb{R}^{M\times M}$ 为第 $l$ 层权重矩阵。范数保持性即，对于反向传播中每一层的误差项 $\delta^{(l-1)} = (W^{(l)})^T \delta^{(l)}$，均有 $||\delta^{(l-1)}||^2 = ||\delta^{(l)}||^2 = ||(W^{(l)})^T \delta^{(l)}||^2$。从这点出发，只需要将 $W^{(l)}$ 初始化为正交矩阵即可。这种方法称为正交初始化。</p><h3 id="6-2-输入值预处理"><a href="#6-2-输入值预处理" class="headerlink" title="6.2 输入值预处理"></a>6.2 输入值预处理</h3><p>除了参数初始化比较困难之外，不同输入特征的尺度差异比较大时，梯度下降法的效率也会受到影响。尺度不同会造成在大多数位置上的梯度方向并不是最优的搜索方向，当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛。如果把数据归一化为相同尺度，则大部分位置的梯度方向近似于最优搜索方向．这样，在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高。对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果。</p><p>假设有 $N$ 个输入样本 ${x^{(n)}}_{i=1}^N$，有如下几种归一化方法：</p><ul><li>最值归一化：该方法通过将每个输入特征的取值范围归一到 $[0, 1]$ 之间。具体的，对于每一维特征，归一化后的特征为<br>$$<br>  \hat{x}^{(n)} = \frac{x^{(n)} - \min_n (x^{(n)})}{\max_n (x^{(n)}) - \min_n (x^{(n)})}<br>$$</li><li>标准化：该方向将输入特征调整为均值为0，方差为1的标准正态分布。首先计算原分布的均值和方差</li></ul><p>$$<br>\begin{aligned}<br>&amp; \mu = \frac{1}{N} \sum_{n=1}^N x^{(n)} \\<br>&amp; \sigma^2 = \frac{1}{N} \sum_{n=1}^N (x^{(n)} - \mu)^2<br>\end{aligned}<br>$$</p><pre><code>之后利用均值和方差，计算每个特征归一化后的数值</code></pre><p>$$<br>    \hat{x}^{(n)} = \frac{x^{(n)} - \mu}{\sigma}<br>$$</p><ul><li>白化：白化是一种重要的预处理方法，用来降低输入数据特征之间的冗余性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。白化的一个主要实现方式是使用主成分分析方法[21]去除掉各个成分之间的相关性。</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h2><p>本文针对优化算法应用在深度学习问题中的若干挑战，对优化算法进行了归类总结。我们首先介绍了基础的梯度下降法和牛顿法，并以梯度下降法为理论基础进行算法改进。我们以批量大小为衡量标准，对优化问题的速度和精度进行了权重。之后，从学习率和梯度估计方法这两个角度出发，介绍了若干针对性的改进方法。最后，跳出优化算法本身，对模型初始化参数和输入值尺度进行了简单的探讨。实际上，优化问题仍然还存在许多可以改进的算法点，例如，牛顿法可以引申出拟牛顿法、共轭梯度法等更加高效的二阶方法，这些方法将在后续工作中继续探讨。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334–334, 1997.<br>[2] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223–311, 2018.<br>[3] John E Dennis Jr and Robert B Schnabel. Numerical methods for unconstrained optimization and nonlinear equations. SIAM, 1996<br>[4] Timothy Dozat. Incorporating nesterov momentum into adam. 2016.<br>[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.<br>[6] Xavier Glorot and Yoshua Bengio. Understanding the diﬀiculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.<br>[7] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243, 2018.<br>[8] Kenton L. Kristina T. Jacob D., Chang M. W. BERT: pre-training of deep bidirectional transformers for language understanding. In proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171–4186, 2019.<br>[9] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.<br>[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.<br>[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.<br>[12] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.<br>[13] Olvi L Mangasarian. Nonlinear programming. SIAM, 1994.<br>[14] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/$k^2$). In Doklady an ussr, volume 269, pages 543–547, 1983.<br>[15] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145–151, 1999.<br>[16] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951.<br>[17] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.<br>[18] Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464–472. IEEE, 2017.<br>[19] Suvrit Sra, Sebastian Nowozin, and Stephen J Wright. Optimization for machine learning. Mit Press, 2012.<br>[20] Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.<br>[21] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37–52, 1987.<br>[22] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 最优化理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 —— 潜在邻近图对抗样本检测</title>
      <link href="/2022/06/05/LNG/"/>
      <url>/2022/06/05/LNG/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文探讨的论文标题为《Adversarial Example Detection Using Latent Neighborhood Graph》。文章标题开门见山，两个关键部分，一个是对抗样本检测（Adversarial Example Detection），这是描述了这篇文章所要完成的任务，另一个是潜在邻近图（Latent Neighborhood Graph），也就是说，文章很可能是要用一个图模型来完成样本检测的任务。带着这两个关键词，我们来详细分析一下这篇 ICCV 会议上的文章。</p></blockquote><p><img src="/pic/LNG/LNG-%E4%B8%BB.png" alt="LNG 概念图"></p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><h3 id="1-1-任务背景"><a href="#1-1-任务背景" class="headerlink" title="1.1 任务背景"></a>1.1 任务背景</h3><p>任务动机很朴素，当前的深度学习技术被广泛应用在了各个领域里面，但是一些攻击者会对整个深度模型进行操控，通过对模型的输入加入一些微小的扰动，来在人们难以察觉的情况下破坏整个模型的预测结果。这样的事情如果发生在一些安全系统当中，比如身份验证之类的重要环节，就会造成毁灭性的影响。因此深度模型的对抗训练必须被重视。</p><p><img src="/pic/LNG/%E4%BB%BB%E5%8A%A1%E8%83%8C%E6%99%AF.png" alt="一些深度模型微扰的例子，涉及cv、nlp等诸多领域"></p><h3 id="1-2-主流方法"><a href="#1-2-主流方法" class="headerlink" title="1.2 主流方法"></a>1.2 主流方法</h3><p>具体来说，为了让深度模型能够更好地去抵御这些扰动样本，目前比较主流的方法可以被归为两大类：主动防御和被动防御。</p><p>第一种是<font color=Red>主动防御</font>的方法。这种方法在模型训练中较为常见，即我们在训练时考虑到输入扰动的情况，然后手动加入一些对抗样本，这样能够提高模型的一个鲁棒性，而整个解空间更加平滑，而不会因为一个微扰让整个预测结果发生了根本性的变化。但是这种方法有一个非常关键的难点在于，它的训练代价比较大。试想一下，对于一个以及部署好了的已经被投放应用的模型，这时候说要让它的防御能力更强一点，势必要去重新训练整个模型，这个带来的代价是极大的，尤其是在真实的工业场景下。</p><p>另一种相对应的方法是<font color=Red>被动防御</font>方法。这种方式简单明了，不需要在训练时加入对抗样本提高鲁棒性，而是只需要在训练之前，就过滤出样本中的对抗样本即可。这样使得输入样本均为干净样本，训练出来的结果自然符合预期。这种方式对于已部署的系统来说很有价值，因为可以避免模型的重新训练。其次，它还可以帮助输入样本进行一次安全性检查，可以有效拦截一些不安全因素。</p><p>本文主要聚焦于被动防御方法，也就是对抗样本的检测。</p><h3 id="1-3-研究动机"><a href="#1-3-研究动机" class="headerlink" title="1.3 研究动机"></a>1.3 研究动机</h3><p>提到对抗样本检测，那就不得不提一下 Dknn 这个深度模型，这也是本文idea的一个核心的参考架构。</p><p>Dknn 是检测对抗样本的一个深度方法，它采用了 knn 的算法思想。我们要判断某个中心样本是否是对抗样本，首先将所有样本输入模型，之后在网络的每一层，每个样本都会得到一个 embedding。之后，沿用 knn 的思想，选择这个中心样本最相近的 k 个邻居，并将这 k 个邻居和中心样本的类别进行比较。如果这些样本基本属于同一类，说明这个中心样本不太可能是对抗样本，如果它们之间对应的类别有明显的不一致，例如，这个中心样本的类别是熊猫，但是它的 k 个邻居里面有一半是表示汽车的样本，那么这时候就可以怀疑这个中心样本可能存在问题。</p><p><img src="/pic/LNG/dknn.png" alt="DkNN 架构的核心思路示意图"></p><p>受到 Dknn 的启发，作者认为，Dknn 在检测对抗样本的时候，是利用了输入样本和它邻近样本之间的联系来判断的，那么可以利用一个动态的图结构，来更加具体地表示这种邻近关系。于是诞生了本文的核心模型，也就是 latent neighboorhood graph（以下简称LNG）。图模型的好处在于，它不光能够表示中心节点和它的邻近点，还能够通过建边来表示点和点之间的关系，这是 Dknn 方法做不到的。其次，把图模型构建出来之后，可以转化成一个二分类问题，利用图神经网络等方法进行分类。</p><h3 id="1-4-优势对比"><a href="#1-4-优势对比" class="headerlink" title="1.4 优势对比"></a>1.4 优势对比</h3><table>    <tr>        <td>LNG</td><td>Dknn</td>    </tr>    <tr>        <td>cover multi-hop heighbors of inputs’ local manifolds</td><td>only cover inputs’ local manifolds</td>    </tr>        <tr>        <td>richer information, aggregate the connectivity learned on the embedding space</td><td>only cover the information of class labels</td>    </tr>        <tr>        <td>incorporate both adversarial and benign neighbors</td><td>only utilize benign neighbors</td>    </tr></table><p>相比于dknn，LNG 的优势在于：<br>（1）图模型的信息表达更加丰富，它不光有节点的信息，也就是中心节点的邻居信息，还聚合了边的信息，也就是节点和节点之间的联系。我们可以通过距离来量化点和点之间的关联。<br>（2）LNG 引入了邻居多跳机制，可以把中心节点的邻居的邻居也给选择进来，让整个图模型的信息进一步丰富起来。</p><h2 id="2-方法架构"><a href="#2-方法架构" class="headerlink" title="2 方法架构"></a>2 方法架构</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>LNG 的方法流程如下所示：</p><p><img src="/pic/LNG/%E6%B5%81%E7%A8%8B.png" alt="LNG 方法流程图"></p><p>第一步，从完整的输入样本中提取出一个子集，称作是参考数据集，这个数据集的是用来构建图模型中的结点的，也就是图模型的结点范围不会超出这个参考数据集。</p><p>第二步，构建完数据集之后需要建图。建图分成两部分，首先是选择节点，其次是将点和点之间构建无向边，从而形成最终的图模型。</p><p>第三步，二分类问题，也就是判断中心节点是否是对抗样本。</p><h3 id="2-2-参考数据集"><a href="#2-2-参考数据集" class="headerlink" title="2.2 参考数据集"></a>2.2 参考数据集</h3><p>首先是参考数据集的构建。对于一个完整的数据集，从中提取出一个样本作为中心样本，我们需要判断这个样本是良性的还是对抗样本。之后，从这个完整的数据集中提取出一个样本子集，作为候选样本。</p><p>接下来有两种数据集的构建方法，第一种是直接把样本子集和中心样本给合起来，作为一个参考数据集，称为良性数据集。第二种是先对这个样本子集进行数据增强，也就是子集中的每个样本都利用对抗算法获得一个对抗样本，之后把扩充后的样本子集和中心样本合并起来，作为一个新的参考数据集，称为对抗数据集。</p><p><img src="/pic/LNG/reference_dataset.png" alt="参考数据集构建流程（自己画的）"></p><h3 id="2-3-潜在邻近图"><a href="#2-3-潜在邻近图" class="headerlink" title="2.3 潜在邻近图"></a>2.3 潜在邻近图</h3><p>接下来是核心步骤 —— 构图。</p><h4 id="2-3-1-结点构造"><a href="#2-3-1-结点构造" class="headerlink" title="2.3.1 结点构造"></a>2.3.1 结点构造</h4><p>首先是图节点的选择。对于中心节点来说，从参考数据集中选出最近的 k 个节点作为邻居。其次，引入了多跳邻居的思想，不仅可以选择中心节点的 k 邻近节点，还可以选择邻居的 k 邻近节点。具体来说，设置一个阈值 L，表示可以迭代的邻居次数。例如 L=2，就可以选择中心节点的邻居，这是一轮，以及邻居的邻居，这是第二轮，那 L=3,4 以此类推，相当于一个广度优先搜索的思想。但是所有选出的点不会超出参考数据集的范围。</p><h4 id="2-3-2-边构造"><a href="#2-3-2-边构造" class="headerlink" title="2.3.2 边构造"></a>2.3.2 边构造</h4><p>接下来是节点之间边的表示，主要还是利用欧氏距离来进行表征，并且为了归一化尺度，用 sigmoid 函数做了一个映射，将边权映射到0到1的区间上。此外，这个 sigmoid 函数中有两个参数 $t$ 和 $\theta$，是放在网络中用来学习的参数。</p><p>$$<br>A_{i,j} = \frac{1}{1 + e^{-t \cdot d(i,j) + \theta}}<br>$$</p><h3 id="2-4-图分类器"><a href="#2-4-图分类器" class="headerlink" title="2.4 图分类器"></a>2.4 图分类器</h3><p>最后一部分是图分类器，用来判定中心节点是良性样本还是对抗样本。文章采用的是经典的图注意力网络模型 GAT，模型的输入是所有样本的 embedding 以及邻接矩阵，输出是一个二维向量。</p><h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>实验共采用了 5 种经典的对抗样本生成方法，包括 FGSM，PGD 等，这是在构建参考数据集的时候，对原数据做数据增强用的。Baseline 主要用了 DKNN 和 KNN 架构，以及 LID 和Hu 等人提出的方法。数据集采用了图像领域经典的几个数据集，包括有 CIFAR-10，ImageNet 和 STL-10。</p><h3 id="3-2-实验细节"><a href="#3-2-实验细节" class="headerlink" title="3.2 实验细节"></a>3.2 实验细节</h3><p>对于每个数据集，分成三个部分：训练集、参考集和测试集，这里的参考集是用来选取超参数的，比如多跳邻居机制里面的参数 L。验证集是从测试集里单独划分出来的，例如对于CIFAR-10 数据集，本实验从测试集中，每个类别随机选了 100 个样本组成了新的验证集。此外，同一个数据集上只能用一种对抗攻击方法，以及在主实验中，使用的是加入对抗样本的参考数据集。</p><p>超参数设置方面，主要是多跳邻居机制的阈值 L 和 knn 算法里面的 k。文章设置了 L=2，k=5。还有一个是 baseline 里面的dknn算法，也要有具体 k 值的设置。实验在三个数据集上的 k 值设置分别为200，40 和 40。</p><p>最后是对于 LNG 图的输出结果的处理。在训练过程中，所有的边的结果是通过欧氏距离和sigmoid 映射来产生的。从模型输出之后，所有的边的信息又被映射为一个 0-1 空间。具体来说，如果这条边的大小大于某个阈值 t，那么认为这条边存在，赋值为 1，否则认为不存在，赋值为 0。</p><h3 id="3-3-threat-model"><a href="#3-3-threat-model" class="headerlink" title="3.3 threat model"></a>3.3 threat model</h3><h4 id="3-3-1-白盒测试"><a href="#3-3-1-白盒测试" class="headerlink" title="3.3.1 白盒测试"></a>3.3.1 白盒测试</h4><h4 id="3-3-2-黑盒测试"><a href="#3-3-2-黑盒测试" class="headerlink" title="3.3.2 黑盒测试"></a>3.3.2 黑盒测试</h4><h3 id="3-4-主实验"><a href="#3-4-主实验" class="headerlink" title="3.4 主实验"></a>3.4 主实验</h3><h4 id="3-4-1-检测已知攻击"><a href="#3-4-1-检测已知攻击" class="headerlink" title="3.4.1 检测已知攻击"></a>3.4.1 检测已知攻击</h4><h4 id="3-4-2-检测未知攻击"><a href="#3-4-2-检测未知攻击" class="headerlink" title="3.4.2 检测未知攻击"></a>3.4.2 检测未知攻击</h4><h3 id="3-5-消融实验"><a href="#3-5-消融实验" class="headerlink" title="3.5 消融实验"></a>3.5 消融实验</h3><h3 id="3-6-图的拓扑结构讨论"><a href="#3-6-图的拓扑结构讨论" class="headerlink" title="3.6 图的拓扑结构讨论"></a>3.6 图的拓扑结构讨论</h3>]]></content>
      
      
      <categories>
          
          <category> 对抗机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文精读 </tag>
            
            <tag> 对抗机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hpctoolkit安装与使用</title>
      <link href="/2022/01/19/hpctoolkit/"/>
      <url>/2022/01/19/hpctoolkit/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/hpctoolkit.png" alt="hpctoolkit 工作流程图"></p><h2 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h2><!-- We now use Spack for building HPCToolkit's prerequisites (replacing the old hpctoolkit externals). You can install HPCToolkit with the "One Button" spack install hpctoolkit method. --><p>本文采用 Spack 来构建 HPCToolkit 的 prerequisites（不使用原有的 hpctoolkit 外部组件）。 </p><ul><li>clone spack 对应的 github 仓库</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;spack&#x2F;spack<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>创建环境变量</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export SPACK_ROOT&#x3D;&#96;pwd&#96;&#x2F;spackexport PATH&#x3D;$&#123;SPACK_ROOT&#125;&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>刷新 shell 环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">source $&#123;SPACK_ROOT&#125;&#x2F;share&#x2F;spack&#x2F;setup-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>检测安装环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">spack compiler find<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>安装</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">spack install hpctoolkit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="常见-bug"><a href="#常见-bug" class="headerlink" title="常见 bug"></a>常见 bug</h2><p><img src="/pic/hpctoolkit/bug1.png" alt="bug 1"></p><p><img src="/pic/hpctoolkit/bug2.png" alt="bug 2"></p><ul><li><p>原因</p><p>  未设置 fortran 编译环境</p></li><li><p>解决方法</p><p>  将安装环境加入 <code>/.spack/linux/compilers.yaml</code> 文件中</p></li></ul><p><img src="/pic/hpctoolkit/bug.png" alt="解决方案"></p>]]></content>
      
      
      <categories>
          
          <category> 并行计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高性能分析工具 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

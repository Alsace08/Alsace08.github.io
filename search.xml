<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>TextCNN 实现文本情感分类任务</title>
      <link href="/2022/07/31/textcnn/"/>
      <url>/2022/07/31/textcnn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch 中常见的 tensor 操作</title>
      <link href="/2022/07/31/tensor-operation/"/>
      <url>/2022/07/31/tensor-operation/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/tensor.png" alt="scalar - vector - tensor"></p><blockquote><p>我们在训练深度网络时，不可避免地会涉及到对张量（tensor）的操作，例如维度变换、数据填充等。每种操作都在 torch 库中有对应的函数，然而，由于操作种类繁多，我们很难记住所有命令并将其区分开来，且容易造成混淆。因此，本文罗列了若干常用的张量操作命令及对应的参数设置，方便以后在进行深度模型的部署时进行查询调用。</p></blockquote><h2 id="1-单个张量的维度操作"><a href="#1-单个张量的维度操作" class="headerlink" title="1 单个张量的维度操作"></a>1 单个张量的维度操作</h2><p>对于单个张量的操作，常见的有维度的变形、扩张、压缩，以及在指定维度下的填充等操作。</p><h3 id="1-1-维度变形"><a href="#1-1-维度变形" class="headerlink" title="1.1 维度变形"></a>1.1 维度变形</h3><blockquote><p>torch.view(shape)：新旧张量数据元素相同，但是尺寸不同</p></blockquote><ul><li>shape - 变形后的尺寸</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 2)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; a.view(2, 16)print(a.size())     # torch.Size([2, 16])a &#x3D; a.view(8, -1)print(a.size())     # torch.Size([8, 4])a &#x3D; a.view(8, 3)print(a.size())     # RuntimeError: shape &#39;[8, 3]&#39; is invalid for input of size 32<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>注：若某一维度的 shape 为 -1 ，则自动计算维度后填充，如例 3</li></ul><h3 id="1-2-维度压缩"><a href="#1-2-维度压缩" class="headerlink" title="1.2 维度压缩"></a>1.2 维度压缩</h3><blockquote><p>torch.squeeze(input, dim=None, out=None)：除去输入张量 input 中维数为 1 的维度。例如，输入张量维度为 (a * 1 * b * c * 1)：（1）若不指定维度 dim 的具体数值，则返回张量的维度为 (a * b * c)；（2）若指定维度，当对应维度的维数为 1，则在对应维度上压缩，例如 dim = 1，当对应维度的维数不为 1，则不进行压缩操作，例如 dim = 0。</p></blockquote><ul><li>input (Tensor) – 输入张量</li><li>dim (int, optional) – 如果给定，则只会在给定维度压缩</li><li>out (Tensor, optional) – 输出张量</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a, dim&#x3D;2)print(a.size())     # torch.Size([4, 4, 2, 1])a &#x3D; torch.randn(4, 4, 1, 2, 1)a &#x3D; torch.squeeze(a, dim&#x3D;1)print(a.size())     # torch.Size([4, 4, 1, 2, 1])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-3-维度扩展"><a href="#1-3-维度扩展" class="headerlink" title="1.3 维度扩展"></a>1.3 维度扩展</h3><blockquote><p>torch.unsqueeze(input, dim=None, out=None)：有维度压缩，就有维度扩展，即对输入张量 input 的指定维度插入维数 1。</p></blockquote><ul><li>tensor (Tensor) – 输入张量</li><li>dim (int) – 插入维度的索引</li><li>out (Tensor, optional) – 输出张量</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(4, 4, 2)print(a.size())     # torch.Size([4, 4, 2])a &#x3D; torch.unsqueeze(a, dim&#x3D;1)print(a.size())     # torch.Size([4, 1, 4, 2])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="1-4-维度填充"><a href="#1-4-维度填充" class="headerlink" title="1.4 维度填充"></a>1.4 维度填充</h3><blockquote><p>torch.nn.functional.pad(input, pad, mode=’constant’, value=0)：不改变维度，仅改变维度数值，在某个维度上进行参数的扩充</p></blockquote><ul><li>pad - 扩充维度，预先定义出某维度上的扩充参数（具体见示例）</li><li>mode - 扩充方法：’constant’, ‘reflect’ 和 ‘replicate’ 三种模式，分别表示常量，反射，复制</li><li>value - 扩充时指定补充值，仅在 mode = ‘constant’ 时有效</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">import torch.nn.functional as Fa &#x3D; torch.zeros(2, 2, 1)print(a)# tensor([[[0.],#          [0.]],#         [[0.],#          [0.]]])print(a.size())# torch.Size([2, 2, 1])a &#x3D; torch.zeros(2, 2, 1)a &#x3D; F.pad(a, pad&#x3D;(1, 2), mode&#x3D;&#39;constant&#39;, value&#x3D;1)  # 在倒数第一个维度上，左边填充 1 个维数，右边填充 2 个维数print(a)# tensor([[[1., 0., 1., 1.],#          [1., 0., 1., 1.]],#         [[1., 0., 1., 1.],#          [1., 0., 1., 1.]]])print(a.size())# torch.Size([2, 2, 4])a &#x3D; torch.zeros(2, 2, 1)a &#x3D; F.pad(a, pad&#x3D;(1, 2, 2, 1), mode&#x3D;&#39;constant&#39;, value&#x3D;1)    # 在倒数第一个维度上，左边填充 1 个维数，右边填充 2 个维数；在倒数第二个维度上，左边填充 2 个维数，右边填充 1 个维数print(a)# tensor([[[1., 1., 1., 1.],#          [1., 1., 1., 1.],#          [1., 0., 1., 1.],#          [1., 0., 1., 1.],#          [1., 1., 1., 1.]],#         [[1., 1., 1., 1.],#          [1., 1., 1., 1.],#          [1., 0., 1., 1.],#          [1., 0., 1., 1.],#          [1., 1., 1., 1.]]])print(a.size())# torch.Size([2, 5, 4])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/pic/tensor/Fpad.png" alt="torch.nn.functional 中的 padding 操作示意图"></p><h3 id="1-5-维度置换"><a href="#1-5-维度置换" class="headerlink" title="1.5 维度置换"></a>1.5 维度置换</h3><blockquote><p>torch.permute(dims)：对张量进行对应维度上的置换，维数值不变。</p></blockquote><ul><li>dims：指定换位顺序，例如 dims=(1, 0, 2)，则维度 0 和维度 1 置换顺序。</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.randn(6, 3, 7)print(a.size())     # torch.Size([6, 3, 7])a &#x3D; x.permute(2, 0, 1)print(a.size())     # torch.Size([7, 6, 3])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-多个张量的维度操作"><a href="#2-多个张量的维度操作" class="headerlink" title="2 多个张量的维度操作"></a>2 多个张量的维度操作</h2><p>对于多个张量的操作，常见的有张量之间的拼接与拆分。</p><h3 id="2-1-维度合并"><a href="#2-1-维度合并" class="headerlink" title="2.1 维度合并"></a>2.1 维度合并</h3><h4 id="2-1-1-不产生新维度"><a href="#2-1-1-不产生新维度" class="headerlink" title="2.1.1 不产生新维度"></a>2.1.1 不产生新维度</h4><blockquote><p>torch.cat(input, dim)：将两个相同维度的张量合并成一个新的张量。想要拼接的维度上的数值可以不同，但其余维度上的数值应完全一致</p></blockquote><ul><li>input：输入张量</li><li>dim：按照维度 dim 进行合并</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.rand(4, 32, 8)b &#x3D; torch.rand(5, 32, 8)c &#x3D; torch.cat([a, b], dim&#x3D;0)print(c.size())     # torch.Size([9, 32, 8])a &#x3D; torch.rand(4, 32, 8)b &#x3D; torch.rand(5, 32, 8)c &#x3D; torch.cat([a, b], dim&#x3D;1)print(c.size())     # RuntimeError: Sizes of tensors must match except in dimension 1. Got 4 and 5 in dimension 0 (The offending index is 1)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-1-2-产生新维度"><a href="#2-1-2-产生新维度" class="headerlink" title="2.1.2 产生新维度"></a>2.1.2 产生新维度</h4><blockquote><p>torch.stack(input, dim)：将若干维度相同，每个维度的数值也相同的张量合并成一个新的张量，并在最外层扩张一个新的维度，该维度的维数即为合并的张量的数目</p></blockquote><ul><li>input：输入张量</li><li>dim：按照维度 dim 进行合并</li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">a &#x3D; torch.rand(32, 8)b &#x3D; torch.rand(32, 8)c &#x3D; torch.rand(32, 8)d &#x3D; torch.stack([a, b, c], dim&#x3D;0)print(d.size())     # torch.Size([3, 32, 8])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-2-维度拆分"><a href="#2-2-维度拆分" class="headerlink" title="2.2 维度拆分"></a>2.2 维度拆分</h3><blockquote><p>torch.split(split_size, dim=0)：将一个张量在某个维度上进行拆分，拆分后该维度上的数值发生改变，其余维度的数值不变</p></blockquote><ul><li><p>split_size</p><ul><li>如果是一个数字 num，表示将维度为 dim 中的值按照 num 平均拆分成多个 tensor；</li><li>如果是一个列表 [num1, num2, num3, …]，表示将维度为 dim 中的值按照该列表进行分配，生成指定个数的 tensor</li></ul></li><li><p>dim：按照维度 dim 进行拆分</p></li></ul><pre class="line-numbers language-py" data-language="py"><code class="language-py">d &#x3D; torch.rand(6, 32, 8)a, b, c &#x3D; d.split([3, 2, 1], dim&#x3D;0)print(a.size())     # torch.Size([3, 32, 8])print(b.size())     # torch.Size([2, 32, 8])print(c.size())     # torch.Size([1, 32, 8])# 能够整除d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split(3, dim&#x3D;0)print(a.size())     # torch.Size([3, 32, 8])print(b.size())     # torch.Size([3, 32, 8])# 无法整除，则取余d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split(4, dim&#x3D;0)print(a.size())     # torch.Size([4, 32, 8])print(b.size())     # torch.Size([2, 32, 8])# 报错示例 1 d &#x3D; torch.rand(6, 32, 8)a, b, c &#x3D; d.split(3, dim&#x3D;0)print(a.size())print(b.size())print(c.size())# ValueError: not enough values to unpack (expected 3, got 2)# 报错示例 2d &#x3D; torch.rand(6, 32, 8)a, b &#x3D; d.split([3, 2, 1], dim&#x3D;0)print(a.size())print(b.size())# ValueError: too many values to unpack (expected 2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>在报错示例 1 中我们发现，该拆分操作旨在将 dim=0 上的 6 降为 3，因此只能拆分出 6/3 = 2 个张量。而在赋值语句中，我们设置了 a, b, c 三个张量，由于无法拆分出这么多张量，故返回报错结果。</li><li>在报错示例 2 中我们发现，该拆分操作返回三个张量结果，而我们设置的函数接收值仅有两个，故返回报错结果。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM+CRF 实现命名实体识别任务</title>
      <link href="/2022/07/28/LSTM-CRF/"/>
      <url>/2022/07/28/LSTM-CRF/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/LSTM+CRF.jpg" alt="LSTM+CRF 模型架构图"></p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 命名实体识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表刷题</title>
      <link href="/2022/07/17/%E9%93%BE%E8%A1%A8%E5%88%B7%E9%A2%98/"/>
      <url>/2022/07/17/%E9%93%BE%E8%A1%A8%E5%88%B7%E9%A2%98/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 数据结构与算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 题库 </tag>
            
            <tag> 链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>形态学：语言的词汇</title>
      <link href="/2022/07/17/%E5%BD%A2%E6%80%81%E5%AD%A6/"/>
      <url>/2022/07/17/%E5%BD%A2%E6%80%81%E5%AD%A6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文所述内容参考《语言引论》一书的第三章。</p></blockquote><p>词汇是语言知识的重要一环，并构成我们心理语法的一个组成部分。懂得一个词，就意味着知道某个特定的语音序列与特定的语义相关联。每个词都是一个音义结合的单位，因此我们的心理词库所储存的每个词必须列出其独特的语音表征，决定词的发音，并附上语义。</p><p>音义关系具有任意性。有些词发音相同，但意义不同（例如 bear 和 bare）；另一些词意义相同，但发音不同（例如 sofa 和 couch）。</p><h2 id="1-词典"><a href="#1-词典" class="headerlink" title="1 词典"></a>1 词典</h2><p>绝大多数词典，均以“规范”而非“描写”语词为己任。</p><p>对于词典中的每个词，都需要提供下列信息：（1）拼写；（2）标准发音；（3）一个或多个语义定义；（4）词性。</p><h2 id="2-实义词和功能词"><a href="#2-实义词和功能词" class="headerlink" title="2 实义词和功能词"></a>2 实义词和功能词</h2><ul><li><p>实义词：名词、动词、形容词、副词等我们可以加以考虑的事物、行为、属性、观念等概念，又称开放类词。</p></li><li><p>功能词：连词、介词、冠词、代词等用来界定语法关系，几乎没有语义内容的词汇，又称封闭类词。</p></li></ul><p>人脑处理功能词和实义词的方式不同，脑损害患者和其他有特定语言障碍的病人对于理解功能词，比理解实义词困难得多。</p><p>实义词用来表达语义，功能词则将实义词与更大的语法语境连成一体。两者在语言中各司其职。</p><h2 id="3-语素"><a href="#3-语素" class="headerlink" title="3 语素"></a>3 语素</h2><p>语素是<font color=Red>语法形式的最基本单位</font>，是语音和语义的任意结合体，是一切语言中最小的符号。一个语素可以是单个语音或多个音节，例如 a（单个语音）、child（单个音节）、water（两个音节）等。</p><p>一个词由一个或一个以上的语素构成，例如 im-possible，由两个语素构成。</p><p>用形态学来阐释语言创造性：我们既可以把一个词分解为其组成成分，对整词的词义进行理解或猜测，还可以将语素结合起来创造新词。</p><h3 id="3-1-黏着语素和自由语素"><a href="#3-1-黏着语素和自由语素" class="headerlink" title="3.1 黏着语素和自由语素"></a>3.1 黏着语素和自由语素</h3><p>形态学知识包含两个部分：（1）关于单个语素的知识；（2）关于语素结合规则的知识。</p><ul><li><p>自由语素：本身就构成词的语素，例如 boy、man。</p></li><li><p>黏着语素：永远不能自己构成词，但总是词的组成部分，是词缀。例如 -er、-ist。</p></li></ul><h4 id="3-1-1-前缀和后缀"><a href="#3-1-1-前缀和后缀" class="headerlink" title="3.1.1 前缀和后缀"></a>3.1.1 前缀和后缀</h4><p>词缀根据出现在其他语素的前面还是后面，分为前缀和后缀。</p><ul><li>同一种含义的语素，在不同语言下的规则方式不同。</li></ul><blockquote><p>在英语中，复数语素 -s 是后缀，但在墨西哥的伊斯姆斯-萨波特克语中，复数语素 -ka 是前缀。</p></blockquote><ul><li>同一个语素，在不同语言下含义不同。</li></ul><blockquote><p>语素 -ak 在土耳其语和卡罗克语（太平洋西北部岛屿上的一种美洲土著语）中的意义不同。在土耳其语中，表示将一个动词派生为名词，而在卡罗克语中，则表示将名词派生为副词，表示“在…里面”。</p></blockquote><p>同时进一步说明：<font color=Red>音义关系具有任意性</font>。</p><h4 id="3-1-2-中缀"><a href="#3-1-2-中缀" class="headerlink" title="3.1.2 中缀"></a>3.1.2 中缀</h4><p>一些语言中还存在中缀，即插入其他语素中间的语素。</p><blockquote><p>菲律宾的邦托克语中有这样一种中缀，-um- 插入名词或形容词的第一个辅音之后，用以将名词/形容词转化为动词，例如：fikas（强壮） —— fumikas（是强壮的）。</p></blockquote><p>英语的中缀通常只能将表达猥亵义的整个词插入另一个词中，最常见的 fuckin，插入后例如 un-fuckin-believable。</p><h4 id="3-1-3-外接缀"><a href="#3-1-3-外接缀" class="headerlink" title="3.1.3 外接缀"></a>3.1.3 外接缀</h4><p>一些语言中还存在外接缀，即在同一个词基语素的开头和末尾附加上的语素。</p><blockquote><p>德语中，规则动词的过去分词，通过在动词词根加上前缀 ge- 和后缀 -t 构成。例如：lieb（爱） —— geliebt（爱的过去分词）。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 语言学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 形态学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022计算机保研记录 | 夏令营经历分享</title>
      <link href="/2022/07/16/%E4%BF%9D%E7%A0%941/"/>
      <url>/2022/07/16/%E4%BF%9D%E7%A0%941/</url>
      
        <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="抱歉，这个密码看着不太对，请再试试。" data-whm="抱歉，这个文章不能被纠正，不过您还是能看看解密后的内容。">  <script id="hbeData" type="hbeData" data-hmacdigest="01be6da8e7dd0a9872d83454bfa5b5885501ad99349e0290ef8f5924d3d0e346">08b2f2d3ee18fbe352ae6912525a7fe1e2ccea5eb06b85bf256686bb549560d4e929a0bc07e5fb0b56008efa48ab5e676fb9797015111c04133f0bf00e43fc4521bd5c41039fbcd19f8ed9727bd2ff906b263d18188a26771ece3d88f05df70604d63283cee7cead0472105150b4f8a48f2d5b4b7423251367ba43e24155c8f1452c031e4e25d3e3aca650aaa641fa67a1def5baa8f16afa9196dd614c9d7258d667b470e6f03626d9cff43e038380b936334634cc8041004be4ef0fdf68cadaa88165a2856dd0b8e7a0c8d078da673999a0d9a4981ed34b1b6fd88c99cc7eb64d60cec1f75674d8c27f33433e6d8eba0da6070ce7f480b870c3c62a8a0de00e18d7c01d71098c3e74f99c72ff2e361903e42f0e158aa1ebad1312ed346d35dc87894bfafc1f61a2d95f43520c6158075a793702a87481bfa666a26e19ba570106f5232fab0b5cec6fc9847203516a825d97df4471618d2dca9578b516a1181cd60a8e03a24fcc953fb04a83cd93def826482dbad8ae44c35996e7057e91497290570beef4ddef4c0a0b6e47ec164d85c803a52da913ef0d90458d5f478abeff00ac00c8199721faeb35e2833cef47590876b028612a45668a7b14c8768d3102eb14e7adfb35d85d35524e8f12507fa84d9b9a033a0cd7d40dde445403bac19896d17555a8579ddfa8f6a7f31525faa79f65336f04b5ac083c307957d76ba3815700cc50ad2a41cef49594e7039b024d8a2de7eba6984d99ae347feb7d917f30e90a34cbd5bdb695157e4672695185dab04e35f21686819608b76724e9028adddad4d727fe2d39b1ab0d7750964af734bf375f9e5e84a66c818177e15e288f44be44ff4765fe3eb9e89be335701855d5d08258602570e51431c4bd0b7c3ef2da4f446e954518d3245ff93db1624bcffbb14fc64d964c40a7968606e6bf2b576609f5d3277ea06f24853bd1e6798ff237bc2b4e27f9c671147c76a112db6223b113c3c0c773756853e5dcd6014530c5e9a77b1a154a98731fc6faf4536290a57a35f6149bc8c5f0d949accc3fb60e289f55d9289072324d4f01282c36c671727b57e4e102e6fc835580f14a833f6b054786aadde3d588085ebdf2d6a2f0caf332f1478d6d9fb090832ce828d35020d7d5e684518a81a8dce25b7048a5db42ede8c163462c97c493e546900b24cb1255ec2da3b96aa474c5bbad25f46d23a1781169ccb1ed22628ce61aab8a3f81d886598d0bfabf057ff597d87ec1482354998ad44d2fd86d03c3e2b36fa167005ae11dc89355c39f306b07331e5879517f284d918c5eb2b78d0a08c07dd201384765c14c6003a099aabb446f43a738662c5c88e1184db1d51024b6c91be2bf870dd439e84bcdc2eb5f1091b5452957eee469ad4a0542ab429604959a17e564c8bc411e875f1495dd18418510827fb93799ef25d867ad820fe23b08585e581e1e33921e7dce577539f617fce0518f5048e64fede5ec3f708b4a450afbb3e6f59fafb89ebc38af9d0126ba567d18e150814185baeb9f29b7004e58aebd0e365dc173aa4b436c47b2233c57b5dfbf237e45388128433b2160136d9ae018efb93a2b521d350c018980ebab5ef4f241970da243ea77e3208642b49eb6e01646699d33d56ed710e1dbbce45c2d86d6e0cf20c5c58801796683da40e365f2c48db3a8bd38c19933135bff8db320f660c0c76311f8e0f997696ef9c6f15d6559d9d4bd98b57af0d8db6ffe42280996cb763c2fcaf7aee9cbf418ce39516cfda2ac08418d572629fe1a61b0499bcb00ab8329cbfb5d5f68d84354eb28bd4a955c9bb0ba8d3fa11de3e16709b4222697d7c92fe89827b7a4b9c3db7e9cf7dd7f8bbc95c02c8d0e65a93979f8bf42b775c380ea3ef0c5536d6fb57f90a687a0c0ac3db06c86f5b816e51b2c1d201ed17e7a416d6b6ac52675d66e001ae8ab4f1dd809c2d178ea1f5bc6ee6219dcad5150187b9b8787cecdb8669ad2371ed58d881d753a1ede4a1a1c7a7583e85bbda4c1a2f2add8f2a73ff08a18ba9ace65e5f1be9100750bd70616b81350e79247343def64c5b6088eb16afb2ec5b0c5cf4732c26facfe25428c8c9793f5b662957555cb7d1bb262194e7778e6578af997fbdc313c40e997aff9074064568ee63352561ba70c2108c1f2b6e071fa2c499e53d40238c652276df6fe8579f1d337695321795f0f0321eeeecac8730cc1007dc686dbc6362bc52cdab60c4a1e9bc9b46bfce82060f75bba675efcfc8b7b1aadbaab43796d0610259575f79dad805027731a424c668be984d56aae97f4922c7e500365c6654b6328eacf9e20694deb6f3fcc6f8ca9076a975caad7c9052065efdf1f84340c844478132d3d2ffe30b4e7bb3875f48f788dbe9101451c82a56b51f1e6bc39d5d78f84cc05adc937bac2d095c077aba6433b2c2316cdac535ed18ad212b5ea3c1e1c863a10f163e294521f504dd205934c8279213185e68cee4c75d1e19f7bf74dc0c0d32682ec0500002274854d118c70014c60ba46993e3d2c302ac365cfa7aca8b3c12f970f15d0e2ffd6b4cd4c09052a0a37bd58709d245167bb68cbdda004e6525bc90e515ea90678c05f7593d4f83706aa079675eb66979b714010dbf7a1d253212beca6c625cc18fd2d052e868551c1457ddfa71cb2a6700ad103d3da3e1ce4e17e0fadfc655e8a975a6b1c866398998c6bcf06b5ecaa3d8c34aadb543e89d8ab2ea3694a9eec15a502f390cb0401732917f7300cc971566b4b78cd6d0f3f92c02aac094bd97f256e91eca05ff0e2c840b5bdf25f932f37264a39abf863bcd5b0ede2c2f3ea3bf5f24906fd746c2240643ce89a6b1f2d3ae9710ce82026d871eccc57a4e88dd686392e22ce5d0323dea08626d828af5f676b9b3bfa1743c871b14474c732d99e63a63ec77a3806f8000f4526a93a053f8471ac5b0eec2c8fc63d90fbbbbb4b8b7546f27fc46d8953138b02a0e101192d5f497874b4c72107017466acd47353e838f0eccb16aac69f21d74ceea7e9a9fcb845be2dd8c6a64f6a7ab05152508a9b03c825dc1656c7c38feddf47a3a7d9c2cb6e02ffd4c7a94621f9cb4ca67d1e463769b990c31ecf9cb6433c47f22010fc631e2a5f20dcf6066574d49ee157bcb5fc7b689e2a478f699896ef3962161db4f9f336bb4a01d122886175ee24800cefef72b14cf853be8a5137e3de99a3f0939e847b8f39527bfa0bcef9a47f5e0c4c0d16846de6e4c726d2e742f00d37b87623e5806b75a9fd8441960f6b919e97c14a62818eb10809d557ad0ed2d2649c84f607b6b391f044147758efc4e654ada373a6cbdfbe349331a44f558949741d417199eaab7e993cd766f366e19730501b85c69f1ce997cb46145d5cb52b3780242dbca6e07d2630f2ea8febbc79358fd4a806e3590639668e35170093b31591f0c39febc6efb4ffd52ca8c51c3a80078c5934493fd239bca707e43856cc88a4eadc80273f766a0725ee9a08390a21bc311be8ff44fc85cdc77cfb0cfe057587c2cd9139811a2c5c2bc7ac541dec486f845ca7555279c8114cee8cfca1fdc3de822b38f1a023b2aad8e871d4c7510ab4897dde4088619f275d64ff0c04c4000ec5d0feb68b9cbcf9ca3ff4e691443bc119645a9921344b38a6a3e73877a3b3c6b9194fe46d29f50dbb8f218b3d55969e5eb352d4337a805f74d7ec0bac0529efaa0946c54c61a7519a18fdb079172867c01dc11feca18f20fda0abe19567d25d351ec6ac536b6ea24831580435826ddfe3296a8d07a5d0a9161cb356324fa005bfcd852d586364d3427c0af9a939e928bd6ca79d8ce9e194a70dba6873465076876d72fab2ae9e63d17e6e31b0ce8d8789ea0087d33cf7d8012ff5cc7345f08df5e56c6950d13813487df8fe50f616e81da3871477ba97d51a8e955951eb0872671dd20550025abbb1c0a119b7e1eda452411313a58b37a819e0ec509a651bf9ed49175baa822c3e8b0f52c2997d6f0f38b2bddc9645fa3d7b72cd4386f8de8e2a44819659f2cedd64b79e99c5fd6c269b936a3d68b76aaef732f88bfd87933ca1bf0d6a2fcc77178b5dd135efbdb6bf7603cefae016551e6b7da88d1ca13e7b34e2c7da82f3c32dd4faad5ccd0e126531fd293d77a2bc275bd0c745cc666c996b1cd8faffce0d8b3afafc3ac7c3ccabdba3684db15580ad37da1569ff5402e651cdac8c446287a2b6fdd1691596a9d5c7aa33ab123be2fb59bf499c3b625525a5fe2ba622b93b112c527a5e343514913949830ee828a2ce459a4c6249f287e930b6766f4bf47f9fffc8fe1e9a496aafe441dc18fd8fcf7486b0b1e234f52924605aa647cb1ffeac1a8576a68dc2186b9996502872242180b12b80f63917a0a8aa487ebaf10e5df904f96a2854949f365b9cd6fce17a638d53599344b1c308f58b1882ba15c672865d76fdc51c39b1d819cacfba33a5fe51cc1633fef81e140edd2a53964b12f9ecdef1eb0ff69bc1159d6356c0181a18640a7781f8e8a746f57c480e80c7f8b6bcc2e5f83c0bba772d8971cdb98b1eb9e39cfa6ec0571629268992908b41c408d6af68670a3d23e911ac2fcb65186939cd22c4a7a55fa7f13a8ca1b7124bfcf65b6d3cbf789f7eeb6d8339a710ed80e6c92caa831ea28fafaef603f25d73aa050ae976a7635b414964982a949723a2143b2cebb37da3791bff32bcd8adcfdf51dee5bc05e277c0e0be185786aa6183520f805da6bc06358e8cabcdc4ea04539246b85f0501c6789fef4dd30e9686d26a37f8214147526a81960cb3a1e5241d45bf1f4c88820c716ed89fdc0131b192571653acb1213e2837755517c0b064718f1aac0adf96c67c33294beaddb6b57765ac7d0e1e6d4a5a9f65a3fdc2cd8c176dca314d098297e8a7c1eadfc38a2d482d420a0a42d1e52dff4fd37dec33b3cacc208675cdd3950a369c2e468e2615afba7cdeb14359f6cd0e03ff15c0c36321ebb581fccec63cc304f23472e2860bdd6c575e63df939da808718fc3ac13c9950b191f5501a714eb08b08b060f179b18e392990d099ff602f623f6b26cd49632444327a844f28581d42df89a9ee73baf6ef874c7177840e2f05ce5d1b46ae8e6995357f6d955d54f1d49699512ad4b95439e607f19d4e45ab4ed2afda945a476574c376ed704ef866edf77848bf8f9abf50383dbf784a9520f6744e25df2f54a666fe0671054dbf44643261ee5903421cbdae7dc1d1f2df19170dde7880d7f59a4889a9b6e8d2f1f1e3cf76ffd5952c920a272963d41a0e8fc7172281dd8c9da473540b153ee39a856572933bf809fd0d0d817387685bf5f588ac41a4a8729c05d37d1e7f7295e6b045cae489f9ec89e82a852f699156995e6be069ea017475f6b7f9fb5ab9fae36e986ea160019ba3502af492c41b9709accc6593c146808931d3501ca3ca32c9425d129d902bb8135ec56ff912471640f0d7892931f04fe71107fbf8cf5f735b6e27f41a2a5011827adb1ae82231b93ea1921828f77304c3fae75e61442022553b0ab8688daa7e7e8ec53655c4692eb8feefa9f9e4239ae260fd717a724395ded93651be98365d0791912cf9e1c7e12dae0580323a47872fb29cbe7c8f76f397180835a83db9ea4cac522f6d7f5005c57f35cbb4a577c4a2c96a345c1d7823d77d36652edd540dbc8a92c14eedd055a76d0321230b19a3b903626332e4ae2eca54264219f4393c5c5a1cff8ae48746231cd783e66956299bf44ff06681f6bfba33305f9a2f33c492ac5c68c74a408bfdfb9c7b0c382a65a317076459f3f13b269e46126e2628760394b06a84d80a0d89dff730dcebef304917aa4668eeda8ea279f0373ccf9d5266530e37fa82b6e4d6eaa6f1e87e7d28f1e963b5095dcf001ea66d68a6fb8be0fa6154b13675d0c4c63e1b636adf38335072d59e8c580bb969bd4d1789e5f5c30db102766b9e8922d15b8347f761571f4989319c35f5624dd02bb4aad23d76ac65d8759e758263e3d4b818d9e6862494abed4d60df4a8133527197873840469119101e1575ab74b42bbfc2acac99dd93d07301c41755a8b41c03db047cd45aa2d2f992cd6aecf4cf7a0f2395da1240470e7c2513b29ead9c9be7fe1f690cc1af84308d084b265d1c2191b9bc384318204acb8ddca498990fdd5ba4df829d3834fe3cdc24d904be654a3819dd81063c8a64bec3bc7972962fd8f4c28c383df12ea75c32abbcc1ad933ff25d60a688b15dda80a7558fb85228c43129f4732ef18d044fb1b9149151235d275cf857abd8d2b1c97694b125074ac78a1400624d7b65b8122d2f3198cc7b5cc56abc628f15adc8c92c27644b16116e0627bafecfaf578254cb3e9f99ef479d294afabeb687f48fc576d23bbad16b4cdc04348f67d9088f3069997ca19ff33a7ea19c1be5e56c58a2562bfe601017b0f8679e9f26353d24e07bbba1e5e830edfe8a3a387f86f5afd02aa245d5e2ebfa6d2f4111857d547175893762644101042edd9715231c7c1acb1b18d9d85f3bebbb1f1a75f9fb65e773dd0ee42a7e51f5004306c70f42c66724ae72c1d153fd20ca180b6a9277b32e42dbcf825f1bda86583741505ac07a4f33ffcefe616139587b4de7840360558ea41cd7ea8f9aeef31da25b05204686dc43e4dedfe8eac46472fd64dcb57d6714858e16b6b17194ec48b5d5307bb4ccf17092a24521a4e09226b65ffb76a868f00a480d695aaebbbc36194194757c9d384660346b3fa2662e17c0f909cfd3e4f5c1e25efcc7ba715403699d1c227fddfbf50602b220b098e62ab6c88b4095c3b9f5408d4ac8a7ec3f2e24240543ddaabea1b682e90432cdc35d8a87888f303974d489b4e769dfbf2a1e66b003a258b66447f36393839b32edf835197c97219152b69538ab6bcbdb524d3ef6c7eeb2bfdef1aa57f55181302a049de912cf9e9377024ba3ba2b2528413dae65dfa3f38c290a3f3db1b416ce3aeec5df4912dbcc7f795ac056f633a470d9c311137cc9b3aab1610126ede59b451febf3b853ecb7cd520d5c2454ef75ec5f5b7442dbd70fb34195c6b96b37c768c16b94b633f3dbb51d746e5cfcadae0e5e96d72c46b1d7e35e39d714e734e6908898cbec616a15378a03f563d834a7ac8eee25c9eaac460ae89c3b472597f055d837d0d2d4396cc8baa576cb0b1c7e289b5592c833a442eb24b517be0a0805821051c6ce6808ff266695e0760d5cc3ee3e66b525c1bd8f5c10e7ff3470810ae3af184b8bd7d6e5ddda03494714a7467fcefe497b061cbe01c33ba7c45dcd66e4741c38a9618794c0f1eecea94b8dda01e941c3aae71cc9f101cf357035d2e4a97f5d56018f6167f455a27764fce52164a9b464f716fb9ffed4522a3bc361a9f38bee9ed0f4660add51f4f3ee6111bd10d59b7057ebdf1add69bc2ae4fdf210af8570dd44ba5172b2a52049015a92a97dde378f946b6c48f831d7ba3205a78c88e06f1474c4eeaf5e5fbc15f0a14ce1143d5e260fb7b7777a31f5799426f3ad64e188b65d308338b9f6419faec3f280f7d772af1b57a41e9545fa6bb59731b9deecd218a4ebf2e0c0db789b35b6e016342ef0b2c10c8b3fe274bb5a4e6ce4cf4f671d7fc331e0f8689e1203ed2885621224218042898c406c39597791c7e3b28b8af14b0b7dd26ec24b7218886e621a85910675f6df4bd509b3681c19c024f97ce5283601609a03bf6e347bb2f0d8963daf76d911186558d6ca064a657a7a5cc37bcb8a31026d15b1d7483996ea472f709d3a00c70ace445a9654d618560ee9c55fbad1d2069968433c0da5fdd9cc0ffb15d108ef86f83bbf553eb35501d7176035dd6d455736265536d21bd2ae25759e2106306654fc62826a839050a41eeacb85c0673a491a1bda9b73a6254502ad705889bcc3a49ca013878ff45eae67217e6e64917bdcf98ae47e79b6c0cfa7d6653aa0cd97f53d8178094c1b3c5deb9194bb95dc6bb58ba3fb0681f52872ba8995235b01f60c321d70875169ffe21ba2d9003962c7db302d3fbbac0a15262a102dde3d87b8a8ee36d034ef3107e567c13f66cc5d1183c567da07009b953fdc0bd14ac39abfd1bead0635598372564826116b60e56534f604a94b83281bb41c67e4af39576e8dfbb3b604227b39b47c263ef4eb5e334933b4937fc6559ae52406cfd9a8a141215fa8d3321f41bba41dee5d12a311ca75e592936e372e1023d1e59e5ea906277bdceed09c7aa0aa501b651ed60e97abe2dc290329f7df787e7fa7a39ebefb0d337a979753b7c07767908106ca3304dad04785eb1aeaf8687cfc20b238d9dd2cf6f568eea284712404773dc4507a6f2f3db65edd207b8153ac2ef4e30b10e60d69c9ffebcecab161e304c9fd4a6d51b4ac7d11b32f76acc282e1b929ce24f2657a6622293e8608d1ed867e8a930b029726dc28f5603e559bc45b9cef6fe76854c293cbdf97e9e36b739369cbbfb36e1ddce1f1273a8907634d5cd1d1def9577fdd50f12c78b35b2a1a4722a9c2656a0a3d96344db8e5ee304a606550a121b67439f8d16ee7ffa99fa6295e9670c5c1f9ed5eb3523d5aca56f82fb86114de7deeb4eff75fd274bc174ed75c53683d29f2e5cdc1611e3bd80bf6978677d6402135a953ec0be09f2c437e5ae271715084f47e2a5e4d399645cdb42f5f4bf76081f5646566d3a50134b5c117e0f56c4e5e9565d26f36d7b71e7afac9f9521b57f4f9d2218769d461889f23edb3ac0a05c5f2c7a2076be0b1688badf6da56740428f7be7abe6c1af265a3bbc02562eee70296335acb48fc39464156db460ae12a477a56925afaeeaa8cf595acb2ac4565fd384418decd965109e3ba1e3e72bb1871c85d695ea70e62bbb3e216fa410ee3c8c089d98796460d074d0e31c3b50c6153cdf8f1c06200521fa7e5e640fa422aab252237e1f366287ab48938ef1f770ff538964b8ac3b96efac1c41993fa8b033aa43120ce9149479bab10dbd84d0e851440ab98f9c6e70950b7d06c36bdda719e5cc49e83b8b305024bd8bcea3c536f29b3eaa474b94e8558a2d06b2c06d37c944e11c5be5b85e97cec5c92e03e1dde6177ab43b403d7762c4843dfe533da9c21bb748155532de45dcc54c24396bb6103c7f4aea71c3ed42a8788c1c62fc521cf9e7a2a18d6f4b6566a741dee3edc63beb41fbdd5e10f6c0e5fb7c969db3945c91223c9143dddf5558d26d32391e05b7091624a12fdc77bd951ea50cc10fb7e2922c959ea3836efcbcf75a2bc48583d2a129e2e8c9b46268ec04ba9224de04241fddd70f2defcfa4de9438146c7a9f6a1a2d524a892ba46025df7b363b93c6d5bedb0fd1e64dbcd2d3358888cda31284e43a4aae28bcfdec2080431a987d305aa0a0df8eb652f46a33e8c86ab4eacd8a6efb05444309830b338bdc8b084b72ee888c2be81f3339c06cfa0a37d4900b5a6ac771d48b820e6702b5361059dafca64f107d06ccc78b6642e27e7b8963c2cedb4b1b3d21a2fc34a11f22323a4e8293941ba9bc7bd3e95c0e93ce5031e74bd204b76c8f2808e3db197cc421dacdef7cbe8bbaa4ede11042947049008d40263c4af19c650752429f494afbe027bd8e078c89e0984df700b03e7e97b7056c287768b9d6cff8e11cba2af60d3433ffa4942c353d64bf17eb2d2774d265e77e4bdc706b0fa5769859f1f1b2a4dac56dff0b0ae33c3093cabff464ae2b0ed3e18343442cc26cdc40fde83627e2552dbaca380ad56528e4feea598f06cbfd7ee58daa8c4c8617faeb07366fea51bc7cc6aa98cff5f3c3992b98f57ded87cae953e9bf0396b07b2bb5f4a4932293e311e66e53ffc5f5b7dfbcb7b1f68ad7607205990516ecda5cf1caec2247008b7b04027c37827e29d20c72aee84c2b7c0363fe6935c39a54585f3269ca0bf5bdaaf09f5c7e920bfb2110df44a798e383cfe7562e7e517d06b30238afa9be97c9754105fd3c63df69675f0d7f07b3e07d3631cfed2a8b8c0ec06c29498c7eb7af251d60c8cc1e7beb3be34e04a07acf2f91fa8d4a4caf6e2b38400b3440cf190453f8a6b1d8785ede8bed374cf35e9b0cad58517bccb84f1d024623ef6b6448f6c1707ac9ee713811f1aa2f8b7bd9900f8f9b8ba5c812f7946eb967b8cc3b7bcc8e31d192354a36049b146632acabac89a43e8cdee28a6f57b2f1baa8e31ca17cc458d535f75ccd9754cfe229aea618827dc1bd28cd347cb1540c14734c413da4a45bd91faf5b0af46ac31d732738bd00dcaef4b74b29c9f23146996f0e22fb4a0619d3322fc71fd76d427674468e4a1fd92a75012e6c13e10bb9fc5f1194cba385a2e44ecb7164a4b4016d34294b3e458de62cad5488b1cd749a0caf3f57e1e975949c28dc7b553d72b73a5cf84490c925aba3450e693e9e008139db2099079923d858cc44cf02de35b55c2520420322b47c8729801c4cc3b0e29fab7355b47af08c2324a9557466f667d86c5344f866ef9ae4bc7c63b664a7ec5674ea264595d034776b1b55d2eee5b53858149958210e84a47c68ad3e4e7c68f2e97455bf7736bbb02b5da2fff5da5a788ba62d16f5dda8ccb5d8e48a5050b8d59dcfff3eeeed3a871aefdb4ffa2e353b022bf81680e9a805044dfa41394e8c968b5e5425b870178236f56e5163d19dd6c6d31f351ddcc65f37657f231d0939d5698a002a2d29954d593a191466331a98d7b7549fac98897538c95574a0f1cb87292b088452ccbceab6c644af92f5c772b382335ae4dc38f0620856960c74c7de863d503a0a140f847413a2616cb200df29a404e61584c342727053094b1dfdba84984d26a3bb3a14dd38e64722eeaa2dfa324a30d4175a8a501046b5bd87219a2b030f6acf46afb64ef8830b2987b1070b817a62861c146985a11302ddfb7e65a2f4ab3d5499c13e070d6fe9fda66d9f43ab22b08b8cbdf891ae36c8dfd5b2d18ad3f6f5841ad6efe1c33dec17ddcf741a0d6da38d965f350c08d8379f0d7f34a43d267bc1292be7a4c930436023afaeedc81da0f4626e81b2fecfb149defd6e7fea29fd542099f879e8232860dffcef9db70f95be8eb92017488211e62391c542735d55f133c375aa0a78792280cb81c74dade8640c52e9a0abe65f854a2bee57183f1c0dea47c4f7f40496d96620b6da54b51cfcebed6124919ac9e46061b06cea3500b3f370c8a8d5f2cb582c33eb32dfa0da5ca47cd39670182abcf3aa695707d49e1cd5d3621ff641826da21b843a230e6e50d4857b8101dd919a9b2536fe23f2cb5cc546191985de2415f5dad1961875c36a6fc61b546c1dcaced52332848741d7f88403d8bc5ac486cdba171bc0c0592a78da876be0a9f07dfb33731740190bad9d00edc875433296772fafedcf1b15239681757ffd93ea220cebac62c43a426f9475a4c60193f6ee165bf9edcd7739b2652cbf58c16dd8a4c2f792457aa021a018920c81703edb4deb6ac7e0148080a91e284abe0135cc15237883c25c9ed6499ece4a90605906eeb51dd7313e7e8066ad9cd02706fd6eae04669faaa9c4eb6de0b1b9b526ec552e8b6f2862f2aecc02011ce92ea81449dd96b9f9370832be3e67540b7b7258f0bcf6bdea931060bd3f3a0281afcf7d1b81ca73ad1ca0d17d5a5e5876ceff9ea04fc1d804fd77b4b7867b42e571a1313a450d7c0663b0a1e7ee3efa4b4bcbb49e5e66bccf01fe3f19f4dbe3d77e515f6aeb2c076d3736ef442bce8223328fc898a80252eb6963a1da6e0dac5343177f5f14b73599cf61dbc027c2543fc57ce25bc9bbfb2711f04b5ee62479ff416740ac27b8352beab69a7c419ea52063481d7c99c5e39693f0774fb3bf23c78648372cacf55b61f21c316dc8998d764c127b23c255af9ba684c3e0a9898adde2678dc36b0dddef44fd4e980a30d3ca0c1ed9207ead8190b73ac5c3c3a94c74cb8ad65980da9a8f9a78a9f30880d7a27c7afa39774c1b1ae150d75875654b336bfe47a851e82553f1121d7128736a05f5064e335345e4b929df36149ff654646163464193af9e367069f18186dc873569e4f6ce6916a18dbe9ce2a75dbac493307ecf72fe2934fa92bb8c886799e18d47dd3624583d40433850db4192b6d8a9d27d713abb4642f3660930cf20d0c29180daeaee180c1ff7eb72692ceffd103e4de389ac884591e4315b4f45ba7a133bef594d50d9f033779e72e10ec7c821199da149f6dcb7b43e68b197b22504e52d348ad432cfc48a1a12d8375515d59da338f0834b9d3671b3ea2c7aad64e5fa4bd8166c03cd275a06d4c6acc254b181a495a5573d17e04ab6f481b19e7971e0b66f82c90cb575448dc85a73225ef40bbb9540c9ba25c60561c0a26c0bd8c073e8eca96dcca7f12b902c98177956ad57615bdda5d63d0f064d75e58c53138a298d8f7071f827d42d3248b5b6c6cca3c16202957adcf872d78bd690f6751cc83f47ed510856b33539bc44eb26a4beed65c401cd34d030be482e4d3a6c1228aeb7112f54b87cf54d3ef5f695ab1fe314a56acc7be4c1439eb5ed1f6a35f0e142bd928ad6df5677b0464388cfdb84ee484ec8a15adf4c451449e8cb17f9aed978ed9fc1c009c51053655e47a18fbdaa6f17f1bcbf34f316b801e36b23bb70b3950362108c7351556717742477f428c3a336f512904662d0731dbf70cfe10c4cfcb5aff20ef5e7f15b5e6ab95fa541fd28078511698f552ec048c48aeb809be585c2fc70bbade40fd2aad9e854dfb0601da61f744faab1c982d0090bbdde6ae396dfe3675768e05b75a44cafc1f826d02c3b79f969891d7c734916406d53ac691e57a00292529fd7c5fc4b3e6cae3d47183352750013cc2a57bb1a196ac002c37f1b3915c625f2a880729518d81ffd52c0baa60818f43898df6a5ee2aa135a9385a19fc9301e07602c4bf1703fd6f4e458eed2c8a1bb9b20ffeb5b935db9aa1552e73e1b6ab35376dccaae08c47a0bda73103f1d6baab76314d49474f17cf2d8ac709d4e37038c12d65f4491c2ecc4fcdc5114125710018a879ab045a2502adc0bdb395551c16e9ef0382d8e5a10d5e1babebae84db4ec14d7d1dc05f7b7ea23817cc9f4dc1bb7b9308a74489aa361a318cbea241859b1daed87c3e919ed7079547e636627e8d3ac89563018a2aeca4f2a8b59b11c870eb6e56bebdd41c741bdc818dcca98307c838af9f8bf698765afa7e7f48ee8b5b3e3e33083601c4ef92d2b9684da17287e91f2f68d996e417c8f75b04330f2147052f60e763fb48215ca17b72f13ec5909ca48bd65dbef20717cec1eaa9bce8f6fa9e5d7786c9ad0e63bcc7b58b1182cede3bff945db6521c129c9b20ec1b77f3b4216959032994e46164711fdd58b45f0f8f818a93f03d99eaae7bec07fd4a374107047c9adbac67fc04897d0df63cb1ed21058ee3b32498e6d6bfc6b923ca05843339eb7e32e53b18cf6093d48945f97bebf072c77be25932ea8bb583fa906c55edfbbe85a11e3b638cc22ecf073487802824dd814d86145eeb1655fed44b9e4bb32274119035ae3230cb8e2952f6847e80c920f98bf7345b29b620c09b0fa68037cf92d64b16cc5273d3d335549bd314b9af03b0bbb8c997e963fb6a6e2edd58284f9a7930dc189cb7427322879029b7b33beeddc10f089dcf69a0f76f221448b08649443726354c3b48261638543c31af710fedf2be7c7ba065552d60bd9fd912dd33b45c1a7ed27bbfa26976c4256118226854428044b3b5f2ff0967caed0ef95707a1c9c391e7d6ce03f036b6d3894344f589b137841ba096c1809033be40996846152132070ba99214c3d753e0baba0a2ce2d236ca73074eee6dfe66545cb7eb61c46633baf6eb105234004810243c19cce340e7e42e8f59ee99a81e9782d9d6221698e3417cc9f36c2993f3dfbb15e32591b1daa164025c40d70ba969ed019bd7d3b1761a6561fd5690339171749b18381113325a13f887b53ca3d8b2810aeef61d653dc71ad20fe7bd4335ea34bce8bb678aaa924edd3140ba744878cd01548fb69ca1f3fce2745e30a8fab54609a92bb5a474ced36de0c241b8c6ed4cf5474e520dfa4a4f1d4cd28d192028178bbb61e94e38703f452398642ff68829f974a4aefcdcf7fbe1b63c873fb24f07670d80642d3bb066c233f25eb9cb9302cc2e2c743df4b56dace94cdcb0123b8d45ba1fe0ebbc085eca57e85c0716b2d00816ccaea09942465985108c44c09272cb32f0abb62cb9228331b057706ba24449883d09dd9c5e4e1c308cc80fb88fc5002a9c203e6b669209a37bfc5eb866eb0f00a4f532ab38cd20544a32589aae8baffc5c2899bd23b80a86cbda26f31f0fe4cddd3a8ccf044b47bb39079849817f387b325447d4f5fe4bf325edd06b963b3ce547dcd2ccab3408c41e6922d7e77e418e34d447ce4505fa35cd9bde313a3be34b44965d41de4f92d8c4545577a5c7ce08ca69f11e087a10daefecc2c627c07f3c37ca3ae243703639a7ca1793a661103b9a3349f0924472e95198f64b2a58034e7222fd7fa34a32e3cbd01ba9d340aaeb00bd231ebbdfd79be3975e9993e78138bc0db1e6e176d13fb2fb036fc683e973cfd93d897c43a411d58c86bc12b518e1907a86ec340f0b95f8d6019da6fd5577db915f48fa0e32ce0bd890fbe81550cf537e054dc846fae5e58d83681bea8a57503d8b93b4283679e1217aaa8fb643900e7a9934fba4f5740bf10fbe23514165c6588a5cac7d9d06e249076076c5773f994fa283eb14b1cd4b48783371e8b243cd3013ac764fa4bb112ec95e089575ae796e29d6d11126abce2eb704593d2e913604fa0f6d702ae8ea7d1458e99ce79e11b523b17b1375e7c328506c2730613f3e0cc246540251a988258bbf1921cc8ab2dfe6a2c58760787c0dcdd06f245a142a9615526e499e8b98439427ff34c635a2274abb06bd82065acf10e7a1c1ba1ffed7c41c6ad850120c3becea1841cf3701c14b58372d12c1317f5588db075c403308fb2abc216fb809f6f56682de41a25ca40b2d0eb9721d88220acc142b0967d1fc95cd235a2f206066e309c963b62edbc225e3760992318e21f2cd2dbd166bac406014c93f9fdfe14f7fcfe9bf9c9f54fb2d224e71f10e19b11ca7a61194b8ec9e8dfbef1fd675c5401d53517c49c5eb6e964ce187c0be97acd70b4b8e39700f0c9d3641903ee85bd1bb21810bdcaa3d989303d771bfceaa07d14682233f64f2422de44373f4ed6acfd534551cbb4fe56df151d44b30780cefdd24c0d4500e06cd35f72a3d85c4f241096902c34f0d78d664beae9100873d57f5ee969468d4b0cfebbaa3a6d3fe10ace5800d61806ef1cb1d25db65abe67911ed14c7fa291fed656f6e46bc84f4c794d0f3b3ffa9e6b651845b19db42026d63dc1ab1d0cf5e296c9c151396783abe2ceb88bec212a728b3a3fb3be02dde8d4373a1dfa8536a4a98bc93f8f01cf71ab5f215711cb4e39258bcd709123c93047aac03e0a852427c219787af42b2bfe04f85a8feb9371c8d03f5198f41880dd6c34d64a6eadd5f4e739ab6bca448006f4f03f7ed9b1d29ced8afaa23fa7a192ac0e56c7c2132c001488b84b02223a6ac14663e8fa4ce422de7f687d04c79d9ea24478285ebed86c5a6f22e737ca28c29d2c7e98ca6922e2b3a7909f6d7492379bce66b721182295226d2846d6dcea0119d218ccf282092a16867779f048739af6f9bdf8e5f52cbd876887ec1f10df6f787b2b88bbe0291afb345e1c4db1ee6637e806036c08e8c7ea5536086e9c73cc54b34d99aa2b9e9db9ee2b2ae609db566a1c6c93011354c5bce3b3b3e7191566d50fc837aab6ca72b7199f6b7ea91e28b2d0d30bb2eaeb2c7c8033c1128b38d65ef2c9088390214f428c3af6a559a639fb5045a6ae31527e8dd746758e0b5d8cfbc23ca1e73252f132ce22eb5d02c8899c1aca8cec1a19183a7964c65a1a7ec1bf4011f20a1d2156c95e1b68d6178e4f94ef5ffe1fd863cf1542ae29390778e015687249b27b1a00378b1a5191b29024e7eaccecdcc285b2469b4fd6cb5025c776a4f2be31eb5f860e87b19337a2922026ceb5880b87694bf6275e2bf92a0db8f40799302a0b29f23fd3bc3678cc6a77a0f5d8380b815163a06b4a94aa10b57ac16bc3bffbf72441f85221d15ede13506801a12b5ff7b9e4b06993fc378b991c96b1fd4bc460ac234cba1cf24591ec3f00abe132113dbe1c91b07bb03517bdee3d5c2b1e496f52207b9bc0d32a2d10fe8d877c36dcc00cc2ef6f1b77f5b18a9eb20fbd20e1fc4ce3bccf3c9f41a405799ddee54070d3802df4a4ea0eba26efcbbc84759b04f9df868cf373303b4f75355ef983555362df450b9960484685defbda75dbfaf69ed5b8a06322220e456da5bc377a5ef3ae9f34163a032b703d523c75bd5d4c8724dd9a709e00b666d0d1f1136fe609632587c2f4e263f186cda68a7b855d1f50c021c1d7420555710f7b5df19cd7f1c64060e054ce661ca53eb3c196a7d84216c2a271cbe399ad12315c54183a41b31d96079087ef0d664247169d600b4deb4075e3fd319352786b238aa62c505881688173b4d5da45a807eb04f07bc5b6fbddea9ad4fdffe9663f6e353d42d5aa1b77561004f917ba2f2e8fddedd885e700c4083118d2a43906431e89984b66f1ab7c0833d32b8d0584ef20fa204d3eab61505ba667156162f523a17f9f02fa7f929bf5aed3cb74757b42efad6c96274936d9ada9e0c3bf4813de77ed2fae933c50ffa5819d90e1cdd1a705619fec877557850d63a4fb7edb1b98916ded7034dc4c55bbbddc35f63ef6dc8820067fc68cd38b55ad90118b47ff416eff3321b9a7b98fc1b866a615aab33e136a1ff8d00debf43bd080f9b02fa637c4691ae687ce1619a780d6d54d6accd834809fdb16b65047c448b41b1ee9a5ecafd32a485de8bfd8b9af479aa9048f007648769bcdcd7a749db8f326933794fa680f72ccea19f265946b35674e7eb12d716a82109f504714320516eec827ca96ad6e8e18334d53708ee690a0f30f12a89de60398bf567a18aa9262a1a3b0f88f1db8780fe7f440682fcfffaa1ca617a639f348761adcea21b6d550c32fbe185c2b0a518c9e2b37b103c52beab673ad6b27b8287e2e3fbb694cb706acf20826c921127514a84dc3968a5573443e01fe6d06cb7c95576109b7a493f6c5b587e469c18ea2eddf37b41c4e3f050a386982b7eb0ea8cf70a5d00bb06b403b7d3370c0d159a0f39410f61441e2a445193fd9d456556f41297c9a26734e3bdf00dd09870098050fa9083badaa5ed940b7355cd0b46fa982fbf7b269c1514c080c2229114ee1d836469836678b4437916b4aee2b8e45da19983bc84768e7e51c2cb4b90c8c664507531dbceddd3aba38cfc74714df3adf82d50e93044a787de5b0504633fb3a22a58c65396b7f6fd7aeafb8ce42c009fe2de3a31c31dfa9ec06ebe58380a0758cbc1071efd8fdfea9c2accf5552d5e270f58547730dc4da42cf2bcc091f5c907f589e46b928ec9a73253de66404545486149fdc1e36347f60edc81fc28d515921857c82afbed8413371d675c442c5797382011e3d624ff2ae8768f7977b613824382f7f3dec4a6d555a61d67179321ee2cbec6ab1e9352252767f824049374fceb7eac3888626e4af236c90bc3d6511aaced6dd0e1146b07c7136c80bc5393ec8a692c695fa44d782450fc34d0f367b095418d91f88e73a5df1dddb08f04e36aae4a749fcd7e828b22523e4a671788f3a48391b8be443226c33e6ff94f9eb352869ad64df01b1d112886b906de2b5d6ad05015d3310763fd5bb4a495f7adef027cdd309e7ca80a5f6480c56b375d57a345eb831fb6f7a4dcea996f9f9a371d1d6e2def22e6ad03f38cdad8a190c0cbdf6a1348d3ac26ad4b719ace4ca7c04de45e8b0bdd979d549d0c82102310f80288b7fc678b0cea4c57c25efe6e36b2e41c45a09e74cbc082ab9066c546ed12b37d83cbe7a8577583372b15f04ccd276001045ec671dc1fa4c055d05f5fb79ea6f8da8c79bf9e32ef2fdc1659d88970749326f5b2989d3f74c22ff8393e87ea075c86fe4392fa458edd7eb08a6c728e39deea55124ed2fddb6ec5d84e5337d2ed6c0d6310a1faa54efe4e77169ed1bc3e9dd220c491d6742fb2e766db820a52bdeb13c364085ce3b4d8c50134aa80c71f605e4eaa914db0bd2c01ceff9e568880b2bf62397c8ca203776adf40e18b38c3c5e33c7e95bd5e6ed10001150f408b545d9f514210575ce5c7480f7ca61994da524a170af31d058f1e5fb8fcea879fdb31cedae47faeae80e3ed45c45cfe0ed906b842e82d5ac2debd577f5c75c7770a2ce42830f1e3aecc47ee218064019eb17c2b46028cc1e964e0064ba8d9d0ecf67ea166b0240cd01f3823185eadd2eda36d15129f7b7d99baadcd19218911d6d36f39a053adec7e9b88d803dda1330cd8408e98910c53485580355db0674c46ad72ac128dde8db3baad9bdf85fd9b5cafedb3e2286d85a16386b105624ddef749f794f2d64a7a0143d2b506133a24c258635f221162b684cb2b29925b38bc1da69cd77d52dae81a71bbeac481af2edb3a07db1d568f4fd6a9341fec43b63b0378226c96f1f4b224a49cfdb5a41fde4ae1ec06c0ae8116e96fd7434aecd463d3410e0e7afbae737f9c3e2396fac4f861787189fd7f01dd22aff51b04f31fb483ae4b297973b7e0ef25e1e06fac6a6ecc6e7c29732e3eb822a9e1ea6b1176ac291c7fa228992e9000a6dec60dd76457043081fed4a805444c86ca7c7259d08defed7a524657f50f14b8284d618d4b5fccad974b513b21660721797779db9d2efc15dfe0cbe2d2c2135dba782dd03a3b04d99ac7adeec38aa68235b28260effe110e226ead6ac13f9969cffd0b71fad66e0a880377d48b5650ad11772c5658603464009444bc3d45260a0a99dadabc8c4df055e5585c8fe0a2a177e850ca15a2337172924733403b510bafe90de8ccab2aa45ba95ff10820e86ed91a1dd96cd3c9eb8b7e62d243be6e54fe3105deb75fa9d4fed4e3b9999a9df46eadc431740f6432cf91343b7670a86454f453bcd28a62e2a1fc437a4771b800f55ebb0936041dc27398443b557ef44914675e210f5c44e6a1a3dad26d23e7d36d93dd5f00b2dc84a0526f300821f4293d9f8df03dab6fea23b142175a90a9fec7f57062bb01777fa5d68a23199750d90cdb2b3310a8a0dc87d16d3563c030b014b055b385de4b841bb3962def736cde856a338b3f281ca12cfc08bf6b28d5dfe641f4def54ac2b1cadd85c51bb2dfc3b2422a125454787d236af4d1a14caa0bbeea7bd051dcb5d5c5b3e54c25816b93b108faed856b3ae71feda1b6e48fcc9d71ee49eb1cbd1f30ea5c3a6dcad1aefc1fb37b9a2e0679a75af58bbd600c23269e25667c3c6d7a5b01d1b2c57b05f513d80bd10c225e312713d96b25e49d87fe6687c1cb87ad82c7275b1cdb1b13939aa50371cded540851cb2cb17fa4c41725a14f92e74706351b543c02cb5a0c1ef52ee4b3eee506174290b8f2a83c38919e6d68fc00f939c5f23ab92a8245e47f73ad5a033e0cec60fd2953b88d041a28b05d071bcf0370cbbfd73c3f45869ad13bd257d7bc09732476b4ac55cd5d8278bd48bd7185d30b1b6cbe48b95c789c8b2e508aa31408cab5ed52c824667f37125f4d32f67cc0156153a9398ef9a21a15c75e79677593e60e48eb01b5c3bfaec5b2f5672587be0fd9c9b64a383aadbe8ddda46b539c54f6fba5488482fa0f8d3b16d756f30ef4554d91390351aee9d3650b627b25755ba65d1c8837ef8c1bbe618b7f8640ca71ce020a7eca4478c9d692c78a50e0bbaddeaac7274e336a4050a09bae415e460208c9112cffd551d1435e8734dd67c12ef40456738307eed89adfa5d4f2834ceb8cf72143c684525dba4bee9260be6abef2df67fd39ca3a1a163ba8d5503ad105d760061a6088942474b48fce06a06ff470237fcbc1a76d937a1895c67ef58a616d31019c5cdb2f79782132f1c52c44c1024ba310d0ca83d75060810b32e8d65a5b5abb63aef7ce337ed67f9641a3701df1f7f79c0f4d08d56d7eb71497f41d0d41ddd082bcba9431fa10b37d4553fba93465c49f972480d359740f95bb5fc5bae15c386881899be1eff3cd08bf95daa6fa93c58d06f2c53fdde0e9dd1c5ed93d1ea43431bab99321a48682cabf13332ef7e1463708d8bf3911dea46396cd83102ea8350def117b15ed2ae65e0a4b287b87357d498586013e79ba30a959ff47612a2d03d15bdee99b9b9421d44098f2a78bb6bc37235b52078d5acb0dded611b452c24380908f897e191fae18f3e3af03f309d402b3bd49af6706c559461d481364f9ffc640ccb61f9673d98433e64c2d5d26ec14c7f387de1ce23b029400848c9f05a79a59ab88205d531f6227a5a3c5c23cd9df94808b7e385307f01005f93f34f2efa9a48737fd453ca06488a3a198992428f38fa050558d83b51654a9a4791876fde71bc1b2b8751a14cdd1b4cc7c429be1f1c255770ae52b20d6a88ea2eaff6a73ad00547ce88b5fddb40c832c3ed1a52f9c9917358279dd898508b00cde62ff56e3c760cc33755a20a2c884043ad8a1a1d738026d926b60766d28ad16387940b55e2a2b26aa40932fbb78116bc29bc35f9370b776ceb0aaa6b63d80cbbeb4fbc85664638861c8f82b224da5b7f3a455dab692c8df9371e0f5db21611c04edf44e13c5a9fc6c1c0f4023f4937cefb15b30fb16604e1d29a62751b147f6b78dbf4ef2d0b6e9f766a5640501c797da839b2045282798dfeff7aadd441facd1ab9635aece01e18fb1ac40706d4fdd7d20514563ea17bd17a8c3eec53ae9533882170c0794ec04957f86dc052c894df697ffb76cbc380a5b8001afce679577a7d403253f3c599c98c3cd47705c8822accfa75a935113ecf0a4e1ea99039223b07e8b4cec1fb693c32748bda04ce5f590c034ba0918d95a0cf70df539d4c40aa95f0860d9eb898d8c46e1c5b84cc4e443bc7a9c47d633f7d91d9b4f7bda85355688a4ed3fde51a3705795fa718072ed22eddd666b3a177d16e2085150361f101e42999ad40927c78a57585fe1c43259a54937bd1522df0d920a189722d5f02e2d2c9ee7cd009885f0a7581ca8f4de242e4129729962bb2b0475d806d52ec122fcc15dad6a8e7eb887801855a85a18e343759c3edd8a7df1b817a0961c85bfc6d4633c154eb24f2b6cded4e750e95123211bed92afbc5e0a31ae520edcb69946150db131a89516c8fb6463e8156bbce3646f104f85ab2bcdc9b2b05ac69a216a62b1658dd8d871890b4adcb537127c7df520b5c891d43ebbea67cb28fcff75d111781c95a87cfe26d2edcfa5170aeecbc2f69608fceb2fee730508cdd0d743a913d18d126e4c44cacad33847ec699daf95fc320a6ad83b02df1e89b7e22b5f30f5b0852b23f521c16da3795ea69b9654d95563fe86a52765c27fa566be427dd100964ca768fee0cf0cfb3bfc6e2315d7162ad4e93876911c2e428b7463320bbaf4dc06336b8b2ac4029f05b22db1809a842e6e66e37893b5d202b960724ac3da123d277a296ad9d8e502972772f7f76e63709deb4c1cbd007815d96ffef422ffa291ea527143d83196a01ff8e37b2d59fd94c10517438957dcc9aff68b0f39999a25eae4cc3af2cb18c4efb472bae8dac9cb48d1611e147ba31200d6ca0a0e37e9f185e3256ecfa4e2d62e832ae07676e48e67084c1d0e05b4922da2a9a61e1fe97e2c4b4f15af29cbc04b8ecc346d85e65de8aa8bf4ed019b94a367dd3a4ddb5e93368ab08971479c75ee1156909ba1c02f50a310dbd79496075947dbfe87c9f44ac22f9d2dd42fd9261e5116f3f281788418b0cf72cf17581c8fcc56f591227d884e4921aa7e1f8ef3928833f90f56c39336dfa363af0a9cb8590298fe60c3e3c7832ec8e010e2ed091bd2683016d5aa1f477ecd8e15b265418c6d335650fd3c9d2853e07721243882a3356c5088416882e31079b8595f32cc5d434e0ca0a37ebe6550e7255697d13d6823c64f32d7d8aa0206101b1af03ed6fdd810756bb29477692ba1ebcfc0866d06d4fb379a53bf62a0afe267a130f5dfcdbc20ea4a35e17afff6941ab16d01588eac771833b67a07644cf57e2ef1168387176f43ad01d187f8304e51c778031f60625d6027a16e4ca6a244457c5b328d983f69f56c98a06b34501ad79e04f21c0457e9581a9eebe0f3a2bed6631659ab5ee3e2b21e932fee126d09ebfc5213d95b90711b9e688bf0624484ea1dc9b10b3435d68c7859fa61233d20421d46b6b9bca32296d059cc775586f066b17d76fe43199d1a2f45bfa214f2a517b18cf7d0e40b81d394b41be86d353ac81100e679718e06bd7ab0084724c7496e8e450e617dc5eb5a1e4737bb41f8000e1f1e11e81b785f9e43233c866acd4e9a936ecd038d8aff1e40dbebabe992a74636c2c6ce49f3542572427eeb9a96e35b4c06663e699fbe694e395a40afe3c2396506795268492a0e49c4b080f1b426e3836085e4e50d88da96c3a662011a9942513b4062ecbb14b096a1fcf20214c45c223c825ffd3a916a79aca84b2c7b508fd5e57048dd1c95724741de5f47c6613ca51fb72be4d43b3dc85cff73cee3d0c96ac91d18fcf8f0e73eaed0a80edce362a5977be78a6353ec3591658f3a991302b8298d64f46c1ecc061a7878924d155d3aeef63eadd96b243000e0968e508b8b4fbec9bc896deeda09b9f615656f49dbee3006d6544ead13ed3a507d0291bd606e1a2df2820f8d5b95456e47649bff7a7a9472946bff556906011af91842e4e09b0228e1615d8782a8c0cc542a27bda3bb7eab41b7a2beebd71a956487b57136b711c6a3289a5e7749468fb59974ba35516b8085e0497cd5da306085d505dd12b025bb4986ecd3653b1932657b7feb446784d5fb4dce545e1c336b93afe695240d3713efd5b419f74d177ec67a597d713754f6a22a84c53632ef5ae617af75bc05e463ad6cf96ba9172a15d241fc2a997c4bf496e89dd44f4fef3d3595309acf341c53967e51257f829f61bcdc19fee5b8f6847c60d344c648545f5ca2a8bb725fd89d6374a9970ee38d1ceb2cf3e8403ae501d191cce06cec9f94bc84b7e6e401daff188c33a133e2f4fb3f254edb89a7016364dbc627a4807b14320325709175568474a8024ebf8bc26e3966bdc42016af56ddcd262d65139b5c3e31d7e60960fb109242af1a69cae35c24202d608cdac2d207d4dcc7ddceea262c57a0d0dbb2c2780fa175a113318010850fd6e34cd10e29a51d45a3fac4b5cac2853957f0c5bfc34f5e70061fa79414048385e963b6f208be0377bb9a986ede348482366b884a58f6b7f68aecbaf49a56365b175aa07dec00863dfe19382dd938052a39d43b8b68bedeb48ab2eedd06617a8779208c786871619f20a0985766fac30632d4333c383a611dfb6d5f04c0d01d335eebaeeb5aad8ae7ef3a319b2a7ba33ef6f5bfcd19cd0104b7d75765198ae931d91f385f824fffabb5817b88f3beacc810766e314b87cd697b6361f787f1481fb99e1d0c5ca69a86a1d72471ff064aa183c963d20e8fcdf0aa23d23027bdd4d208db4986d5f177191ecc9a27788687c7f5cdf831cf096ffa27d5a3ac43df0ad9303b827550a699b702183b61828c0522c54a22795818c2488ac8a342537cdf80972aa43b79362d91031b299cff9b8d8afa24d84044918f9c663d68a3e2b52f91baf8dfcc5d4e6d8c04bc7d67a342b47e97eb26b71ab9737cc6e27f73eebac9062796562219078d0f6d3c5baad509a17ecf3dac2f4217c6aff8c318a1c5d1b0d698758e1e6169306ff1aaa1f20816adcac0ff174e5d472d9bb70f70b4cf082503b2089ede4e03fd12513eca183649bde9e84a76f0b74533cd757889fa32d7708962b92346ee48a5867ed7a8d4e895f2b5d87b1af1142e04a55d1e8ececc9d1fba17959434c87d28a2069aa43b7c4a831db68bf1eb58f3fdee3501c9001845bf04bce3d3c0a5b418946f90b791cc5044d2c83fdc8e389dbfddf5d716d0f75937f3044604f592a475b6cd76f50be0ec784537405419eb39eb2ba49b3625f77c5fb893219cde5928df57520869ea1ab5238893925892374cecce8d0b10da426a7b64559f623e0f4b98d4aa1918be0f5be31243520ca21c055c5b72a69a4ef8676b6ef931fd5d84d0877dba6f067fa7e6edadc4a0092842444d9d3df210a3cabbf2e2726cb906b13a5e4fb42a76ae23be69849de0c867fd66b6c3d0cc88f990611f285df276398c56035efc13cd2165dc3f17675d1d96369322961208ff8fb39e05226fcece88aae6b1d66220324e19975616a5266308d7a6e8b507fd76cb7cd86352da346c67aeee28cf72832bc41bae06799d108b52decc30167472b8c23ca28099dfe6a89701d43afb5db995a5a89cfb32b9e4d260bd8518e6f29230041ea37501b5f59a7fa22c276db2fcead9d4067abb069ce09fcb9b26b8fcd20b7264b7529ac36f045f85951a73fe95bdecf6250ca8946bc8726c31e81b0b4f8eb59138e4033283754f27e7237937ac6a1f69bb9db857bbf3deb89bcc158e0a75b73573ff19757a920dc84de7644b2ab6f725d8f93fa642761d6a6ba4f81d667a56c63a171db620efda83ff278cc2cca9ff56f69912c77d2226e0a8beb216160ae4f7b0d42925674b588ce5f3858fe6e36d14fc78b04d124b68e76f860c12f5a6db04e1462722f482e89b8497029643c0b5f44327e12e1aace17efd919665e9e62d83e656a0533bd4ee9a3d11f199f95934af98104c60ff47c7702ef4ee9f75ae550a298ea8fbaa123f6916659274eb41be0939ab3143e9a71013f6dee8472c7aab29fb04d7ef55c926a6e69fac71956486a0e39307db05fe14e1acc01a447ee584928f4b7ddceb5348742d785fb8bc34b4cd9b43a6d852ee66994d87303a467a1cf5ee466a43f2bb175828b50dbc9baba5f7c9f41fd83b299cde91047052cece3dd1da644e3381dc700b39d409076953ec0032ea7ea81358d919c88208998162b972c274675ca2622a2793623b84d77d27691fd162a27499af2bd19985b000a8ad8e1bc9f404cdf129535066d165232a7b0fbb3f4d47e953e395cd61d855a914207ddef1c9483ef76d7d267eb37c8d003352b475a4ea768385bb81ef979087da9ff4fd1e2fbce737d9930d1f6bf3cc047c67290a753f4614b9b471e0c9da26310792928f7725d7f8c20cb49ce2be2553a1199462137b30197379da028539a205eb04d2703979333cd752748b375acf7074da3338a3e03df6a16860e9b3dbf0adcab4eb3fb7826c59a1bd976e8d9c05000aafb0bb92d102395779aa0656729c7d402cc2135511ffd42917983472f432d41de24f247dabd273a6bdc9a4c4ec68ffad2180ebf21dbc99ad4c8a41670f95faf21fdee4bff5140f8661637d4e283aec1a9527ab2a5fcbaa47127022703f51541f2e54b69f57a1746464a0125306e454e6a931e6f1a278eafb5f4d3732c90be81aed7d8cb37bd7601b5596346e25d92d3083f8129805961b9ee0f7bac8ff13b826186109f6a4859546dcdcdea03425e2679847877e0eefec3ba97d6b62312063e3cf5470ea7d4ef908a84fcb22e0b6dd27fe022102394d4e7a80311f89c2691d28dedbb7ffef9de5f7c50ad636a4eb95039caeb8850700c9c27cb52e386b9e8a5f1a3e5127f15d4161937cdbee9e1f6184b38e0153a5ed70c244713b9a60cba2ee0fa96490a5e3cbf58d228c50b1c38bf9425ff9fbe6d9759fae222035bd795f98d7ae8d51adfd7750ecf0ce1b05812ef5d76702a103a1dc7c25fbca1302ef444b825cd3f6e25a3e6eab22522c5e5d704d4d9e21b087e77c3a2729c35c53927dc6f1dfcad85c36042218664ebebfda8df296ce92a1a07852f824931f8912c3d831eb75da4d9d99c11b6795e2d42c03118392103b52efa2f73477235effd254c77796b0e00814e9f2d0951778f0c9c9e69292710d0828f768399f824729c0cc88fddd6072b5a0a79853bd42ad0697e88726bebe757f451a2c7e24d573ee41385bdadff92878529e78c268514d6218f5585597f7d7d1b42f1de4d9dff56c9b9ff8439f7d4f2f5b638ec40c027b0fe515832c7e06ad0b86698b05923a27c9af179804863bb541516cec5a4259f736e75e174f754d41d4a12b4942ef74d9a3e2f758307502f5dd0559eb357efefc9ea36ad6c3a04605f40716030149df0d2eb8fefed111da98be04848d49d590acc14a57a0d9be358231421da886f217d0e1a7518e9957e463cb63430fd71db004c673612acf3ff894643004528b648204bfb2ff92f51bbb9dd5465f708df80cee88c2589821ff450a1d5789b0d53b0b63473d82c9badc0602df99103dd4edfd1500939452b97983deb1e8a1e0e18a1fa967e5d0d6ecbb38a22c81afb27713a69552615a6395d025a463a1c60eb7a9bdd9c87e7da1a2cda51450735e8146f5bdfe826411615361c4eb61a08f7b126151d1db7ddfc1913748a53450de3900be042f190f16254afcf3cb27274a466504a84d023b3feb5d71b6fda582b846b6410f2f3274dc747dc07bf5b1875708552bfc819c3f2df48e402cef574e9f52f6c7e124178d43e6b3c3909f0899550f4a62d153255751723b8d324c7e60481ab645fd0c895809eb9133fe79a0a798ac10b4dba5608a4aae5c32bd334ddfd8099215126f303c8edc0f4bb58ef7f930499836113541b54579eab469799edc37cde68c5bc28ba09b277d8c09b624a3b6a51ee51bb6f7247264fdd91d49dbfe95a1de5453487ae6345e2bc813f8a7461407fc6597aa210fad931e33d27253f48b6e7fbc3926aed5ff196746e22fb22b75d44f20955b5199b3b771e3d7dcfed5f4ac467ece67d115aa7a3aa72d0baed16488eccb992360e3315e5bbf9172db0fe7d06c8e253fc5f0100d885396a922d5b4b0c47efcc8c6c5a32bb960a47be0686606b1888ef993f36ab2015a8c7e9d3aa26e561e2b4ab6baf3f2146ced56603f7a171aec69423a16937cf0a3aa8a6c20672876fac87aa1f4e6447ce75ac5566bdcd5d56bab23f036dc4c17c431f3dbc178c767a9fa837b2c02f0b6f7f7a5570d37a94c7c919a20164e4ca8b75598233f22fd08cdda35742889e835cb14e74660048a065a81e1823c1efb34d8d1bb7bf1986cc4d343213b89421a91f028177be12c9820cf749b0f697f23720ab21ee31c41f27eb8262277553a576884512cde3920777cc98ddea6ee972df5a3b7a5f34ce443e7577467f58568f04041ccf917e70e3ac87fe99ec05a23dcddcf57e5c4fe5ebec3fd566a3c4cfee9afd97a9a3ce3c1b13bdf2aae5c2ae5e65aedd149d56b1fecacd1e3f52927c3c7fcbeec49bc5aa4f1752fde4da11546fd56137fca9ac3117fabec6c04ebd42e3e57aa793b55626e140ad1eadaca629b7eb7f897d8d167f90c3bcfccdfef44bf73528909d8f47ee16d26916015a34b09cba101a839170cfc36d91d3cae55a67de3199d7542ce3f59b77d785a7f9ad8b23474ae0841f0aad201b923132cd9c24d6ad2588bb7f7cfff0f94b339d9e0aa9929c75e8f24ec8c1fef9cf39405d2343adf68339a5c09eeae349dcc35a02c2e943735747154290a83fb823af7d0169df2767d3c5f820ac8244841a220dc190d2174b0ea411ff96f3912caa0c270c28057f084440d3e38cab64c8d52b3c35fffefd0f9139031de29f56893cff5c00979a7d6c381d919c4d7b78ef7e3dae8cce54760970626f9ca5ae32c48eebc870cc911b970ebb339ce0415f2516fd002ca8c09090a0e31f46439c6b060aa3902326f5414b6fe4c9dd84f4f346d7c66b2972e7f4e76cb2e85d7b479169d7a89f7399e294d010b4279ef03375fbda21b056b288fdf1a882ee8c4aecb061dc237a3c8f30e08cbd2dcab7a73d076ac08438bc3c41bb440625e90c84c24896f862e288a5f2834c816a11f5c47d0fedde8f5a514436e743ccf360ba41fdab33c77251605c83c8c8d1db2870983acca3e01089bb7d4bf7d7d4fad684bf5da480b0351aa4169d70cf905ab4fb0e3fd30613122e7c8ce356ba0923596800adcc19dd52a1e10d1b9ae17a94e26998b30f047f76e6e5a920d07e52d98aa18536482a9de659d75c6e8459c0223c38f54704ec5d66219dfa3dd000b595bec6e8b834f9056b0fdbd6801d43ce4dba2523524bd09a28448763ba95cc6a62bf5b8934031a5a5e77e9da9d56d4f3147bfad0c874c8a3d6ca1503ab7f044fa183768d9d4a7d255eb8f6bd383d242c0ac788d86e6bc29abea9c0364e737dd8c3ad8303ca601236a6f2c11088f779c3d466cf9adb49e92ae3fbe3b0fb030008144191e0a782d88bf8edd6d876ca7d1ca061d6b4bbc51de228dccd398895d6fba5ebb18e6b50a3c22e2a29c520b2a9ea295ba3d337a524e914df075319113697572773b81bb3b3c2552d5a6da2d0a79dbc26f3c1524da35d595f21735cc73e2c6eb28710f74f107735ed794c1c338b22da79caf778b84dbbf5ed4ca0dbdea8c818dda3ac96918bb4f86452e92d7b74eeff2b8efa48c68b1aac02bde2bf102cca9cf213daff3f6e67b4c77bf2d4bd91c71156a386c206ccd4201e1a1dd5a741aa79831158c3d10cbc858ce1ef0e5cfca30108e9ddf84d639f719bbaab70e607926d3403c4cdcf03d4326360b6ff9b4956e7a5f0d76262eb6b6a61a37ed1e5a7d0dc9122f74e74d1dcdf111253fd3d75ce81538374584aed8e94d3d7ead3dd90b0f97d1186a7ae56880f31fa81333bc1d541ca90866c2186e0d55d551ef55d65585efb7936aae80dad899fc6af897887b830a7483a7ce7cdb76211df54b1542ba3593b23523afbf812234dfa4b13a963488eeec0dfdcf2b4f55dc8d4ca2870fb1b40ea9c57bca1e1ae7138fce0352201d9e5db352c8357eb62eb9fd17003e8fc7893378ccfa8c94749b049d18180a9241583062b3f7da687b5eed986862b06cd209c40fdea99c9bc34c4491b6d8294639d8cbd86c524ef6115aa1f34b3411261c466b88e297046ce21dd19d18fdf91eed4b869807d29680a5d4fd1d26234310a3fc54c3bd5cfbea50ac63d7d08740473e1238d4d0a25f0e52bdda01b87f8faa1d70fad255fc6e7b12b82f459aea8daffff6b065bb7ef9ea70601ca2ee184d64aa2158c908811207b14cfca67e6492532b63b7fedf7d3f5afcf7a24454b272546dc4a102ebbd0e6a55347c477435099d6bc8b9fccbcfcb1923ed26e6b200d02fbfa0c4e8c0e0c11351c73f521a67079886338b6bef486946d3edd46f2edf9763c429c8f210a8270b6bdec93084469c224610ae84ebc9489ed090a21d66768c1032fd9e0cedba46a443adf629318046d63a4ed354cc0612f7d95296bb576ef667be9b7dc5a82d80aa0a9a43b55e767e91fcd318f78aae0bdd3a97cb5fe61de05535fa1f916c9332040d7dfb4838b0492d33f1adbb7e2fa8160255e16ecc0213ded9109ad35040c0d05c40de94c7bea1f67402fc9ddb9ce67688e7f971cfe51f697504ac718eae1958d97d453983f323b7c1c354182acd5a3af5d2502f2ad63f7de0653fdad4834a802e0fa24b538e5206f413da6260d58f6e74f4419c4d78f63b77a70f1b5c9b9d07ba12b4416198d3a522cb05b391e1ba51882f3c07922163e506b4e513e50e7f01fe25c47288bf133e097fba14a724ddd04fb7cd59c123cbd1905786f60c9417fee2f8c47b94219c9cfbdbde92384380e15b49ebd1719d6b0fc7a05b08be72641da283090590903d5f41fadeddc4a7b61281bd165f6d99146bf91914147635d53f0186902bb2d6878b378df47b84535686885922bb849cf69b92f6008b9dfc886ddc6e8cdab17690b49bd18de7304b556f1e3b8e4611b3fae754bdef81632834eab03296bfcfa977ed203e386fe22f733d9bec865d55e391dc0c7f2347d65748cba9f77e0626be96e162bfa54da7046291ffa2e4c355bd9404048a37ca35fcc7d5f743e1fee2985a213fa77903654cd312a8405a1a4e15dbf7223dc46d87099b6d03584b1cc56123a56c0bfaedc598d57d1e16d7c58c7b8834084e2f35752da014dfc7b14de162400d15ad4a1caae02e0427661208bb2230ebdf6ef075f99255b556274824b1d27c615db885fc951f249d2097c73fbaf30c9de7aca09135444ea675585ac75dac9c4c2afe8a78793724e966e3094f103de4ce11fb753e95b4f07765afeba1a88f7033038f65ba8e5a1a29a5d831b4816cafc050267d64760d9f4df19b8b2cdb32e47639264288b28d8e5eed04757264fee21f65f6539b9dfb275694e931565e9b6a0e7823b1111d95c4148c269eb571f553726e50532c8cd1d6af552123809bce47444614dd220baa2a0e06a24e2953a41228bbe9ac120e4a8481195efc72d00619c8b61e2b9f918de1335a22484d23cc901a4ea41ba2fd8f8b30421a929908b1da27ea92bc2870686b0971e798e529dbd1729d9335b120246433246effd2920d8d061213d5df9481b52250f718f1c45ec18800f52126afb6e58dc966040091cdee5ac98c8915d793052ddb9e0119a7aa72ddbe81232e8bd3f60c7c91e0f4d0bada253b5eedb0dac85f7229b68db52ea84b224b286660b35d004cb77fc346ac1c25e01748755c35c10dcc12d51bb1e007fcf8015d99309ac0cbdff6c766fad786c7862ef34e702037d664f58aed8e3a3a653ad1a4882a01385eb228bb27786ca7cf3159d411e2764bfe61b0210b1497ab8b991ccaddc315d3e9ef1f29e09c41cc7e5484c47aa14b5064a01489889933627efe73cfbc42dbd5973c961e25a3191db1c5c806cf1c9e8c4a714ec89bac0c0fc806c3d6e0e28bd016043262caf38ea0353e5329e9d7144f126d1cdf94b9ba8149ef27e3c303ba9ac5c900e1ead68248f22dbcf788dc17133ae7e7a4d37d7d30b659339f92abc7cb8a873f7238a5445b5a228f211cfa35d88e1dd926233621a06657715dc51a2a0655984f04aaa475976868a3b188e0354a167c5f12a2cdd1e9ef36e1b1fdc194d03dedc4a13903f8432a46e2cf5815a9dac8f30a76b7460b8b4ebc0664e44f5b13065d64da9c72049f18c899fa980979a16c581f42ec3ca0b6976da2cdb6ab4a55ec5e26b5436480bb88e5f466e096b53182f956c5020f22a3bbc44c69348f22ba3065681ecc69f9b2e36eedb65f9c369d4544c5bc2f94bbf218346f027696ccbdc78fbac9f02e9c74fd806ad006732594c15aa178994a2e0181f726147a2c2b15d24696b0edc965cec62b85fa2a449fe01da82048687efdbc7eec70167373795f39643745cb1c04612f5945d71c77c503f3d8244fc1db751fcacc9d05369edaafbc7fab326c19f69edefa563f0be16c066716d768b1eaaa3236630353435f0bcafe672ba03056544c6c2c484a14945d4337aadffd7befe1caa6756dfb094a0202753e33cae4abeadb912ce323d39df6b77648f0f65c238e7564df4d31faea1a1b45e8065d1723223af21ac46463eb98aff920054e0a3e81683829dc292f403b11a9e141c27903516155024680814c07f80adc461dd90dd5ddc555b6db0e636c8bf74a0ea20ff082c0d0c4ac6c1f1b796e1246e1ad1d2b2b391892f7728ce3cc39c8c69d815faa346c1936e11dbd03658fbba6fcaa4684c3a8d908821ed1324efbbe8086eab3281543c93685706091150b8f42cbeb0a2b4887b542113297f63050118b4d20dc3d6a137b7f3bd7fda4b6721ea528a6004ea2c26486474066fab18d6759a2275a25ab792a23aee066ed1ede8b5de69d85399c7bb6cb8e858eac337e94b799575cf0158c8c26d27b786c9a2b8a3fb290c08cf49a8eeea4877ed12e3c7fca0d90bba1e4a66a558dff0629b4b0c5f6da4bdf1ef0603532bcc3e1735d9f34633691a3e2d7b76ddd0b309668b0be9c4d7ff39ba2307d73b07c5dec42dad4d62b6b4886d41ab6607c9c670a12a010bc6b6334bd10347318cfd9881702ed5675cef9f902b45b68af94ab2c5b2470dc0eb495ecef6aed95a034713ef49a9dd3f0cf9d17c6f1b6c15b0b681ca029bcb27f1ed10ac630c3ad2f20fdb7c6b41799831e8838005519b4787c626b46306110233353348f43680e98c58459e97d952fc4cec14ba4f4717ee729d6479f3ea8e84bcb5572669e70ca44d32564e2e608146aed432429490b2dc3d628ee0ae1a4bea9360c855086eda9fd2eb7b192d52165460c9e260b6652ec05d02ad10d0b6e8940d345aa2bd93ea0e5d69d6bfb577b16c9cbbbcd9177fd23c1a1cbc782bd1a23e3b5b97afacaec83179bd14c6a4b59d09864dbd132874776370a3d143b4ceb9d6afdd3d3e6e1677a21eb3a485f3d08af0b4c986febcc4290c0fe6c6d779e4eef76b584f1f0d21baf08465e49d0ff1eba3ce03270618fdbfb2c6301606a833de336a92c97a80373f4d8ca6bd7258ad793eda0120d8eac57cca0f6df107c74c13d1ddf51f55ef1705579e1238dfcfa99ded255cc3d929584418828b6dbaa8c9878448fa1436ead1e541ff511f9dedc72b735b68c4df2a06232cd18fec42bc6904b48d1aaaef80792384c1c1c17fc930d42e5cabf0ddb2ce8559bae9cea2252b2b1f15a786cc724f4f4c4d58aaa01db61dbbb991967bd4432e9eb3e3be95a89f0eda717ddf5c6c0f19dcb6db9df7c16c47ac5c47c6e15916956cb85bf77671db4d8dc7f527c4ba9e724968c88e3de242b9a294e574b3be7aecd5d90ac54279f3e3df663a9f143414f3263db03fd3064445a15bca81fea01c684598e54b6839a12381ca310875f70c6f722dddc8506f4922ff55d5d935cca12c3768a7feae91030329f45cc592fdcc0f76309c0b8c3f810fca107e7c09d8871a5bb0ef9a71193cd20b9d96c84a487885904632219b6fbaf85a44d614899c6ff2a6f6e2d6837ec67ab30a43b1ef4371b919b96c643d013056656e4ca6158e479668a6aa879a0538019b3422e8de21935c310f638241b774041db669fc64ab644e99c08fd090b65a4c76c32aa4c3471569fcfcff6c13e3583b420628fcea2e3375943ac536a4e56b63b98be5fa1ece905e08bc2e6335cc8d84fe76a9c59b968612184c6fa7020dc894e9f430ad35141bf6273ddf898e7592f0110fb7af0f14acceb0ada5b7a6298464f16d62e5c4be1acbc0a2ed8f79e77a538cbd922c2a34f8868cabba7ace69df2f97ed5d109c2df1a507fb969fbe981d3a7233ce14bbd5cd3d44d6c1190bd02390d4cecee96cec7b49d3dfba4b2d9abaae984fdce5d20fd6a10c350cbfa2ab7fff71778c406df6b95fa0f693907b77a2473913bba3b43a771c66187312c30b94e3c1988f0e9df18b42e4a0dae97aeb5a14a821338ed90c281624cf3cac2b6d91cbb4e1d3de6d9643f0671d1d25503b702df195822eca549316f649e6a6a9893e671c5365135862e1587cee985a07ba9d746e6c0b7d3febd5ccdb08b0ccfdaeaef4d1b62ae0a591977331ba84f964581411c16d86ef8948b036dc84bb3673ecf7653ce82d1e8f14b4296017b21d786732c113d1cd31c946eafe4795f16d9f377640b4ee9977e110ce92062123fb279a9d39e22dbba1219fb7c507b7fc9ac10fd0777a2bbe5b6a0b09bcb96bb6c99d7048fc7d81785fc5fd12c156a7ca5be08b4d9c470fa0ab7b5f4370197126a84acbddb021a29f6d9e14e5ce113553b9f4b275412afd9918c921bbb56c5883f67a6440a4242f47686c94a8ecd1610554c15047662ef61a30e517646697e0b98fbc81d695922001b0f3a95338568255882d16529c06720233c459cf0997eb23cfc2732b5f42fc878a2aaa6770813bd0b4ae80fb8cb5f97db4d6a94562f07f6519e2c351de9de5d6e1bc619c6a24749f370ecaaf66449ffca5451a898e826dc5141d60480df3e02b2802f9cf7cc711521dba195ae2259955519837fe1ff597586b58b46b1435bc3f338b9de0febebd588202899cbf1b5ee7f91720ebaa2c6e2ddbb6c4ba4e6dcf57153409cd0edf33e49d2b1fc52f3dea44b2c6239014ef954ccc87d7299fd02835cc4711cabc98d4100fb5dac2840ae84999c6dd612f37f60775b9420e8c6c824a955c0e1681f9f42446fb1f22f0e20dcc9e8c9835ffcc11203d1b229843619415137504608b5b943f8a98faaff555b52e210e79f0550d0af15e093fa04afd0187769d6c00ce83738016ca16b6019c5eb811b18223d937f15f5c6bfde89745d300285f222b7584909ec7fc513807939653ae9c6a3c90611d5e49572b83df68392c0af5c3b7b63df13c0b222e16db523b02b0e1f71ce5c38aacf8ba12afe7461f7684c18dfb19dec0a1a92f56df0c3f64bb8f98c2a624fb6715afb343a9ec14a96f00b51dde21a9c0c9e8d167a57b7536236ac9e73aa182cba72a60df32f1ce4f8b445284531ac27cf8f684aa4237d5d4bc41e3a4a40232914337a4585a1f3ac8c092c6f6ed2a0cb20787dc771552922579e4dbad2645033afd3ecf95697ac7d81536ccd6e2a6a256e1634b0fb408e16841632134473355eee17d64fa98404a3d6a8b5ac853335ea0017b941824f6d13b3f4668c340943904488efad4688c01f0af16c572b5910f720e6bb33838fbe5e62d798261e34f19a32ba8e6cce50fb5b16266414203c2465cef96f35a5ef53c5b7f6ca321007fc8445504d60d7af17e9c915f732eefa01fd5566f3bb9eb6e754166427b3f00de5370283e2e19a4435674dea47a9f9ad64318a5413945f16640158ef244789e2d9b7581fcf5a6a6199e9b59e4092a3606f86ccd1779786ae617fda3ecdd389a8ab20703c38f28aaa98670dafd2f3e5bfee22f9017e62acf8cff0d3b832b690c6910389169e9c25bceb6a843bf5c9dc803252b14d3da2f4f52733d38438b525f976b51d21e70c20739168aa6c64bc875ab0da316be60c4d331d062a267ed8a6fef05b690e27f43bcd730a3607b8bc169ae1a9d475480fbbeb8bbbbf5b9389dcf6487568f5b41cd68646ee9fb314ee324da1003a5322b035c1c4d48352a4394f5ce7c2e90adf9cfccbe1b593ac1b6383a56cd308512791bbbd3af26e9569e68591893eb1585866f32a1739d2a1070410a3a54f368a753ef27951a7881eb9d3bb80140e9f375260e036cc53fa9b482edd8832e7c526c0709fd1ccdc04ef7bf7fb8e2c8627be4ccda3c2274a3a96ac90bae563d053e0e8f4cf00994cd8f1f91e0865c04f134dd7268e3d04417430d4f4c2c5282ed4e3e5675cc5cfd43df658f232c84e230e0345b59e961d369826d67a4d1d5b9e8b907f5149343b3</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">您好，这里需要密码。</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
      
      
      <categories>
          
          <category> 保研 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研夏令营 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督学习</title>
      <link href="/2022/07/01/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
      <url>/2022/07/01/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅谈自动摘要生成任务</title>
      <link href="/2022/07/01/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/"/>
      <url>/2022/07/01/%E6%96%87%E6%9C%AC%E6%91%98%E8%A6%81/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/Automatic-Summarization.png" alt="自动文本摘要生成"></p><h2 id="1-抽取式摘要"><a href="#1-抽取式摘要" class="headerlink" title="1 抽取式摘要"></a>1 抽取式摘要</h2><blockquote><p>抽取式摘要任务本质上已经变成了一个序列标注任务，即对每个句子打标签，判定这个句子到底要不要被放在摘要里面。当然，这里的标注不一定是标注整个句子，也可以是一些更细粒度的特征，后面会介绍几个相应的算法。对于通用模型架构而言，首先是 encoder，经过句子级别的 encoder 和文档级别的 encoder，获得原文句子和文章级别的 embedding 表征。之后是 decoder，利用输出的摘要语句和原文的语义编码，来映射到对应的序列标注，获取最终抽取的结果。</p></blockquote><p><img src="/pic/summarization/%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8.png" alt="序列标注任务：encoder-decoder 架构"></p><h3 id="1-1-基于句子的抽取"><a href="#1-1-基于句子的抽取" class="headerlink" title="1.1 基于句子的抽取"></a>1.1 基于句子的抽取</h3><p>比较常用的是 RNN 和 Transformer 的架构。RNN 的这篇文章在构建文档级别的语义表征的时候，不仅利用了句子级别的含义表征，还掺杂了一些其他的元素，比如句子的显著性、新颖性、绝对和相对位置等因素，通过词嵌入的形式加入在最终的表征里面。之后 Transformer 出现，Transformer 就取代了 RNN 成为了新的主流架构。Bertsum 这篇文章，是利用 Bert 模型在摘要任务上面做的微调。它是将每个句子开头的 CLS 字符的词嵌入作为整个句子的表征，经过 BERT 输出之后，新建了一个摘要层，用来微调抽取式摘要任务，它里面给了 MLP、RNN、Transformer 三种类型的架构。这也是Transformer 架构用在抽取式摘要上的一个主流方法，直到现在很多做抽取式摘要的还把它作为 baseline 来对比实验。</p><p><img src="/pic/summarization/RNN+transformer.png" alt="（左）SummaRuNNer，即 RNN 架构；（右）BertSum，即 Transformer 架构"></p><p>这是两个主流架构，并没有引入过多的摘要任务的归纳偏置，之后的很多研究也继续在抽取句子这个级别上做了很多延展性的工作，主要还是基于<font color=Red>如何表征句子的语义</font>，让句子表征蕴含更多有价值的信息。我们可以看下这篇文章，这是 20 年 ACL 上的一篇抽取式摘要的文章，它仍然是对原文中句子层面的内容进行的抽取，但是在进行句子表征时，加入了一个叫做关键词的内容。因为一个句子中肯定存在很多不重要的信息，关键的东西就那么几个，比方说人物、地点之类的，那它的思路就是要把每个句子中的关键词信息充分地融在句子表征当中，首先构建了一个叫做 nerual topic 的 model 来提取句子中的关键词，之后对关键词和句子构建图模型，最后利用图神经网络对图中的句子结点进行分类，获得句子的序列标注。近两年来这种图模型的方法不断地涌现出来，包括复旦大学之前做过几篇也是用图来构建摘要文本的，这里不赘述。图模型对于文本及其之间的关联，能够非常好的表现出来。</p><p><img src="/pic/summarization/graphsum.png" alt="GraphSum 模型架构"></p><h3 id="1-2-基于子句的抽取"><a href="#1-2-基于子句的抽取" class="headerlink" title="1.2 基于子句的抽取"></a>1.2 基于子句的抽取</h3><p>句子层面的抽取做到这个地步很难继续深入了，所以相关研究人员开始不满足于句子的抽取，把思路转向了更加细粒度的方法，比如说将一个句子拆分若干子句，然后标注子句是否被提取。这里介绍两个模型，一个是 DiscoBert，这个模型的思想跟上面 GraphSum 比较像，也是构图，只不过那个是构建的句子中的关键词和句子之间的图，并且只对句子结点进行标注，但这个模型是把句子拆成子句，然后构建子句之间的图模型，并且对子句进行了序列标注，在粒度上更细了一步。</p><p>它的子句采用了 RST 树来进行提取，是一种句法分析树，利用句法分析来获取子句之间的关系。之后还是先用 Transformer 对每个子句进行编码，然后把图模型放进图神经网络里面进行结果预测，得到最终每个子句的标注。</p><p><img src="/pic/summarization/DiscoBert.png" alt="DiscoBert 模型架构"></p><p>同样的，还有一个 SSE 模型，也是做了子句粒度上的抽取。这个似乎更加简单，它甚至没有构图，纯粹是用句法树抽了一下子句（基于 Penn Treebank），然后放进 Transformer 里面做了一个二分类任务。</p><p><img src="/pic/summarization/SSE.png" alt="SSE 模型架构：（左）子句抽取；（右）二分类编码器"></p><p>所以我们发现，如果这个提取的粒度是介于词语和整句之间的，大多数现有的工作都是通过一些基础性的句法分析模型来获取句子结构，之后构建子句之间的联系来回归到原有的 RNN 或者 Transfomer 框架之中。</p><h3 id="1-3-混合粒度抽取"><a href="#1-3-混合粒度抽取" class="headerlink" title="1.3 混合粒度抽取"></a>1.3 混合粒度抽取</h3><p>当然，如果粒度更细一点的话，可能就不需要做这些预处理的工作（句法分析等），比如说这一篇名叫 swap-net 的工作，它是考虑了句子粒度和词语粒度这两个混合粒度。也就是说，在抽取的摘要中，可能会同时包含原文中的整句和词语，因为原文中可能会有一些很重要的关键词，如果放在整句当中，它的重要性可能不会那么显著，甚至会被忽视，为了避免这种现象的出现，它在 Decoder 解码时设计了一个交换机制。每一步的解码进行一个判定，判定抽取原文中的句子还是词语。如果抽句子，给原文中的所有句子输出一个概率分布，然后选取概率最高的句子，如果抽词语，同理给词语输出概率分布。这样一个交换机制，能够使得抽取的摘要在保证语法和逻辑完整的同时，嵌入了更多重要的关键词。</p><p><img src="/pic/summarization/swapnet.png" alt="Swap-net 模型架构"></p><h3 id="1-4-总结"><a href="#1-4-总结" class="headerlink" title="1.4 总结"></a>1.4 总结</h3><p>总结了这些关于抽取式摘要的方法，不管是抽整句也好，还是抽子句、抽词语，总之现在的方法正在不断地往更细粒度的方向去发展，更加注重模型对于句间逻辑关系，表述灵活性等等方面的表征。但是我在调研的时候并没有发现纯抽词语的方法，就算是抽词语，像上面提到的swap-net，也是边抽词语边抽句子的。</p><p>究其原因，个人认为，首先抽取式摘要相比较生成式摘要的一个最大的优势，便是我们从原文中抽取出的句子，至少是合乎语法并且逻辑通顺的，唯一需要处理的是句子和句子之间的关联是否足够强，以及信息冗余的问题，这是生成式摘要所具有挑战性的任务。生成式摘要由于是从词表里面抽词语，所以拼成的句子合不合语法与逻辑还要另说。那对于抽取式摘要来说，如果单抽词语，那它的最大优势，也就是合乎语法以及句内逻辑通顺，就利用不上了。另一个是，如果变成了单抽词语，抽取式摘要这个问题就退化成了生成式摘要问题，甚至应该叫进化，但是进化的不完全。因为生成式摘要是从一个很庞大的外部词表里面去抽词语，但是抽取式摘要是从原文包含的词语构成的词表里面去抽，从这点上来看，抽取式摘要的灵活性就被大大局限住了。虽然可能对原文中的关键词抽取的会更加准确，但是从整个摘要级别来看，可能整个语句的组织和含义会非常受限，语法也不能够得到保证。所以单做词语级别的抽取式摘要，不如直接做生成式摘要。</p><p>因此，它为生成式摘要方法也算是提供了一些启发，因为原文中的词语构成的词表，对于摘要来说一定是一个核心词表，单从外部词表来抽词语的话，可能会提取不清原文中的关键词。所以就出现了生成式摘要中的一个非常经典的架构，叫做指针生成网络。这个网络它不仅抽取外部词表中的词语，还有一定的概率回过头抽取原文中的关键词，可以说是利用到了抽取式摘要的一些特性，最大限度地保留住了原文中的一些琐碎的信息。</p><h2 id="2-生成式摘要"><a href="#2-生成式摘要" class="headerlink" title="2 生成式摘要"></a>2 生成式摘要</h2><h3 id="2-1-seq2seq-方法-——-初代指针网络"><a href="#2-1-seq2seq-方法-——-初代指针网络" class="headerlink" title="2.1 seq2seq 方法 —— 初代指针网络"></a>2.1 seq2seq 方法 —— 初代指针网络</h3><p>最早的指针-生成网络是在 16 年被提出的，当时的生成式模型主要是基于 RNN 模型。纯 RNN 模型的思路就是，每次解码的时候，都从外部词表中选取最大概率的那一个，作为当前步生成的词语。但是这就带来一个问题，原文中的关键词可能会被忽视掉，一些细节很难被保留。于是出现了指针生成网络。<font color=Red>指针指的是指向原文中的关键词，生成就是从外部词表中选取词语。</font>这篇文章里面除了指针网络还给了这样几个技巧。一个是缩减了词表，只保留了一些高频词和原文中的词语，还有是在词向量中嵌入了一些语言学特征。这些方法对于摘要任务的速度和精度上都有所提升。因为我们知道，相比于抽取式摘要，生成式摘要最大的缺点，一个是训练速度慢，一个是可能语法逻辑不通顺。</p><blockquote><p>Trick 1：LVT 方法 —— 考虑到摘要的多数词来自原文，采用 LVT 方法，用于缩减 decoder 词汇表，只保留一定数量的高频词和原文所包含的词。这样做大大降低了decoder的soft-max计算耗时，并且加速模型收敛(模型只需关注核心词)。<br>Trick 2：词向量融合语言特征 —— 词嵌入中融入了一些语言学特征，包括NER，TF，IDF，以及词性POS。转为离散值，用one-hot向量表示，与词向量一起拼接为一个较长的向量。<br>Trick 3：指针-生成转换器 —— Decoder中，G表示generator(基于Seq2seq生成一个词)，P表示pointer(直接copy原文中的一个词)。当switch开关为1时，采用generator；当switch开关为0时，采用pointer。pointer计算Attention分布，基于Attention分布生成一个pointer位置指针，直接copy原文中与位置指针对应的词即可。</p></blockquote><p><img src="/pic/summarization/%E5%A2%9E%E5%BC%BA%E7%89%88RNN.png" alt="基于 RNN 的 seq2seq 模型"></p><p>指针生成网络的核心思想在于，既能够抽取外部词表中的词语，又能够抽取原文中的词语，这样能够最大限度地去锁住原文中的关键信息。最早的指针生成网络机制是，在每一步解码的时候，有一个 switch机制，先去判定是生成原文中的词语，还是从词表里面选择词语，这就有点像刚刚提到的 swap-net 模型，因为它也是解码的时候用 switch 机制来判定输出整句还是输出词语。之后利用attention 权值来选择最大概率的词语。</p><p><img src="/pic/summarization/%E5%A2%9E%E5%BC%BA%E7%89%88RNN2.png" alt="初代指针网络模型架构"></p><h3 id="2-2-增强版指针-生成网络"><a href="#2-2-增强版指针-生成网络" class="headerlink" title="2.2 增强版指针-生成网络"></a>2.2 增强版指针-生成网络</h3><p>但是初代的指针网络，在选择指针和生成器的时候是分离开来的。也就是说，模型会先判断用外部词表还是原文词表，之后就只盯着某一个具体的词表去抽。在这个基础上，改良版本的指针网络被提出，这个方法的改进之处在于，模型同时去考虑外部的词表和原文的词表，把这两个词表生成词语的概率做一个加权的叠加，来选择最终的生成词语。</p><p>改进后的指针-生成网络，可以看到，每一步解码的时候，原文中的每个词语都有一个 attention 权重，外部词表也会有一个生成的概率分布。之后，通过设置一个概率 p，来衡量到底是多考虑一些原文中的词语，还是外部的词语，将两个概率分布相加之后，得到最终的概率分布。如果是未登录词，概率就设置为 0。相比较最初的指针生成网络而言，这样的生成方式可能会加入一些综合考量的因素在里面，能够在保留原文关键信息的同时，让整个生成的语句更加的连贯，这比单独考虑某一个词表会更好一些。</p><p><img src="/pic/summarization/pointernet.png" alt="改良版指针-生成网络架构"></p><p>之后还提到了一个叫汇聚机制（coverage）的小 trick，它的目的主要是消除一些生成词语的重复现象，比如说再前面某个关键词被提取出来了，尽管很重要，但是他不能被一直重复提取，一是冗余，二是可能会使得其他一样也比较重要的信息被忽略了，目光只盯着这一个关键点了。所以说这个机制就是累加了之前所有的 attention 的得分，并且设置一个惩罚机制，惩罚你提取重复的单词。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 文本摘要生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 —— BERT 预训练语言模型</title>
      <link href="/2022/06/19/bert/"/>
      <url>/2022/06/19/bert/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/bert.png" alt="Google 公司推出的 Bert 预训练语言模型"></p><blockquote><p>本文所探讨的论文标题为 《BERT：Pre-training of Deep Bidirectional Transformers for Language Understanding》。BERT 可以称作是预训练语言模型的开山之作了，和 OpenAI 的 GPT 模型是同时期的产物，但性能优于 GPT。BERT 模型基于 Transformer 架构实现，是一种全新的双向编码器语言模型。与ELMo、GPT等单向语言模型不同，BERT 旨在构建一个双向的语言模型，来更好地捕获语句间的上下文语义，使其在更多的下游任务上具有更强的泛化能力。因此，预训练完成的 BERT 模型被迁移到下游任务时，只需要在添加一个额外的输出层便可以进行微调，例如问答和语言推理任务，并不需要针对具体的任务进行模型架构的修改。BERT 模型在概念上简单却具有强大的性能，它在11项经典的自然语言处理任务上取得了最优的效果，包括将 GLUE 数据集的得到提升至80.5%（相比于之前的最优模型提升了7.7%），将SQuAD v1.1问答测试数据集的 F1 值至 93.2（提升了1.5个点），以及将SQuAD v2.0 数据集的 F1 值到 83.1（提升了5.1个点）。</p></blockquote><h2 id="1-研究概述"><a href="#1-研究概述" class="headerlink" title="1 研究概述"></a>1 研究概述</h2><h3 id="1-1-研究背景"><a href="#1-1-研究背景" class="headerlink" title="1.1 研究背景"></a>1.1 研究背景</h3><p>大规模标注语料库的匮乏，成为了制约NLP（Natural Language Processing）领域发展的一大重要因素。为了使NLP模型能够充分地利用海量廉价的无标注数据信息，预训练语言模型（Pre-trained Models, PTMs）应运而生。通过模型预训练，我们可以从海量数据集中初步获取潜在的特征规律，再将这些共性特征移植到特定的任务模型中去，将学习到的知识进行迁移。具体来说，我们需要将模型在一个通用任务上进行参数训练，得到一套初始化参数，再将该初始化模型放置到具体任务中，通过进一步的训练来完成更加特殊的任务。预训练模型的推广，使得许多NLP任务的性能获得了显著提升，它为模型提供了更好的初始化参数，大大提高了其泛化能力。至此，NLP领域进入了一个新的研究阶段。</p><h3 id="1-2-问题分析与解决"><a href="#1-2-问题分析与解决" class="headerlink" title="1.2 问题分析与解决"></a>1.2 问题分析与解决</h3><p>当前的预训练模型主要分为基于特征和微调两大类，但它们大都基于单向的语言模型来进行语言学习表征，这使得许多句子级别的下游任务无法达到最优的训练效果。因此，本文提出了名为BERT的双向预训练表征模型，很大程度上缓解了单向模型带来的约束。同时，引入了“完形填空”和“上下句匹配”分别作为单词级别和句子级别的两大通用任务，对BERT模型进行训练。实验表明， BERT模型的应用使得当前的11个NLP任务均取得了SOTA的效果。</p><h3 id="1-3-相关工作"><a href="#1-3-相关工作" class="headerlink" title="1.3 相关工作"></a>1.3 相关工作</h3><h4 id="1-3-1-基于特征的无监督方法"><a href="#1-3-1-基于特征的无监督方法" class="headerlink" title="1.3.1 基于特征的无监督方法"></a>1.3.1 基于特征的无监督方法</h4><p>基于特征的方法主要是指单词嵌入表征学习。首先将文本级别的输入输出为特征向量的形式，再将预训练好的嵌入向量作为下游任务的输入。</p><p>词嵌入向量[1-5]是单词表征学习的最细粒度。通过统计学习或深度学习方法，文本中的单词被映射至向量空间中的密集向量。随着人们对于文本连贯性的关注，句子[6-7]和段落[8]级别的嵌入表征被提出，更多的数据特征被获取，进一步提升了预训练效果。相比于从头开始的词嵌入训练，预训练的引入对于各类任务的性能具有显著的提升效果。 </p><p>上述模型均从单词拼写的层面进行了表征学习，并没有考虑单词在句中的使用形式。Matthew Peters等人在此基础上提出了名为ELMo[9]的语境字词嵌入表征法，该模型会根据句子的上下文，对同一个单词返回特定语境下不同的嵌入表征。在一些NLP基准任务上[10]，例如情感分析[11]、问答系统[12]、命名实体识别[13]，ELMo均取得了最优性能。这也是NLP领域中第一个开始关注上下文的预训练模型，为本文BERT模型的提出奠定了坚实的基础。</p><h4 id="1-3-2-基于微调的无监督方法"><a href="#1-3-2-基于微调的无监督方法" class="headerlink" title="1.3.2 基于微调的无监督方法"></a>1.3.2 基于微调的无监督方法</h4><p>基于微调的方法主要是指，我们在某些通用任务上预训练完成的模型架构，可以被直接复制到下游任务中，下游任务根据自身需求修改目标输出，并利用该模型进行进一步的训练。也就是说，下游任务使用了和预训练相同的模型，但是获得了一个较优的初始化参数，我们需要对这些参数进行微调，从而在特殊任务上获得最优性能。基于该方法，Alec Radford等人提出了OpenAI GPT[14]模型，它在许多句子级别的任务上获得了SOTA效果。</p><h4 id="1-3-3-基于有监督数据的迁移学习"><a href="#1-3-3-基于有监督数据的迁移学习" class="headerlink" title="1.3.3 基于有监督数据的迁移学习"></a>1.3.3 基于有监督数据的迁移学习</h4><p>我们也可以基于存在大量有监督数据集的任务来获取预训练模型，例如自然语言推理和机器翻译。预训练的思想也被广泛应用到CV领域， Jason Yosinski在ImageNet数据集[15]上获取的预训练模型[16]，在许多下游任务中都取得了较优的性能。</p><h2 id="2-解决方法"><a href="#2-解决方法" class="headerlink" title="2 解决方法"></a>2 解决方法</h2><h3 id="2-1-问题描述"><a href="#2-1-问题描述" class="headerlink" title="2.1 问题描述"></a>2.1 问题描述</h3><p>在BERT出现之前，已有的预训练语言模型大多为单向模型架构。例如OpenAI 推出的GPT模型[14]，便是引入了Transformer Decoder层[2]中的掩码注意力机制，使得模型能够充分学习上下文语义。然而，单向模型架构仍然限制了预训练模型在NLP任务上的泛化能力，诸多NLP任务难以从单向架构中学习到更多有用的特征，例如问答系统[12]。因此，需要继续对当前的预训练架构进行优化，使得其能够适应更多种类的任务，增强其在NLP领域的通用性。</p><h3 id="2-2-创新思想"><a href="#2-2-创新思想" class="headerlink" title="2.2 创新思想"></a>2.2 创新思想</h3><p>BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，它创造性地将Transformer中的Encoder架构引入预训练模型中，成为第一个使用双向表征的预训练语言模型。同时，为了适应该双向架构，BERT引入了两项新的NLP任务——完形填空和上下句匹配，来捕获词语级别和句子级别的表征，并使之具有更强的泛化能力。</p><h3 id="2-3-具体方法"><a href="#2-3-具体方法" class="headerlink" title="2.3 具体方法"></a>2.3 具体方法</h3><p>BERT整体框架包含Pre-training和Fine-tuning两个阶段，如图2.1所示。Pre-training阶段,模型首先在设定的通用任务上，利用无标签数据进行训练。训练好的模型获得了一套初始化参数之后，再到Fine-tuning阶段，模型被迁移到特定任务中，利用有标签数据继续调整参数，直至在特定任务上重新收敛。</p><p><img src="/pic/bert/%E6%9E%B6%E6%9E%84.png" alt="BERT的pre-training和fine-tuning架构"></p><h4 id="2-3-1-模型架构"><a href="#2-3-1-模型架构" class="headerlink" title="2.3.1 模型架构"></a>2.3.1 模型架构</h4><p>BERT模型采用了Transformer中的Encoder架构，通过引入多头注意力机制，将Encoder块进行堆叠，形成最终的BERT架构。为了适应不同规模的任务，BERT将其结构分为了base和large两类。较小规模的base结构含有12个Encoder单元，每个单元中含有12个Attention块，词向量维度为768；较大规模的large结构含有24个Encoder单元，每个单元中含有16个Attention块，词向量维度为1024。通过使用Transformer作为模型的主要框架，BERT能够更彻底地捕获语句中的双向关系，极大地提升了预训练模型在具体任务中的性能。</p><p>BERT 模型的输入由三部分组成。除了传统意义上的 token 词向量外，BERT 还引入了位置词向量和句子词向量。位置词向量的思想与 Transformer 一致，但 BERT 并未使用其计算公式，而是随机初始化后放入模型一同训练；句子词向量实质上是一个0-1表征，目的是区分输入段落中的上下句。这三种不同意义的词向量相加，构成了最终输入模型的词向量。</p><p><img src="/pic/bert/%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%85%A5.png" alt="BERT的输入格式"></p><h4 id="2-3-2-预训练"><a href="#2-3-2-预训练" class="headerlink" title="2.3.2 预训练"></a>2.3.2 预训练</h4><p>BERT的预训练（pre-training）部分使用了完形填空和上下句匹配两项无监督任务。“完形填空”代表了词语级别的预训练任务，该任务对输入句子中若干随机位置的字符进行遮盖，并利用上下文语境对遮盖字符进行预测。“上下句匹配”代表了句子级别的预训练任务，该任务给出两个句子，利用句子之间的语义连贯性判定这两个句子是否存在上下句关系。这两项预训练任务对于大量NLP任务的架构具有更好的代表性，同时也更能匹配模型本身的双向架构，对模型的泛化能力有着巨大的提升帮助。</p><h4 id="2-3-3-微调"><a href="#2-3-3-微调" class="headerlink" title="2.3.3 微调"></a>2.3.3 微调</h4><p>训练具体任务时，我们只需将具体任务中的输入输出传入预训练完成的 BERT 模型，继续调整参数直至模型再次收敛。该过程称为微调（fine-tuning）。相比于预训练来说，微调的代价是极小的。在大部分NLP任务中，我们只需要在GPU上对模型进行几个小时的微调，便可使模型在具体任务上收敛，完成训练。</p><h2 id="3-实验分析与结论"><a href="#3-实验分析与结论" class="headerlink" title="3 实验分析与结论"></a>3 实验分析与结论</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>本文将BERT模型迁移至11个NLP基准任务上进行了微调训练，均取得了SOTA的效果。另外，为了探究模型的不同组成部分对整体性能的影响，本文还进行了若干消融实验，对BERT的预训练任务、模型规模等要素进行了实验评估，充分论证了双向模型的重要性。</p><h3 id="3-2-数据集和主实验分析"><a href="#3-2-数据集和主实验分析" class="headerlink" title="3.2 数据集和主实验分析"></a>3.2 数据集和主实验分析</h3><p>BERT共实现了对于11个NLP基准任务的微调训练，共对应4个数据集。本部分将详细描述各个数据集及其对应的基准任务，并介绍每个数据集上的参数设置和实现细节，以及对主实验的结果进行简要分析。</p><h4 id="3-2-1-GLUE"><a href="#3-2-1-GLUE" class="headerlink" title="3.2.1 GLUE"></a>3.2.1 GLUE</h4><p>GLUE[17]数据集共收集了包含自然语言推理、语义相似性判断等任务在内的9项NLP基准任务，并与OpenAI GPT、ELMo等性能较优的基准模型进行了结果对比。实验微调了3个epoch，将batch size设置为32，并利用验证集选择最佳学习率。实验结果如表3.1所示，结果表明，相较于当前性能最优的模型，BERTBASE 和 BERTLARGE 模型在所有任务上的性能表现均获得了较为可观的提升，平均准确度分别超过SOTA结果4.5%和7.0%。同时，BERTLARGE 在所有任务上的性能均超出了BERTBASE ，且在少样本数据集上的性能尤为突出。</p><p><img src="/pic/bert/GLUE%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.1 GLUE数据集（九项自然语言理解或生成任务）的实验结果"></p><h4 id="3-2-2-SQuAD-v1-1"><a href="#3-2-2-SQuAD-v1-1" class="headerlink" title="3.2.2 SQuAD v1.1"></a>3.2.2 SQuAD v1.1</h4><p>SQuAD v1.1[12]是一个问答任务数据集，共收集了100k组问答语句对。给定一个问句和一个包含答案的文段，需要提取出文段中该问句对应正确答案的文本范围。实验微调了3个epoch，将batch size设置为32，并将学习率固定为5e-5。实验结果如表3.2所示，结果表明，对于集成模型和单一模型这两种框架而言，BERT相比于现有的最优模型在F1指标上分别获得了1.5%和1.3%的提升，且BERT单一模型的性能甚至超过了当前最优的集成模型的性能。</p><p><img src="/pic/bert/SQuADv1.1%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.2 SQuAD v1.1数据集（基础版问答任务）的实验结果"></p><h4 id="3-2-3-SQuAD-v2-0"><a href="#3-2-3-SQuAD-v2-0" class="headerlink" title="3.2.3 SQuAD v2.0"></a>3.2.3 SQuAD v2.0</h4><p>SQuAD v2.0是在SQuAD v1.1数据集上的一个拓展，该数据集中所提供的文段中，有一定的可能性不存在对应答案，从而使得问题更切合实际。实验微调了2个epoch，将batch size设置为48，并将学习率固定为5e-5。实验结果如表3.3所示，结果表明，与先前的若干工作[18, 19]相比，BERT相较于现有的最优模型，在F1指标上获得了5.1%的提升。</p><p><img src="/pic/bert/SQuADv2.0%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.3 SQuAD v2.0数据集（拓展版问答任务）的实验结果"></p><h4 id="3-2-4-SWAG"><a href="#3-2-4-SWAG" class="headerlink" title="3.2.4 SWAG"></a>3.2.4 SWAG</h4><p>SWAG[20]是一个具有对抗性生成情形的自然语言推理数据集，它包含了113k组句子对。通过理解给定的句子，我们需要从对应的四个句子中选择最有可能延续在该句子之后的选项。实验构建了四个输入序列，每个序列包含了给定句子和可能的延续句子之间的连接。同时，还引入了一个参数向量，它与每个句子开头的 [CLS] 符号之间的点积表示该选项的最终得分。实验微调了3个epoch，将batch size设置为16，并将学习率固定为2e-5。实验结果如表3.4所示，结果表明，BERTLARGE的性能相较于ESIM+ELMo提升了27.1%，相较于OpenAI GPT提升了8.3%。</p><p><img src="/pic/bert/SWAG%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="表3.4 SWAG数据集（对抗文本生成任务）的实验结果"></p><h3 id="3-3-消融实验及结果分析"><a href="#3-3-消融实验及结果分析" class="headerlink" title="3.3 消融实验及结果分析"></a>3.3 消融实验及结果分析</h3><p>本部分对BERT模型的多个部分进行了消融实验研究，旨在探寻它们对于整体模型的重要程度。</p><h4 id="3-3-1-预训练任务"><a href="#3-3-1-预训练任务" class="headerlink" title="3.3.1 预训练任务"></a>3.3.1 预训练任务</h4><p>本部分通过对 BERT 预训练任务进行消融，旨在论证 BERT 深度双向模型这一创新思想的重要性。实验共设置了两组消融，其中一组使用双向完形填空任务但不使用上下句预测任务，另一组同样不使用上下句预测任务，但实现完形填空任务时采用从左到右的标准模型。文章首先探究了上下句预测任务的取消带来的影响，发现其严重降低了 QNLI，MNLI 和 SQuAD 1.1 这三个任务的性能。其次，通过改变完形填空任务的训练方式，来探究双向训练带来的影响。实验结果如表 3.5 所示，结果表明，在所有任务上，从左到右的单向模型性能都收获了更差的效果，在 MRPC 和 SQuAD 这两个任务上尤为显著。</p><p><img src="/pic/bert/%E5%AF%B9%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%9E%8D%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.png" alt="表3.5 对预训练任务进行消融的实验结果"></p><h4 id="3-3-2-模型规模"><a href="#3-3-2-模型规模" class="headerlink" title="3.3.2 模型规模"></a>3.3.2 模型规模</h4><p>本部分旨在探究模型大小对微调任务准确度的影响。实验设置了若干具有不同层数、隐层维度以及注意力头数目的模型，并在 GLUE 数据集上进行了微调训练。实验结果如表 3.6 所示，结果表明，即使是在有标签数据量较小的数据集上，随着模型规模的提高，任务的准确度都获得了显著的提升。现有的最大规模的 Transformer 模型[21]具有 235M 的参数量，而 BERTLARGE 进一步将参数量增加至 340M，并且使性能获得了更大的提升。此实验进一步论证了，如果模型已经经过了充分的预训练，那么当将模型缩放到一个极限的规模尺寸时，仍然能够在小规模的微调任务上产生较大的改进。</p><p><img src="/pic/bert/%E5%AF%B9%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%9E%8D%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C.png" alt="表3.6 对模型规模进行消融的实验结果"></p><h3 id="3-4-实验总结"><a href="#3-4-实验总结" class="headerlink" title="3.4 实验总结"></a>3.4 实验总结</h3><p>实验结果表明，深层的双向语言模型能够极大地改善 NLP 任务的性能。同时，预训练模型的迁移学习，逐渐成为语言理解系统中不可或缺的一部分，它甚至能够使得一些低资源的任务从深度单向架构中受益。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1]    Brown P F, Della Pietra V J, Desouza P V, et al. Class-based n-gram models of natural language[J]. Computational linguistics, 1992, 18(4): 467-480.<br>[2]    Ando R K, Zhang T, Bartlett P. A framework for learning predictive structures from multiple tasks and unlabeled data[J]. Journal of Machine Learning Research, 2005, 6(11).<br>[3]    Blitzer J, McDonald R, Pereira F. Domain adaptation with structural correspondence learning[C]//Proceedings of the 2006 conference on empirical methods in natural language processing. 2006: 120-128.<br>[4]    Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26.<br>[5]    Pennington J, Socher R, Manning C D. Glove: Global vectors for word representation[C]//Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 2014: 1532-1543.<br>[6]    Kiros R, Zhu Y, Salakhutdinov R R, et al. Skip-thought vectors[J]. Advances in neural information processing systems, 2015, 28.<br>[7]    Logeswaran L, Lee H. An efficient framework for learning sentence representations[J]. arXiv preprint arXiv:1803.02893, 2018.<br>[8]    Le Q, Mikolov T. Distributed representations of sentences and documents[C]//International conference on machine learning. PMLR, 2014: 1188-1196.<br>[9]    Peters M E, Ammar W, Bhagavatula C, et al. Semi-supervised sequence tagging with bidirectional language models[J]. arXiv preprint arXiv:1705.00108, 2017.<br>[10]    Peters M, Neumann M, Iyyer M, et al. Deep contextualized word representations[A]. Conference of the North American Chapter of the Association for Computational Linguistics[C]. New Orleans, Louisiana, Association for Computational Linguistics, 2018a: 2227-2237.<br>[11]    Socher R, Perelygin A, Wu J, et al. Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proceedings of the 2013 conference on empirical methods in natural language processing. 2013: 1631-1642.<br>[12]    Rajpurkar P, Zhang J, Lopyrev K, et al. Squad: 100,000+ questions for machine comprehension of text[J]. arXiv preprint arXiv:1606.05250, 2016.<br>[13]    Sang E F, De Meulder F. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition[J]. arXiv preprint cs/0306050, 2003.<br>[14]    Radford A, Narasimhan K, Salimans T, et al. Improving language understanding with unsupervised learning[J]. 2018.<br>[15]    Deng J, Dong W, Socher R, et al. Imagenet: A large-scale hierarchical image database[C]//2009 IEEE conference on computer vision and pattern recognition. Ieee, 2009: 248-255.<br>[16]    Yosinski J, Clune J, Bengio Y, et al. How transferable are features in deep neural networks?[J]. Advances in neural information processing systems, 2014, 27.<br>[17]    Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis platform for natural language understanding[J]. arXiv preprint arXiv:1804.07461, 2018.<br>[18]    Sun F, Li L, Qiu X, et al. U-net: Machine reading comprehension with unanswerable questions[J]. arXiv preprint arXiv:1810.06638, 2018.<br>[19]    Wang W, Yan M, Wu C. Multi-granularity hierarchical attention fusion networks for reading comprehension and question answering[J]. arXiv preprint arXiv:1811.11934, 2018.<br>[20]    Zellers R, Bisk Y, Schwartz R, et al. Swag: A large-scale adversarial dataset for grounded commonsense inference[J]. arXiv preprint arXiv:1808.05326, 2018.<br>[21]    Al-Rfou R, Choe D, Constant N, et al. Character-level language modeling with deeper self-attention[C]//Proceedings of the AAAI conference on artificial intelligence. 2019, 33(01): 3159-3166.</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 自然语言处理 </tag>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>综述：深度学习中的优化方法</title>
      <link href="/2022/06/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/"/>
      <url>/2022/06/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<blockquote><p>优化是深度学习中的关键组成部分。在深度模型中，我们通常需要设计一个模型的损失函数来约束训练过程，并朝着最小化代价函数的方向去训练。然而，由于深度网络的复杂性，使得深度学习中的优化问题面临着诸多困难和挑战，至今仍未得到一个完美的解决方案。目前，深度神经网络的参数学习主要是通过梯度下降法来寻找一组可以最小化结构风险的参数。本文从梯度下降法出发，通过探寻深度优化问题中的若干可改进点，对优化算法的变体进行归类探究。除此之外，文章还简要分析了模型的初始化参数和输入值尺度这两个重要方面，辅助优化算法在深度模型上获得更好的效果。</p></blockquote><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><p>优化算法是深度学习中十分重要的环节。深度学习中的优化，往往是通过给定一个以模型参数为变量的函数，一般称为损失函数或目标函数，通过具体算法找到合适的参数值，使得该函数最小化[17]。从理论上来说，对于一个存在解析形式的函数，一定能够通过最优性条件[1;13]求解出显式最优解。然而，由于方程求解的难度过大，往往使用基于迭代的数值计算方法去优化目标。较为常见的有梯度下降法、牛顿法、共轭梯度法等。</p><p>摒弃了传统机器学习对于人工特征工程的高度依赖，深度学习往往通过构建复杂度高、参数量大的深层次网络模型，去让模型自主学习到繁杂的数据特征。但是，面对如此高复杂度的含参模型，算力的限制对模型的训练速度提出了很高的要求。同时，高维度的损失函数存在着大量不可预知的特殊因素，例如函数中的鞍点、高原、平坦区域等，这些对优化算法的性能也提出了很高的要求。因此，相较于传统的机器学习优化算法[19;2]，必须在它们的基础上做出相应的改进，以解决它们被应用在深度学习中时面临的诸多困境。</p><p>本文从最传统的一阶和二阶优化算法出发，首先在第 2 节介绍了经典的梯度下降法和牛顿法，为后续的变体算法做铺垫。之后三个章节从三个视角切入，对优化算法的改进版本进行了分类和对比。第 3 节从训练速度和精度的角度出发，探究了在训练过程中，每次训练时输入数据的规模大小对优化问题的影响。第 4 节探究了收敛速度对整个训练过程的影响，通过改进学习率设置来让收敛速度自适应整个训练过程，保证收敛的准确性。第 5 节考虑到随机梯度下降中梯度估计的随机性，故对梯度估计方法进行了修改，使梯度更新在训练过程中更加稳定。最后，第 6 节放眼优化算法之外，探究了模型初始化参数和输入值尺度两大重要因素对优化准确度的影响，并给出了已有的一些解决方案。全文的工作流程以及对优化问题的划分如图 1 所示。</p><p><img src="/pic/optimization/overview.png" alt="图 1：本文对于优化问题的归类划分，以及不同算法之间的关联。文章将按照该分类依据进行对应方法的介绍。"></p><h2 id="2-背景"><a href="#2-背景" class="headerlink" title="2 背景"></a>2 背景</h2><h3 id="2-1-形式化描述"><a href="#2-1-形式化描述" class="headerlink" title="2.1 形式化描述"></a>2.1 形式化描述</h3><p>以有监督模型为例，我们首先对优化问题的定义进行简单的描述。假设输入数据$(x_i, y_i) \in \mathbb{R}^{d_x} \times \mathbb{R}^{d_y}$，$i = 1,2,…,n$，其中$n$为输入的样本数量。对于具体任务而言，我们需要学习一个网络映射，将输入值$x_i$映射为一个新的取值$\hat{y}_i$，作为该网络的输出值，即模型的预测值。之后，将预测值$\hat{y}_i$与真实值$y_i$进行对比，该过程一般通过构造损失函数来实现，该函数往往反映了真实值和预测值之间的误差大小。优化算法所需要解决的，便是最小化该损失函数这一重要问题。</p><p>我们首先构造一个网络映射 $f_{\theta} : \mathbb{R}^{d_x} \times \mathbb{R}^{d_y}$，其中 $\theta$ 为需要优化的网络参数集合。同时，定义损失函数为 $\mathcal{L}(\cdot, \cdot)$，则优化问题可被形式化定义为</p><p>$$<br>    \min_{\theta} \frac{1}{n} \sum_{i=1}^n \mathcal{L} (y_i, f_{\theta}(x_i))<br>$$</p><p>损失函数的选取对于优化问题也十分关键，通常来说，我们需要根据具体任务的需求来设置特定于任务的损失函数。例如，对于回归预测问题，我们通常采用均方误差损失函数，即 $\mathcal{L}(y, y’) = ||y - y’||^2$。对于分类问题，我们通常采用交叉熵或KL散度来比较预测分布和真实分布之间的差异。</p><h3 id="2-2-基础算法"><a href="#2-2-基础算法" class="headerlink" title="2.2 基础算法"></a>2.2 基础算法</h3><p>本部分我们将介绍深度学习优化问题中的两大基础算法 —— 梯度下降法和牛顿法，分别对应基本的一阶和二阶算法。后续针对优化算法的改进，通常在这两个基础算法上进行迭代更新。</p><h4 id="2-2-1-梯度下降法"><a href="#2-2-1-梯度下降法" class="headerlink" title="2.2.1 梯度下降法"></a>2.2.1 梯度下降法</h4><p>梯度下降法是一种基于迭代的最小化目标函数的优化算法。对于函数 $f(x)$， 若 $f(x)$ 在 $x$ 附近连续可微，则 $f(x)$ 下降最快的方向为 $f(x)$ 在 $x$ 点处的梯度反方向。基于这一思想，假设目标函数为 $J(\theta)$ ，其中 $\theta$ 为模型参数，则梯度下降法需要利用该函数的反向梯度 $-\nabla_{\theta} J(\theta)$，来对目标参数进行更新。同时，该方法还设置了一个学习率 $\alpha$，来进一步限制每次更新时的步长大小。</p><p>具体的，假设参数 $\theta$ 的初始值为 $\theta_0$，则对于第 $t$ 次参数更新，迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \nabla_{\theta} J(\theta)<br>$$</p><p>通过迭代更新，将会生成参数序列 $\theta_0. \theta_1, \theta_2, …，\theta_t, …$，直至该序列收敛到期望最优解 $\theta^*$，则算法终止。<br>该算法由于利用了目标函数的一阶导数进行迭代更新，故被归为一阶优化算法。</p><h4 id="2-2-2-牛顿法"><a href="#2-2-2-牛顿法" class="headerlink" title="2.2.2 牛顿法"></a>2.2.2 牛顿法</h4><p>牛顿法[3]是一种基于二阶导数的迭代优化算法。对于目标函数 $J(\theta)$，假设 $J(\theta)$ 连续二阶可微，将其在 $\theta_t$ 处进行二阶泰勒展开，并取二阶近似</p><p>$$<br>    J(\theta) \approx J(\theta_t) + \nabla_{\theta} J(\theta_t)^T (\theta - \theta_t) + \frac{1}{2} (\theta - \theta_t)^T \nabla^2_{\theta} J(\theta_t)(\theta - \theta_t)<br>$$</p><p>其中，$\nabla^2_{\theta} J(\theta_t)$ 是 $J(\theta)$ 在 $\theta_t$ 处的 Hesse 矩阵。令 $\nabla_{\theta} J(\theta) = 0$，则</p><p>$$<br>    \nabla_{\theta} J(\theta_t) + \nabla^2_{\theta} J(\theta_t)(\theta - \theta_t) = 0<br>$$</p><p>设 $\nabla^2_{\theta} J(\theta_t)$ 可逆，则可以得到牛顿法的迭代公式</p><p>$$<br>    \theta_{t+1} = \theta_t - \nabla^2_{\theta} J(\theta_t)^{-1} \nabla_{\theta} J(\theta_t)<br>$$</p><p>相比于基于一阶导数的梯度下降法，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑走了一步之后，坡度是否会变得更大。因此，牛顿法比梯度下降法看得更远一点，能更快地走到最底部。而梯度下降法每次只从当前所处位置选一个坡度最大的方向走一步。可视化效果如图 2 所示。</p><p><img src="/pic/optimization/%E4%B8%80%E9%98%B6%E4%BA%8C%E9%98%B6.png" alt="图 2：一阶和二阶算法的迭代下降过程。红线表示牛顿法，更新方向相对精准，但计算时间相对较长；绿线表示梯度下降法，更新方向相对随机，但节约了大量计算时间。"></p><p>尽管牛顿法收敛更快，但是每次迭代的时间比梯度下降法长，因此相对而言并不常用。接下来介绍的所有优化算法，均在一阶梯度下降法的基础上进行改进。</p><h2 id="3-输入数据的规模：精度与速度的权衡"><a href="#3-输入数据的规模：精度与速度的权衡" class="headerlink" title="3 输入数据的规模：精度与速度的权衡"></a>3 输入数据的规模：精度与速度的权衡</h2><p>深度学习中，为了减少单次训练时的随机性影响，避免因为数据噪声产生的不收敛现象，而引入了“批处理”的思想。简而言之，就是在单次训练过程中，向网络中传入多组数据，数据规模称为 batch size，一次完整的训练称为一个 batch。对于一个batch中的每个训练样本，我们都会得到一个单独的损失值，通过对所有样本的损失值取平均，构造最终的损失函数，来减弱单样本的随机性误差。然而，随着batch size的增大，每次训练需要的样本数增多，所耗费的时间代价也会随之增加。因此，需要在精度和速度之间，找寻到一个微妙的平衡。</p><p>本部分将以 batch size 为分类依据，介绍三种典型算法。首先介绍随机梯度下降（SGD）和批量梯度下降（BGD），分别对应速度和精度的两个极端情形。最后在此基础上引入小批量梯度下降（MSGD），通过调整控制 batch size的大小，来平衡精度和速度两项指标之间的关系。</p><h3 id="3-1-全批量梯度下降（BGD）"><a href="#3-1-全批量梯度下降（BGD）" class="headerlink" title="3.1 全批量梯度下降（BGD）"></a>3.1 全批量梯度下降（BGD）</h3><p>对于每一次参数更新，最朴素的思想则是将所有训练样本放入网络，利用完整的数据集进行损失函数的计算。这种思想的主要依据在于，利用全数据集训练时，确定的梯度下降方向能够更好地代表样本总体，从而能够更加准确地朝向极值方向。</p><p>假设数据集规模为 $n$，输入样本为 $(x^{(i)}, y^{(i)})$，$i = 1,2,…,n$，则 $t+1$ 时刻的迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \frac{1}{n} \sum_{i=1}^n \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>大规模样本下，模型收敛的稳定性得到了保证。同时，由于batch size较大，所需要的batch数随之减少，因此总的训练时间会进一步缩短。然而，过大的 batch size ，使得模型的探索性变差，容易在起始点附近很近的地方停止更新。同时，样本中的噪声影响显著削弱，使得函数难以跳出尖锐极小点[9]，从而限制了模型的泛化能力。</p><h3 id="3-2-随机梯度下降（SGD）"><a href="#3-2-随机梯度下降（SGD）" class="headerlink" title="3.2 随机梯度下降（SGD）"></a>3.2 随机梯度下降（SGD）</h3><p>随机梯度下降[16]处理了 batch size 大小为 1 时的情形，即每次进行参数更新时，只使用一个训练样本。假设输入为 $x^{(i)}$，对应标签为 $y^{(i)}$，则迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>相比于全批量梯度下降法，随机梯度下降的一大优势在于，由于每次训练样本的随机性，使得样本中的噪声也能够被充分利用，进而提升了模型的泛化能力。同时，由于每次训练时只需要考虑唯一的样本，因此训练速度也会显著增加，可以被用作在线学习。<br>然而，该方法的缺点也十分明显。由于噪声的增多，尽管模型的泛化能力增强，但也增加了模型的收敛难度。尽管每次迭代整体上会朝着最优方向下降，但每次更新时的方向较为杂乱，且容易陷入局部极小值或鞍点。</p><h3 id="3-3-小批量梯度下降（MSGD）"><a href="#3-3-小批量梯度下降（MSGD）" class="headerlink" title="3.3 小批量梯度下降（MSGD）"></a>3.3 小批量梯度下降（MSGD）</h3><p>在 BGD 和 SGD 中，我们发现，batch size的大小对优化算法的结果，会产生较为显著的影响，且主要集中在“精度”和“速度”这两个重要维度。在 BGD 中，我们使用了全数据集样本进行单次训练。尽管样本数的扩大，会使得模型收敛更加稳定，但单次训练需要遍历完整的数据集，使得训练速度大大降低。在 SGD 中，我们使用了随机的单一样本进行单次训练。由于每次只需计算一个样本，因此单次训练的速度大大提高，但噪声的影响在训练过程中被显著放大，对于模型的收敛准度带来了极大的挑战。</p><p>MSGD 方法的引入，正是为了解决batch size 过大或过小，而导致的精度和速度无法收敛的问题。假设每一次训练中使用 $m$ 个样本，则 $t+1$ 时刻的迭代公式为</p><p>$$<br>    \theta_{t+1} = \theta_t - \alpha \cdot \frac{1}{m} \sum_{i=1}^m \nabla_{\theta} J(\theta; x^{(i)}; y^{(i)})<br>$$</p><p>一般来说，$m$ 的取值在几到几百之间，可以根据具体任务规模而定。batch size的变动可以随之对学习率的大小进行调整。batch size设置较大时，随机梯度方差越小，则训练更稳定，可以设置更大的学习率。batch size 较小时，需要较小的学习率来保证收敛稳定。对于batch size和学习率之间的调整关系，我们可以采用线性缩放原则来进行大致估计，即批量大小增加 $k$ 倍时，学习率也增加 $k$ 倍。</p><p>小批量梯度下降已经称为了梯度下降法中的一个最优变体模型。然而，该算法仍然存在着若干挑战。首先，batch size 作为一个重要的超参数，需要我们根据具体训练效果进行人工调整。其次，学习率设置和梯度估计方法这两部分，在迭代公式中也是不可被忽视的因素。因此，我们需要对这两部分进行进一步的优化，使得 MSGD 方法能够获得更好的性能。</p><h2 id="4-收敛速度控制：学习率调整"><a href="#4-收敛速度控制：学习率调整" class="headerlink" title="4 收敛速度控制：学习率调整"></a>4 收敛速度控制：学习率调整</h2><p>在前一部分中，所有的参数迭代公式均包含一项超参数，即学习率 $\alpha$，用来控制参数在更新迭代时的步幅大小。学习率的选取对于参数的更新至关重要，它决定了网络模型是否能够收敛至全局最小值。若学习率过大，则模型难以收敛，若学习率过小，则模型收敛过慢，如图 3 所示。因此，我们需要在梯度下降法的基础上，对训练过程中的学习率进行优化调整，使得模型的收敛过程更加符合预期效果。</p><p><img src="/pic/optimization/learning_rate.png" alt="图 3：学习率过大或过小时的收敛情况。图（a）表示学习率过大的情况，损失始终处于振荡下降状态，无法准确收敛至最小点；图（b）表示学习率过小的情况，损失下降过于缓慢，导致探索到更优的最小点的概率大大减小。"></p><h3 id="4-1-学习率预热-——-Warmup"><a href="#4-1-学习率预热-——-Warmup" class="headerlink" title="4.1 学习率预热 —— Warmup"></a>4.1 学习率预热 —— Warmup</h3><p>由于刚开始训练时,模型的参数是随机初始化的，此时若选择一个较大的学习率,可能会使得模型不稳定，产生振荡现象。因此，通过预热学习率的方式，可以使得训练初期的学习率较小,在小学习率下，模型可以慢慢趋于稳定。等模型相对稳定后，再去选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。</p><p>假设预热的迭代总次数为 $T$，初始学习率为 $\alpha_0$，当前迭代次数为 $t$，则最初的 $T$ 次迭代中，学习率动态变化的计算方式为</p><p>$$<br>    \alpha_t = \alpha_0 \cdot \frac{t}{T}<br>$$</p><h3 id="4-2-学习率衰减-——-Decay"><a href="#4-2-学习率衰减-——-Decay" class="headerlink" title="4.2 学习率衰减 —— Decay"></a>4.2 学习率衰减 —— Decay</h3><p>一般来说，在参数更新初期，由于距离最优点较远，因此应该保证较大的学习率，充分探索邻近区域，保证较快的收敛速度。到达后期时，参数逐步逼近最优点，因此需要保证较小的学习率，来防止在最优点附近来回震荡。</p><p>基于这样的经验，我们发现，随着参数更新的不断进行，需要不断降低学习率的数值，使得更新的步幅逐渐变小，保证收敛的稳定性。从这点出发，我们可以通过将学习率常量改变为一个随时间动态可变的数值，来满足这一要求。</p><p>不失一般性，我们将学习率按迭代次数 $t$ 进行衰减。设学习率初始值为 $\alpha_0$，在 $t$ 次迭代时的学习率为 $\alpha_t$，则可给出 $\alpha_t$ 关于迭代次数和学习率初始值的衰减函数表达式 $\alpha_t = f(\alpha_0, t)$。常见的衰减表达式有如下几种：</p><ul><li>阶梯衰减： 该方法每经过 $T_1, T_2, …, T_m$ 次迭代，将学习率衰减为原来的 $\beta_1, \beta_2, …, \beta_m$ 倍，所有参数均为人工设置的超参数。</li><li>指数衰减： 设 $\beta &lt; 1$ 为衰减率，则 $\alpha_t = \alpha_0 \beta^t$，$\beta$ 可取0.95，0.98等。</li><li>自然指数衰减： 设 $\beta$ 为衰减率，则 $\alpha_t = \alpha_0 e^{-\beta \times t}$，$\beta$ 可取0.02，0.05等。</li><li>逆时衰减： 设 $\beta$ 为衰减率，则 $\alpha_t = \alpha_0 \frac{1}{1 + \beta \times t}$。</li><li>余弦衰减： 设 $T$ 为总的迭代次数，则 $\alpha_t = \frac{1}{2} \alpha_0 (1 + \cos (\frac{t \pi}{T}))$。</li></ul><h3 id="4-3-Warmup-和-Decay"><a href="#4-3-Warmup-和-Decay" class="headerlink" title="4.3 Warmup 和 Decay"></a>4.3 Warmup 和 Decay</h3><p>对于一个完整的训练过程，通常结合使用预热和衰减两种学习率调整方式。训练开始时，由于参数初始化的随机性，需要将学习率控制在一个较小的范围，逐渐增大，直至训练稳定。稳定后，模型的分布就较为固定了，若还沿用较大的学习率，就会破坏稳定性，很有可能错过局部最优点。这时应逐步降低学习率，使得模型能够稳定收敛至局部最小值。</p><h3 id="4-4-周期性学习率调整"><a href="#4-4-周期性学习率调整" class="headerlink" title="4.4 周期性学习率调整"></a>4.4 周期性学习率调整</h3><p>通过学习率预热和衰减的方式，收敛稳定性的问题已经可以被初步解决。然而，在参数迭代的过程中，函数鞍点和尖锐最小值的存在，使得模型参数极有可能陷入较差的局部最优解。因此，考虑到这些特殊情况的存在，我们需要适当的对学习率进行增大，使得参数能够有更大概率逃离尖锐或平坦最小值。尽管这样的操作，在短期内可能损害优化过程，但从长远来看，有助于找到更优的局部最小值[7]。</p><p>周期性学习率[12;18]调整方法是在保证模型收敛稳定的基础上，对函数中的特殊点进行了一个更为周到的考虑，使其能够跳出较差的局部最优，向着更优的局部最小去探索。常用的周期学习率调整方法有如下两种：</p><ul><li>循环学习率： 让学习率在某个区间内周期性增大或减小。假设一个循环周期为 $2 \Delta T$，其中前 $\Delta T$ 步线性增大，后 $\Delta T$ 步线性减小。假设循环周期数为 $m$，则第 $t$ 次迭代时，所在的循环周期数 $m$ 为<br>$$<br>  m = [1 + \frac{t}{2\Delta T}]<br>$$</li></ul><p>假设第 $m$ 个周期内，学习率最大和最小值分别为 $\alpha_{\min}^m$ 和 $\alpha_{\min}^m$，则第 $t$ 次迭代的学习率为<br>$$<br>    \alpha_t = \alpha_{\min}^m + (\alpha_{\max}^m - \alpha_{\min}^m)\cdot \max(0, 1 - |\frac{t}{\Delta T} - 2m + 1|)<br>$$</p><ul><li>带热重启的随机梯度下降： 该方法采用热重启方式来替代学习率衰减方法。每间隔一段时间，学习率会重新初始化为某个预设的值，之后继续衰减。</li></ul><p>假设整个迭代过程共重启 $M$ 次，每次的重启周期为 $T_m$，重启后采用余弦衰减方法来降低学习率。则 $t$ 次迭代后的学习率为<br>$$<br>    \alpha_t = \alpha_{\min}^m + \frac{1}{2}(\alpha_{\max}^m - \alpha_{\min}^m)(1 + \cos(\frac{T_{cur}}{T_m} \pi))<br>$$</p><p>其中，$\alpha_{\min}^m$ 和 $\alpha_{\min}^m$ 表示第 $m$ 个周期内，学习率的最大和最小值。$T_m$ 为重启周期，$T_{cur}$ 为上次重启之后的 epoch 数。</p><p>不管是哪种周期调整方式，尽管在某一段时间内，学习率会周期性上升或下降，但从全局来看，学习率整体仍保持着下降的趋势，这也为模型的收敛稳定性提供了保障。</p><h3 id="4-5-自适应学习率"><a href="#4-5-自适应学习率" class="headerlink" title="4.5 自适应学习率"></a>4.5 自适应学习率</h3><p>在梯度下降法中，每次参数迭代的对象都涉及到多个参数。然而，对于每个参数，对应的学习率都完全相同。由于每个参数在对应维度上的收敛速度互不相同，则会导致某些参数对于更新方向不够敏感，从而造成精度损失。早期的一种在训练时适应模型参数各自学习率的启发式方法是，如果损失对于某个给定参数模型的偏导保持相同符号，则学习率增加，反之减小。这也是自适应学习率思想的早期雏形。</p><p>自适应学习率是一种适应于参数的动态学习率设置方法，该方法根据参数收敛的不同情况，分别设置对应的学习率。接下来介绍三种经典的自适应学习率算法 —— AdaGrad、RMSprop、AdaDelta。</p><h4 id="4-5-1-AdaGrad"><a href="#4-5-1-AdaGrad" class="headerlink" title="4.5.1 AdaGrad"></a>4.5.1 AdaGrad</h4><p>AdaGrad 算法[5]采用累计平方梯度来独立适应所有模型参数的学习率，其学习率设置与梯度历史平方值总和呈现反比关系。若该参数的损失偏导较大，则学习率衰减速度较快，若损失偏导较小，则学习率下降较慢。其目的是让参数在更为平缓的倾斜方向上取得更大的进步。</p><p>首先，在第 $t$ 次迭代时，我们需要计算每个参数梯度平方的累计值<br>$$<br>    G_t = \sum_{\tau=1}^{t} g_{\tau} \odot g_{\tau}<br>$$</p><p>其中，$\odot$ 为按元素乘积，$g_{\tau} \in \mathbb{R}^{|\theta|}$ 为第 $\tau$ 次迭代时的梯度。则 AdaGrad 最终的参数更新为<br>$$<br>    \begin{aligned}<br>        \theta_{t+1} &amp;= \theta_t + \Delta \theta \<br>        &amp;= \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\odot g_t<br>    \end{aligned}<br>$$</p><p>尽管 AdaGrad 算法考虑到了不同参数的梯度变化情况，但是当经过一定迭代次数之后，若仍然未找到最优点，则由于学习率较小，会很难继续找到最优点。</p><h4 id="4-5-2-RMSprop"><a href="#4-5-2-RMSprop" class="headerlink" title="4.5.2 RMSprop"></a>4.5.2 RMSprop</h4><p>AdaGrad 算法采用了历史累计平方梯度来适应参数学习率，这会使得学习率迅速衰减，因此。RMSprop[20]在此基础上进行了优化，采用指数衰减平均来丢弃遥远的历史。具体来说，它将梯度积累方法改变为指数加权的移动平均方法，使得过于遥远的梯度在调整学习率时的重要性降低。这样的改进，使得学习率并不完全呈现衰减趋势，可变小也可变大，避免了过早衰减的问题。</p><p>方法的整体迭代流程与 AdaGrad 类似，唯一不同的是将累计梯度平方改为了指数衰减移动平均。具体计算方法为<br>$$<br>    \begin{aligned}<br>    G_t &amp;= \beta G_{t-1} + (1 - \beta) g_t \odot g_t \<br>    &amp;= (1 - \beta) \sum_{\tau = 1}^t \beta^{t-\tau} g_{\tau} \odot g_{\tau}<br>    \end{aligned}<br>$$</p><p>同样的，RMSprop 最终的参数更新为<br>$$<br>    \begin{aligned}<br>        \theta_{t+1} &amp;= \theta_t + \Delta \theta \<br>        &amp;= \theta_t - \frac{\alpha}{\sqrt{G_t + \epsilon}}\odot g_t<br>    \end{aligned}<br>$$</p><p>其中，$\beta$ 为衰减率，一般设置为 0.9；$\alpha$ 为初始学习率，一般设置为 0.001。</p><h4 id="4-5-3-AdaDelta"><a href="#4-5-3-AdaDelta" class="headerlink" title="4.5.3 AdaDelta"></a>4.5.3 AdaDelta</h4><p>AdaDelta[22]在 RMSprop 的基础上做了进一步的改进。和 RMSprop 一样，在调整学习率时采用了指数衰减移动平均的方法。唯一不同的时，AdaDelta 在进行参数更新时，其参数更新差值 $\Delta \theta$ 也采用了指数衰减平均的方式进行动态变化。</p><p>具体的，该方法将参数更新表达式中的初始学习率 $\alpha$ 替换为一个随时间变化的参数 $\sqrt{\Delta X^2_{t-1} + \epsilon}$，其中微小量 $\epsilon$ 用来保证数值稳定。则 $\Delta \theta$ 的指数衰减移动平均计算方式为</p><p>$$<br>    \Delta X^2_{t-1} = \beta_1 \Delta X^2_{t-2} + (1 - \beta_1)\Delta \theta_{t-1} \odot \Delta \theta_{t-1}<br>$$</p><p>其中 $\beta_1$ 为衰减率，最终的参数更新方式为<br>$$<br>    \begin{aligned}<br>        \theta_{t+1} &amp;= \theta_t + \Delta \theta \<br>        &amp;= \theta_t - \frac{\sqrt{\Delta^2_{t-1}} + \epsilon}{\sqrt{G_t + \epsilon}}\odot g_t<br>    \end{aligned}<br>$$</p><h2 id="5-梯度估计修正"><a href="#5-梯度估计修正" class="headerlink" title="5 梯度估计修正"></a>5 梯度估计修正</h2><p>在梯度下降法中，一个重要的前提条件是，函数在某一点的最速下降方向为该点的梯度反方向，因此采用梯度对参数进行迭代更新。然而，在小批量梯度下降中，由于样本选取的随机性，导致其梯度估计也存在一定的随机性，从而导致损失振荡下降，训练不够稳定。因此，需要设计一种新的梯度估计方法，让梯度的更新在训练过程中保持稳定，不受样本随机性的干扰。</p><h3 id="5-1-Momentum-动量法"><a href="#5-1-Momentum-动量法" class="headerlink" title="5.1 Momentum 动量法"></a>5.1 Momentum 动量法</h3><p>若能降低单次训练中随机样本带来的梯度随机性，则能够有效缓解损失振荡的现象。一种较为直接的想法是，采用最近一段时间训练时的平均梯度来代替单次训练的梯度，从而使参数更新的方向更加稳定，提高整体的优化速度。</p><p>动量法[15]则是基于这样的思想，对梯度下降的更新方向进行了稳定，缓解了振荡现象，从而加速了优化速度。具体地，它将参数更新中的负梯度更新方向改为了负梯度的“加权移动平均”方向，使得每个参数的实际更新差值由最近一段时间内梯度的加权平均值决定。在第 $t$ 次迭代中，参数的更新公式为<br>$$<br>    \begin{aligned}<br>    \theta_t &amp;= \theta_{t-1} + \Delta \theta_t \<br>    &amp;= \theta_{t-1} + \rho \Delta \theta_{t-1} - \alpha g_t \<br>    &amp;= \theta_{t-1} -  \alpha \sum_{\tau = 1}^t \rho^{t-\tau} g_{\tau}<br>    \end{aligned}<br>$$</p><p>其中，$\rho$ 为动量衰减因子，通常设置为 0.9或近似值；$\alpha$ 为学习率。如图 4 所示，引入了累计加权平均梯度的动量方法后，振荡现象明显减弱，参数能够更快并更准确地向最优方向收敛。</p><p><img src="/pic/optimization/%E5%8A%A8%E9%87%8F.png" alt="图 4：引入动量法前后的梯度下降收敛过程。左侧为引入动量法前的收敛过程，收敛的振荡现象较为严重；右侧为引入动量法后的收敛过程，梯度的更新方向更加准确，减少了收敛的不确定性，也加速了整个收敛过程。"></p><p>动量法实质上来自于一个物理类比。动量是质量和速度的乘积，我们规定单位质量，则动量大小即为速度大小，在迭代公式中即为 $\Delta  \theta$。我们将收敛过程抽象成一个小球从山顶向山谷滚去，在更新公式中，当前时刻的梯度值 $-\alpha g_t$ 即为力的提供者，推动小球沿着最为陡峭的方向向下滚动，从而累计动量，速度不断加快。另外，累计梯度 $\rho \Delta \theta_{t-1}$ 代表了粘滞阻力，当小球遇到上坡或障碍时，该阻力的存在能够及时阻止其偏离方向，从而阻止其发生大幅度振荡，加速其滚动至最低点。</p><h3 id="5-2-Nesterov-加速梯度动量"><a href="#5-2-Nesterov-加速梯度动量" class="headerlink" title="5.2 Nesterov 加速梯度动量"></a>5.2 Nesterov 加速梯度动量</h3><p>动量法中，单一梯度被修正为累计加权平均梯度，有效缓解了振荡现象的发生。然而，这样的方法仍然存在一定的问题。仍以小球滚动为例，动量法中，只有当小球开始产生振荡，偏离最速下降方向之后，它才会意识到错误，开始进行相应的调整。而更科学高效的方法应该是，在小球意识到将要偏离方向之前，就开始有意识地对自身的下降方向进行调整。它应当具备一定的预测判断能力。</p><p>动量法中，我们首先计算了当前时候的负梯度值 $-g_t$，并与上一时刻的梯度更新方向 $\Delta \theta_{t-1}$ 进行加权求和，获取当前时刻的梯度更新结果。而在 Nesterov 加速梯度动量~\cite{nesterov1983method}中，梯度计算的执行被安排在施加当前速度之后。具体的计算公式如下</p><p>$$<br>    \begin{aligned}<br>    \theta_t &amp;= \theta_{t-1} + \Delta \theta_t \<br>    &amp;= \theta_{t-1} + \rho \Delta \theta_{t-1} - \alpha \nabla_{\theta}(\theta_{t-1} + \rho \Delta \theta_{t-1})<br>    \end{aligned}<br>$$</p><p>Nesterov 动量可以解释为往标准动量方法中添加了一个校正因子。在动量法中，梯度更新方式可以被拆分为两步。首先，利用累计梯度校正参数，即 $\hat{\theta}<em>t = \theta</em>{t-1} + \rho \Delta \theta_{t-1}$，之后，再加入当前梯度值，进行梯度下降，即 $\theta_{t} = \hat{\theta}<em>t - \alpha g_t$。然而，我们在首先对参数进行了校正后，仍然对当前时刻的梯度进行了计算，一种更合理的方式应该是，对校正后的参数进行梯度计算，能够进一步缓解当前梯度的随机性带来的误差。因此，将 $\alpha g_t$ 改为 $\alpha \nabla</em>{\theta}(\theta_{t-1} + \rho \Delta \theta_{t-1})$，便得到了 Nesterov 动量的核心思想。该方法与原始动量方法的可视化比较如图~\ref{fig:comparison} 所示。</p><p><img src="/pic/optimization/%E5%8A%A8%E9%87%8F%E6%AF%94%E8%BE%83.png" alt="图 5：动量法和Nesterov加速梯度 动量的更新方式比较。左侧为动量法的更新方式，累计动量和当前梯度的计算同时进行；右侧为Nesterov 加速梯度动量的更新方式，首先对当前动量进行更新，之后在更新后的基础上进行梯度计算。"></p><h3 id="5-3-Adam"><a href="#5-3-Adam" class="headerlink" title="5.3 Adam"></a>5.3 Adam</h3><p>Adam 算法[10]是动量法和 RMSprop 的结合，它不但使用动量作为参数更新的方向，同时也可以自适应地调整学习率，帮助其稳定收敛。</p><p>第 $t$ 次迭代时，Adam 算法首先计算了累计梯度平方的加权平均 $G_t$ 和梯度加权平均 $M_t$，计算公式如下<br>$$<br>    \begin{aligned}<br>    &amp;G_t = \beta_1 G_{t-1} + (1 - \beta_1) g_t \odot g_t \<br>    &amp;M_t = \beta_2 M_{t-1} + (1 - \beta_2) g_t<br>    \end{aligned}<br>$$</p><p>其中，$\beta_1$ 和 $\beta_2$ 分别为两个移动平均的衰减率，取值通常趋近于 1。在迭代初期，$G_t$ 和 $M_t$ 通常会比真实值更小，且偏差较大。因此需要对偏差进行修正<br>$$<br>    \begin{aligned}<br>    &amp;\hat{G}_t = \frac{G_t}{1 - \beta^t_1} \<br>    &amp;\hat{M}_t = \frac{M_t}{1 - \beta^t_2}<br>    \end{aligned}<br>$$</p><p>修正后，可以得到 Adam 算法的最终更新公式<br>$$<br>    \theta_{t} = \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{G}_t} + \epsilon} \hat{M}_t<br>    \label{Adam}<br>$$</p><p>一般来说，$\beta_1$ 取0.999，$\beta_2$ 取0.9，$\epsilon$ 为稳定数值的微小量，一般取 $10^{-8}$。与其他算法相比，Adam 算法在实际应用中表现更为突出。</p><h3 id="5-4-Nadam"><a href="#5-4-Nadam" class="headerlink" title="5.4 Nadam"></a>5.4 Nadam</h3><p>Nadam[4]算法在 Adam 算法的基础上，进一步引入了 Nesterov 加速梯度。在公式<del>\ref{Adam} 中，梯度估计值 $\hat{M}_t$ 是利用动量法进行更新的（参考公式</del>\ref{21} 和~\ref{22}）。Nadam 对梯度估计值进行了调整。修正后的迭代公式为<br>$$<br>    \begin{aligned}<br>        \theta_{t} &amp;= \theta_{t-1} - \frac{\alpha}{\sqrt{\hat{G}_t} + \epsilon} \hat{M}<em>t \<br>        &amp;= \theta</em>{t-1} - \frac{\alpha}{\sqrt{\hat{G}<em>t} + \epsilon} (\frac{\beta_1 M</em>{t-1}}{1 - \beta_1^t} + \frac{(1 - \beta_1)g_t}{1 - \beta_1^t})<br>    \end{aligned}<br>$$</p><h3 id="5-5-梯度截断"><a href="#5-5-梯度截断" class="headerlink" title="5.5 梯度截断"></a>5.5 梯度截断</h3><p>在深层次网络中，由于网络层数过深，在进行梯度的反向传播链式法则计算时，很有可能由于某一层梯度的过小或过大，造成累乘的放大效应，导致梯度消失或梯度爆炸现象的出现。例如，若权重初始化值较小，在各层上相乘得到的数值均在0-1之间，而激活函数梯度也在0-1之间，则连乘后数值会变得非常小，导致梯度消失。权重初始化较大同理。同样的，不合适的激活函数也会导致同样的效果。例如 sigmoid 函数。</p><p>在梯度计算时，最简单的避免梯度消失或梯度爆炸的方式是梯度截断。其思想也很简单，即把梯度限定在一个区间，当梯度的模小于或大于这个区间时就进行截断。一般有按值截断和按模截断两种方式。按值截断即限制了梯度向量中每个元素的取值。若某个参数的梯度小于或大于某个预设的阈值，则将该梯度固定为该阈值。按模截断限制了梯度向量的模长，假设截断阈值为 $\tau$，若 $||g_t|| &lt; \tau$，则保持 $||g_t||$ 不变；若 $||g_t|| &gt; \tau$，则 $g_t = \frac{b}{||g_t||} g_t$。</p><h2 id="6-外部因素制约：参数和输入"><a href="#6-外部因素制约：参数和输入" class="headerlink" title="6 外部因素制约：参数和输入"></a>6 外部因素制约：参数和输入</h2><p>由于深度学习优化问题本身的复杂性，仅仅对优化算法本身进行改进还不足以保证整个问题的完善求解。更多时候，一些外部因素往往会成为优化算法成功执行的制约条件。本部分将从参数初始化和输入值预处理出发，简要介绍如何对优化算法之外的一些关键要素进行优化，从而辅助优化算法在深度学习问题上更好地执行。</p><h3 id="6-1-参数初始化"><a href="#6-1-参数初始化" class="headerlink" title="6.1 参数初始化"></a>6.1 参数初始化</h3><p>深度学习模型的优化算法是迭代算法，因此要求指定一些开始迭代的初始点。训练深度模型是一个困难的问题，大多数优化算法都很大程度地受到初始化选择的影响。初始点能够决定算法是否收敛，有些初始点不稳定，使得该算法会遭遇数值困难。当学习收敛时，初始点可以决定学习收敛得多快，以及是否收敛到一个代价高或低的点。</p><p>现代的参数初始化策略大多为启发式的，该问题事实上仍然没有被很好地解决。比较常用的初始化方法有这样几种：（1）预训练初始化。预训练模型的一大初衷，就是帮助下游任务获得一套更优的初始化参数，加速下游任务的收敛，提高泛化能力。例如经典的BERT[8]、Roberta[11]等模型，至今仍被广泛沿用，尤其是自然语言处理任务。将预训练模型应用在下游任务上的过程被称为微调。（2）随机初始化。随机初始化的一大目的，是为了破坏“对称权重”现象。如果具有相同激<br>活函数的两个隐藏单元连接到相同的输入，且这些单元具有相同的初始参数，那么优化算法将一直以相同的方式更新这两个单元。因此，随机化权重可以确保没有输入模式丢失在前向传播的零空间中，没有梯度模式丢失在反向传播的零空间中，使得不同神经元之间的区分性更好。（3）固定值初始化。对于一些特殊的参数，我们可以根据经验用一个特殊的固定值来进行初始化．比如偏置通常用0 来初始化，但是有时可以设置某些经验值以提高优化效率。</p><p>总体来说，随机初始化通常更能够显著改善优化问题的结果。对于预训练初始化，尽管具有更好的收敛性和泛化性，但是灵活性不够，不能在目标任务上任意地调整网络结构。而固定值初始化过于依赖于经验，很多时候显然不具有说服力。因此，好的初始化方法对于优化问题依然十分重要。</p><p>比较简单的随机初始化方法固定了初始权重的方差，采用经典分布对初始值进行采样。例如高斯分布初始化，参数满足$\theta \sim \mathcal{N}(0, \sigma^2)$。或者采用均匀分布初始化，参数 $\theta$ 在给定区间 $[-r, r]$ 之间选择。若满足方差为 $\sigma^2$，则 $r = \sqrt{3 \sigma^2}$。不管采用何种分布，方差的设置都十分关键。若方差过小，则神经元输出过小，经过多层网络后会逐渐消失；若方差过大，则激活函数容易过饱和，尤其是对于sigmoid类型的函数而言，容易导致梯度消失。</p><p>一种更加有效的随机初始化方式，是根据神经元的连接数量来自适应地调整初始化分布的方差，尽可能保持每个神经元的输入和输出的方差一致。较为典型的是 Glorot and Bengio 提出的标准初始化方法[6]，对于 $m$ 个输入和 $n$ 个输出的全连接层，其初始化参数分布为 </p><p>$$<br>    \theta \sim U(-\sqrt{\frac{6}{m+n}}, \sqrt{\frac{6}{m+n}})<br>$$</p><p>上面介绍的两种基于方差的初始化方法都是对权重矩阵中的每个参数进行独立采样．由于采样的随机性，采样出来的权重矩阵依然可能存在梯度消失或梯度爆炸问题。为了避免这样的问题出现，我们希望误差项在反向传播中具有范数保持性。具体的，假设一个 $L$ 层的等宽线性网络，前向传播过程为 $y = W^{(L)} W^{(L-1)} \dots W^{(1)} x$，其中 $W^{(l)} \in \mathbb{R}^{M\times M}$ 为第 $l$ 层权重矩阵。范数保持性即，对于反向传播中每一层的误差项 $\delta^{(l-1)} = (W^{(l)})^T \delta^{(l)}$，均有 $||\delta^{(l-1)}||^2 = ||\delta^{(l)}||^2 = ||(W^{(l)})^T \delta^{(l)}||^2$。从这点出发，只需要将 $W^{(l)}$ 初始化为正交矩阵即可。这种方法称为正交初始化。</p><h3 id="6-2-输入值预处理"><a href="#6-2-输入值预处理" class="headerlink" title="6.2 输入值预处理"></a>6.2 输入值预处理</h3><p>除了参数初始化比较困难之外，不同输入特征的尺度差异比较大时，梯度下降法的效率也会受到影响。尺度不同会造成在大多数位置上的梯度方向并不是最优的搜索方向，当使用梯度下降法寻求最优解时，会导致需要很多次迭代才能收敛。如果把数据归一化为相同尺度，则大部分位置的梯度方向近似于最优搜索方向．这样，在梯度下降求解时，每一步梯度的方向都基本指向最小值，训练效率会大大提高。对于尺度敏感的模型，必须先对样本进行预处理，将各个维度的特征转换到相同的取值区间，并且消除不同特征之间的相关性，才能获得比较理想的结果。</p><p>假设有 $N$ 个输入样本 ${x^{(n)}}_{i=1}^N$，有如下几种归一化方法：</p><ul><li>最值归一化：该方法通过将每个输入特征的取值范围归一到 $[0, 1]$ 之间。具体的，对于每一维特征，归一化后的特征为<br>$$<br>  \hat{x}^{(n)} = \frac{x^{(n)} - \min_n (x^{(n)})}{\max_n (x^{(n)}) - \min_n (x^{(n)})}<br>$$</li><li>标准化：该方向将输入特征调整为均值为0，方差为1的标准正态分布。首先计算原分布的均值和方差<br>$$<br>  \begin{aligned}<pre><code>  &amp; \mu = \frac&#123;1&#125;&#123;N&#125; \sum_&#123;n=1&#125;^N x^&#123;(n)&#125; \\  &amp; \sigma^2 = \frac&#123;1&#125;&#123;N&#125; \sum_&#123;n=1&#125;^N (x^&#123;(n)&#125; - \mu)^2</code></pre>  \end{aligned}<br>$$<br>  之后利用均值和方差，计算每个特征归一化后的数值<br>$$<br>  \hat{x}^{(n)} = \frac{x^{(n)} - \mu}{\sigma}<br>$$</li><li>白化：白化是一种重要的预处理方法，用来降低输入数据特征之间的冗余性。输入数据经过白化处理后，特征之间相关性较低，并且所有特征具有相同的方差。白化的一个主要实现方式是使用主成分分析方法[21]去除掉各个成分之间的相关性。</li></ul><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h2><p>本文针对优化算法应用在深度学习问题中的若干挑战，对优化算法进行了归类总结。我们首先介绍了基础的梯度下降法和牛顿法，并以梯度下降法为理论基础进行算法改进。我们以批量大小为衡量标准，对优化问题的速度和精度进行了权重。之后，从学习率和梯度估计方法这两个角度出发，介绍了若干针对性的改进方法。最后，跳出优化算法本身，对模型初始化参数和输入值尺度进行了简单的探讨。实际上，优化问题仍然还存在许多可以改进的算法点，例如，牛顿法可以引申出拟牛顿法、共轭梯度法等更加高效的二阶方法，这些方法将在后续工作中继续探讨。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>[1] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational Research Society, 48(3):334–334, 1997.<br>[2] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. Siam Review, 60(2):223–311, 2018.<br>[3] John E Dennis Jr and Robert B Schnabel. Numerical methods for unconstrained optimization and nonlinear equations. SIAM, 1996<br>[4] Timothy Dozat. Incorporating nesterov momentum into adam. 2016.<br>[5] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research, 12(7), 2011.<br>[6] Xavier Glorot and Yoshua Bengio. Understanding the diﬀiculty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.<br>[7] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. arXiv preprint arXiv:1810.13243, 2018.<br>[8] Kenton L. Kristina T. Jacob D., Chang M. W. BERT: pre-training of deep bidirectional transformers for language understanding. In proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 4171–4186, 2019.<br>[9] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.<br>[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.<br>[11] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.<br>[12] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016.<br>[13] Olvi L Mangasarian. Nonlinear programming. SIAM, 1994.<br>[14] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/$k^2$). In Doklady an ussr, volume 269, pages 543–547, 1983.<br>[15] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):145–151, 1999.<br>[16] Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical statistics, pages 400–407, 1951.<br>[17] Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747, 2016.<br>[18] Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464–472. IEEE, 2017.<br>[19] Suvrit Sra, Sebastian Nowozin, and Stephen J Wright. Optimization for machine learning. Mit Press, 2012.<br>[20] Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26–31, 2012.<br>[21] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37–52, 1987.<br>[22] Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 最优化理论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文精读 —— 潜在邻近图对抗样本检测</title>
      <link href="/2022/06/05/LNG/"/>
      <url>/2022/06/05/LNG/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本文探讨的论文标题为《Adversarial Example Detection Using Latent Neighborhood Graph》。文章标题开门见山，两个关键部分，一个是对抗样本检测（Adversarial Example Detection），这是描述了这篇文章所要完成的任务，另一个是潜在邻近图（Latent Neighborhood Graph），也就是说，文章很可能是要用一个图模型来完成样本检测的任务。带着这两个关键词，我们来详细分析一下这篇 ICCV 会议上的文章。</p></blockquote><p><img src="/pic/LNG/LNG-%E4%B8%BB.png" alt="LNG 概念图"></p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1 引言"></a>1 引言</h2><h3 id="1-1-任务背景"><a href="#1-1-任务背景" class="headerlink" title="1.1 任务背景"></a>1.1 任务背景</h3><p>任务动机很朴素，当前的深度学习技术被广泛应用在了各个领域里面，但是一些攻击者会对整个深度模型进行操控，通过对模型的输入加入一些微小的扰动，来在人们难以察觉的情况下破坏整个模型的预测结果。这样的事情如果发生在一些安全系统当中，比如身份验证之类的重要环节，就会造成毁灭性的影响。因此深度模型的对抗训练必须被重视。</p><p><img src="/pic/LNG/%E4%BB%BB%E5%8A%A1%E8%83%8C%E6%99%AF.png" alt="一些深度模型微扰的例子，涉及cv、nlp等诸多领域"></p><h3 id="1-2-主流方法"><a href="#1-2-主流方法" class="headerlink" title="1.2 主流方法"></a>1.2 主流方法</h3><p>具体来说，为了让深度模型能够更好地去抵御这些扰动样本，目前比较主流的方法可以被归为两大类：主动防御和被动防御。</p><p>第一种是<font color=Red>主动防御</font>的方法。这种方法在模型训练中较为常见，即我们在训练时考虑到输入扰动的情况，然后手动加入一些对抗样本，这样能够提高模型的一个鲁棒性，而整个解空间更加平滑，而不会因为一个微扰让整个预测结果发生了根本性的变化。但是这种方法有一个非常关键的难点在于，它的训练代价比较大。试想一下，对于一个以及部署好了的已经被投放应用的模型，这时候说要让它的防御能力更强一点，势必要去重新训练整个模型，这个带来的代价是极大的，尤其是在真实的工业场景下。</p><p>另一种相对应的方法是<font color=Red>被动防御</font>方法。这种方式简单明了，不需要在训练时加入对抗样本提高鲁棒性，而是只需要在训练之前，就过滤出样本中的对抗样本即可。这样使得输入样本均为干净样本，训练出来的结果自然符合预期。这种方式对于已部署的系统来说很有价值，因为可以避免模型的重新训练。其次，它还可以帮助输入样本进行一次安全性检查，可以有效拦截一些不安全因素。</p><p>本文主要聚焦于被动防御方法，也就是对抗样本的检测。</p><h3 id="1-3-研究动机"><a href="#1-3-研究动机" class="headerlink" title="1.3 研究动机"></a>1.3 研究动机</h3><p>提到对抗样本检测，那就不得不提一下 Dknn 这个深度模型，这也是本文idea的一个核心的参考架构。</p><p>Dknn 是检测对抗样本的一个深度方法，它采用了 knn 的算法思想。我们要判断某个中心样本是否是对抗样本，首先将所有样本输入模型，之后在网络的每一层，每个样本都会得到一个 embedding。之后，沿用 knn 的思想，选择这个中心样本最相近的 k 个邻居，并将这 k 个邻居和中心样本的类别进行比较。如果这些样本基本属于同一类，说明这个中心样本不太可能是对抗样本，如果它们之间对应的类别有明显的不一致，例如，这个中心样本的类别是熊猫，但是它的 k 个邻居里面有一半是表示汽车的样本，那么这时候就可以怀疑这个中心样本可能存在问题。</p><p><img src="/pic/LNG/dknn.png" alt="DkNN 架构的核心思路示意图"></p><p>受到 Dknn 的启发，作者认为，Dknn 在检测对抗样本的时候，是利用了输入样本和它邻近样本之间的联系来判断的，那么可以利用一个动态的图结构，来更加具体地表示这种邻近关系。于是诞生了本文的核心模型，也就是 latent neighboorhood graph（以下简称LNG）。图模型的好处在于，它不光能够表示中心节点和它的邻近点，还能够通过建边来表示点和点之间的关系，这是 Dknn 方法做不到的。其次，把图模型构建出来之后，可以转化成一个二分类问题，利用图神经网络等方法进行分类。</p><h3 id="1-4-优势对比"><a href="#1-4-优势对比" class="headerlink" title="1.4 优势对比"></a>1.4 优势对比</h3><table>    <tr>        <td>LNG</td><td>Dknn</td>    </tr>    <tr>        <td>cover multi-hop heighbors of inputs’ local manifolds</td><td>only cover inputs’ local manifolds</td>    </tr>        <tr>        <td>richer information, aggregate the connectivity learned on the embedding space</td><td>only cover the information of class labels</td>    </tr>        <tr>        <td>incorporate both adversarial and benign neighbors</td><td>only utilize benign neighbors</td>    </tr></table><p>相比于dknn，LNG 的优势在于：<br>（1）图模型的信息表达更加丰富，它不光有节点的信息，也就是中心节点的邻居信息，还聚合了边的信息，也就是节点和节点之间的联系。我们可以通过距离来量化点和点之间的关联。<br>（2）LNG 引入了邻居多跳机制，可以把中心节点的邻居的邻居也给选择进来，让整个图模型的信息进一步丰富起来。</p><h2 id="2-方法架构"><a href="#2-方法架构" class="headerlink" title="2 方法架构"></a>2 方法架构</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><p>LNG 的方法流程如下所示：</p><p><img src="/pic/LNG/%E6%B5%81%E7%A8%8B.png" alt="LNG 方法流程图"></p><p>第一步，从完整的输入样本中提取出一个子集，称作是参考数据集，这个数据集的是用来构建图模型中的结点的，也就是图模型的结点范围不会超出这个参考数据集。</p><p>第二步，构建完数据集之后需要建图。建图分成两部分，首先是选择节点，其次是将点和点之间构建无向边，从而形成最终的图模型。</p><p>第三步，二分类问题，也就是判断中心节点是否是对抗样本。</p><h3 id="2-2-参考数据集"><a href="#2-2-参考数据集" class="headerlink" title="2.2 参考数据集"></a>2.2 参考数据集</h3><p>首先是参考数据集的构建。对于一个完整的数据集，从中提取出一个样本作为中心样本，我们需要判断这个样本是良性的还是对抗样本。之后，从这个完整的数据集中提取出一个样本子集，作为候选样本。</p><p>接下来有两种数据集的构建方法，第一种是直接把样本子集和中心样本给合起来，作为一个参考数据集，称为良性数据集。第二种是先对这个样本子集进行数据增强，也就是子集中的每个样本都利用对抗算法获得一个对抗样本，之后把扩充后的样本子集和中心样本合并起来，作为一个新的参考数据集，称为对抗数据集。</p><p><img src="/pic/LNG/reference_dataset.png" alt="参考数据集构建流程（自己画的）"></p><h3 id="2-3-潜在邻近图"><a href="#2-3-潜在邻近图" class="headerlink" title="2.3 潜在邻近图"></a>2.3 潜在邻近图</h3><p>接下来是核心步骤 —— 构图。</p><h4 id="2-3-1-结点构造"><a href="#2-3-1-结点构造" class="headerlink" title="2.3.1 结点构造"></a>2.3.1 结点构造</h4><p>首先是图节点的选择。对于中心节点来说，从参考数据集中选出最近的 k 个节点作为邻居。其次，引入了多跳邻居的思想，不仅可以选择中心节点的 k 邻近节点，还可以选择邻居的 k 邻近节点。具体来说，设置一个阈值 L，表示可以迭代的邻居次数。例如 L=2，就可以选择中心节点的邻居，这是一轮，以及邻居的邻居，这是第二轮，那 L=3,4 以此类推，相当于一个广度优先搜索的思想。但是所有选出的点不会超出参考数据集的范围。</p><h4 id="2-3-2-边构造"><a href="#2-3-2-边构造" class="headerlink" title="2.3.2 边构造"></a>2.3.2 边构造</h4><p>接下来是节点之间边的表示，主要还是利用欧氏距离来进行表征，并且为了归一化尺度，用 sigmoid 函数做了一个映射，将边权映射到0到1的区间上。此外，这个 sigmoid 函数中有两个参数 $t$ 和 $\theta$，是放在网络中用来学习的参数。</p><p>$$<br>A_{i,j} = \frac{1}{1 + e^{-t \cdot d(i,j) + \theta}}<br>$$</p><h3 id="2-4-图分类器"><a href="#2-4-图分类器" class="headerlink" title="2.4 图分类器"></a>2.4 图分类器</h3><p>最后一部分是图分类器，用来判定中心节点是良性样本还是对抗样本。文章采用的是经典的图注意力网络模型 GAT，模型的输入是所有样本的 embedding 以及邻接矩阵，输出是一个二维向量。</p><h2 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>实验共采用了 5 种经典的对抗样本生成方法，包括 FGSM，PGD 等，这是在构建参考数据集的时候，对原数据做数据增强用的。Baseline 主要用了 DKNN 和 KNN 架构，以及 LID 和Hu 等人提出的方法。数据集采用了图像领域经典的几个数据集，包括有 CIFAR-10，ImageNet 和 STL-10。</p><h3 id="3-2-实验细节"><a href="#3-2-实验细节" class="headerlink" title="3.2 实验细节"></a>3.2 实验细节</h3><p>对于每个数据集，分成三个部分：训练集、参考集和测试集，这里的参考集是用来选取超参数的，比如多跳邻居机制里面的参数 L。验证集是从测试集里单独划分出来的，例如对于CIFAR-10 数据集，本实验从测试集中，每个类别随机选了 100 个样本组成了新的验证集。此外，同一个数据集上只能用一种对抗攻击方法，以及在主实验中，使用的是加入对抗样本的参考数据集。</p><p>超参数设置方面，主要是多跳邻居机制的阈值 L 和 knn 算法里面的 k。文章设置了 L=2，k=5。还有一个是 baseline 里面的dknn算法，也要有具体 k 值的设置。实验在三个数据集上的 k 值设置分别为200，40 和 40。</p><p>最后是对于 LNG 图的输出结果的处理。在训练过程中，所有的边的结果是通过欧氏距离和sigmoid 映射来产生的。从模型输出之后，所有的边的信息又被映射为一个 0-1 空间。具体来说，如果这条边的大小大于某个阈值 t，那么认为这条边存在，赋值为 1，否则认为不存在，赋值为 0。</p><h3 id="3-3-threat-model"><a href="#3-3-threat-model" class="headerlink" title="3.3 threat model"></a>3.3 threat model</h3><h4 id="3-3-1-白盒测试"><a href="#3-3-1-白盒测试" class="headerlink" title="3.3.1 白盒测试"></a>3.3.1 白盒测试</h4><h4 id="3-3-2-黑盒测试"><a href="#3-3-2-黑盒测试" class="headerlink" title="3.3.2 黑盒测试"></a>3.3.2 黑盒测试</h4><h3 id="3-4-主实验"><a href="#3-4-主实验" class="headerlink" title="3.4 主实验"></a>3.4 主实验</h3><h4 id="3-4-1-检测已知攻击"><a href="#3-4-1-检测已知攻击" class="headerlink" title="3.4.1 检测已知攻击"></a>3.4.1 检测已知攻击</h4><h4 id="3-4-2-检测未知攻击"><a href="#3-4-2-检测未知攻击" class="headerlink" title="3.4.2 检测未知攻击"></a>3.4.2 检测未知攻击</h4><h3 id="3-5-消融实验"><a href="#3-5-消融实验" class="headerlink" title="3.5 消融实验"></a>3.5 消融实验</h3><h3 id="3-6-图的拓扑结构讨论"><a href="#3-6-图的拓扑结构讨论" class="headerlink" title="3.6 图的拓扑结构讨论"></a>3.6 图的拓扑结构讨论</h3>]]></content>
      
      
      <categories>
          
          <category> 对抗机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对抗机器学习 </tag>
            
            <tag> 论文精读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hpctoolkit安装与使用</title>
      <link href="/2022/01/19/hpctoolkit/"/>
      <url>/2022/01/19/hpctoolkit/</url>
      
        <content type="html"><![CDATA[<p><img src="/pic/hpctoolkit.png" alt="hpctoolkit 工作流程图"></p><h2 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h2><!-- We now use Spack for building HPCToolkit's prerequisites (replacing the old hpctoolkit externals). You can install HPCToolkit with the "One Button" spack install hpctoolkit method. --><p>本文采用 Spack 来构建 HPCToolkit 的 prerequisites（不使用原有的 hpctoolkit 外部组件）。 </p><ul><li>clone spack 对应的 github 仓库</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">git clone https:&#x2F;&#x2F;github.com&#x2F;spack&#x2F;spack<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>创建环境变量</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">export SPACK_ROOT&#x3D;&#96;pwd&#96;&#x2F;spackexport PATH&#x3D;$&#123;SPACK_ROOT&#125;&#x2F;bin:$PATH<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><ul><li>刷新 shell 环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">source $&#123;SPACK_ROOT&#125;&#x2F;share&#x2F;spack&#x2F;setup-env.sh<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>检测安装环境</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">spack compiler find<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><ul><li>安装</li></ul><pre class="line-numbers language-shell" data-language="shell"><code class="language-shell">spack install hpctoolkit<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="常见-bug"><a href="#常见-bug" class="headerlink" title="常见 bug"></a>常见 bug</h2><p><img src="/pic/hpctoolkit/bug1.png" alt="bug 1"></p><p><img src="/pic/hpctoolkit/bug2.png" alt="bug 2"></p><ul><li><p>原因</p><p>  未设置 fortran 编译环境</p></li><li><p>解决方法</p><p>  将安装环境加入 <code>/.spack/linux/compilers.yaml</code> 文件中</p></li></ul><p><img src="/pic/hpctoolkit/bug.png" alt="解决方案"></p>]]></content>
      
      
      <categories>
          
          <category> 并行计算 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 高性能分析工具 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
